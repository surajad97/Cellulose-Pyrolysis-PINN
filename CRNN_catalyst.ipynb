{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79a67ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Random, Plots\n",
    "using Zygote, ForwardDiff\n",
    "using OrdinaryDiffEq, DiffEqSensitivity\n",
    "using LinearAlgebra\n",
    "using Statistics\n",
    "using ProgressBars, Printf\n",
    "using Flux\n",
    "using Flux.Optimise: update!\n",
    "using Flux.Losses: mae, mse\n",
    "using BSON: @save, @load\n",
    "using DelimitedFiles\n",
    "using YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27fa18ff-8c89-4c9b-a63b-d62f67907886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C:\\\\Users\\\\Suraj\\\\Documents\\\\Biomass_run\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19fc3173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"100\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"GKSwstype\"] = \"100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "887d845b-4175-4e8c-bde1-8bd2756a55b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 15 entries:\n",
       "  \"lr_max\"        => 0.002\n",
       "  \"grad_max\"      => 100.0\n",
       "  \"maxiters\"      => 50000\n",
       "  \"n_epoch\"       => 5000\n",
       "  \"lr_min\"        => 1.0e-5\n",
       "  \"is_restart\"    => false\n",
       "  \"lr_decay\"      => 0.5\n",
       "  \"lr_decay_step\" => 500\n",
       "  \"nc\"            => 1\n",
       "  \"expr_name\"     => \"5s8r1c-01\"\n",
       "  \"nr\"            => 8\n",
       "  \"n_plot\"        => 10\n",
       "  \"ns\"            => 6\n",
       "  \"lb\"            => 1.0e-8\n",
       "  \"w_decay\"       => 1.0e-8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = YAML.load_file(\"./config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0f0c5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e-8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr_name = conf[\"expr_name\"]\n",
    "fig_path = string(\"./results_catalyst/\", expr_name, \"/figs\")\n",
    "ckpt_path = string(\"./results_catalyst/\", expr_name, \"/checkpoint\")\n",
    "config_path = \"./results_catalyst/$expr_name/config.yaml\"\n",
    "\n",
    "is_restart = Bool(conf[\"is_restart\"])\n",
    "ns = Int64(conf[\"ns\"])\n",
    "nr = Int64(conf[\"nr\"])\n",
    "nc = Int64(conf[\"nc\"])\n",
    "nss = Int64(conf[\"ns\"]) - Int64(conf[\"nc\"])\n",
    "lb = Float64(conf[\"lb\"])\n",
    "n_epoch = Int64(conf[\"n_epoch\"])\n",
    "n_plot = Int64(conf[\"n_plot\"])\n",
    "grad_max = Float64(conf[\"grad_max\"])\n",
    "maxiters = Int64(conf[\"maxiters\"])\n",
    "\n",
    "lr_max = Float64(conf[\"lr_max\"])\n",
    "lr_min = Float64(conf[\"lr_min\"])\n",
    "lr_decay = Float64(conf[\"lr_decay\"])\n",
    "lr_decay_step = Int64(conf[\"lr_decay_step\"])\n",
    "w_decay = Float64(conf[\"w_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e1b7c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llb = lb;\n",
    "global p_cutoff = -1.0\n",
    "\n",
    "const l_exp = 1:9\n",
    "n_exp = length(l_exp)\n",
    "\n",
    "l_train = []\n",
    "l_val = []\n",
    "for i = 1:n_exp\n",
    "    j = l_exp[i]\n",
    "    if !(j in [1,5,9])\n",
    "        push!(l_train, i)\n",
    "    else\n",
    "        push!(l_val, i)\n",
    "    end\n",
    "end\n",
    "\n",
    "opt = Flux.Optimiser(\n",
    " ExpDecay(lr_max, lr_decay, length(l_train) * lr_decay_step, lr_min),\n",
    " ADAMW(lr_max, (0.9, 0.999), w_decay),\n",
    ");\n",
    "\n",
    "#opt = Flux.Optimise.ADAMW(lr_max,(0.9,0.999), w_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "412c195f-3267-4c53-a5f3-ffa1f7211b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[2, 3, 4, 6, 7, 8], Any[1, 5, 9], 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train, l_val, n_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f734609-f5d7-4370-9b84-cb53a97c6995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./results_catalyst/5s8r1c-01/config.yaml\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if !is_restart\n",
    "    if ispath(fig_path)\n",
    "        rm(fig_path, recursive = true)\n",
    "    end\n",
    "    if ispath(ckpt_path)\n",
    "        rm(ckpt_path, recursive = true)\n",
    "    end\n",
    "end\n",
    "\n",
    "if ispath(\"./results_catalyst\") == false\n",
    "    mkdir(\"./results_catalyst\")\n",
    "end\n",
    "\n",
    "if ispath(\"./results_catalyst/$expr_name\") == false\n",
    "    mkdir(\"./results_catalyst/$expr_name\")\n",
    "end\n",
    "\n",
    "if ispath(fig_path) == false\n",
    "    mkdir(fig_path)\n",
    "    mkdir(string(fig_path, \"/conditions\"))\n",
    "end\n",
    "\n",
    "if ispath(ckpt_path) == false\n",
    "    mkdir(ckpt_path)\n",
    "end\n",
    "\n",
    "cp(\"./config.yaml\", config_path; force=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb7a6db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_exp (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_exp(filename)\n",
    "    exp_data = readdlm(filename)  # [t, T, m]\n",
    "    index = indexin(unique(exp_data[:, 1]), exp_data[:, 1])\n",
    "    exp_data = exp_data[index, :]\n",
    "    exp_data[:, 3] = exp_data[:, 3] / maximum(exp_data[:, 3])\n",
    "    return exp_data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4736e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_exp_data = [];\n",
    "l_exp_info = zeros(Float64, length(l_exp), 3);\n",
    "for (i_exp, value) in enumerate(l_exp)\n",
    "    filename = string(\"Exp_data5/expdata_no\", string(value), \".txt\")\n",
    "\n",
    "    exp_data = Float64.(load_exp(filename))\n",
    "\n",
    "    #exp_data = exp_data[1:80, :]\n",
    "    \n",
    "    # if value == 22\n",
    "    #     exp_data = exp_data[1:90, :]\n",
    "    # elseif value == 23\n",
    "    #     exp_data = exp_data[1:100, :]\n",
    "    # elseif value == 24\n",
    "    #     exp_data = exp_data[1:100, :]\n",
    "    #end\n",
    "\n",
    "    push!(l_exp_data, exp_data)\n",
    "    l_exp_info[i_exp, 1] = exp_data[1, 2] # initial temperature, K\n",
    "end\n",
    "l_exp_info[:, 2] = readdlm(\"Exp_data5/beta.txt\")[l_exp];\n",
    "l_exp_info[:, 3] = (readdlm(\"Exp_data5/cata_conc.txt\")[l_exp]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c8e9a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76×3 Matrix{Float64}:\n",
       "    0.0  461.61  1.0\n",
       " 1000.0  503.42  0.994208\n",
       " 1250.0  513.9   0.988417\n",
       " 1500.0  524.35  0.978764\n",
       " 1600.0  528.47  0.974903\n",
       " 1650.0  530.59  0.972973\n",
       " 1700.0  532.67  0.969112\n",
       " 1750.0  534.74  0.967181\n",
       " 1800.0  536.8   0.96332\n",
       " 1850.0  538.89  0.959459\n",
       " 1900.0  540.95  0.955598\n",
       " 1950.0  543.05  0.951737\n",
       " 2000.0  545.12  0.947876\n",
       "    ⋮            \n",
       " 4718.0  658.32  0.214286\n",
       " 4918.0  666.67  0.212355\n",
       " 5118.0  675.01  0.210425\n",
       " 5318.0  683.32  0.208494\n",
       " 5518.0  691.63  0.206564\n",
       " 5718.0  699.98  0.202703\n",
       " 5918.0  708.29  0.200772\n",
       " 6118.0  716.6   0.198842\n",
       " 6318.0  724.9   0.194981\n",
       " 6518.0  733.22  0.19305\n",
       " 6718.0  741.57  0.19112\n",
       " 6918.0  749.94  0.189189"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_exp_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0fe6abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×3 Matrix{Float64}:\n",
       " 497.56   2.5  0.0\n",
       " 523.22  10.0  0.0\n",
       " 490.64   2.5  0.001\n",
       " 461.61   2.5  0.01\n",
       " 517.04  10.0  0.001\n",
       " 490.7   10.0  0.01\n",
       " 473.2    5.0  0.0\n",
       " 473.17   5.0  0.001\n",
       " 473.25   5.0  0.01"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_exp_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8def2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(12345)\n",
    "np = nr * (ns + nc + 3) + 1\n",
    "p = randn(Float64, np) .* 5.e-2;\n",
    "p[1:nr] .+= 0.8;  # w_lnA\n",
    "#p[nr*(nss+1)+1:nr*(ns+1)].*= 10            #w_cat_in\n",
    "p[nr*(ns+1)+1:nr*(ns+nc+1)] .= 0         #w_cat_out\n",
    "p[nr*(ns+nc+1)+1:nr*(ns+nc+2)] .+= 0.8;  # w_Ea\n",
    "p[nr*(ns+nc+2)+1:nr*(ns+nc+3)] .+= 0.1;  # w_b\n",
    "p[end] = 0.1;  #slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d418be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p2vec (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function p2vec(p)\n",
    "    slope = p[end] .* 1.e1\n",
    "    w_b = p[1:nr] .* (slope * 10.0)\n",
    "    w_b = clamp.(w_b, 0, 50)\n",
    "\n",
    "    w_out = reshape(p[nr+1:nr*(nss+1)], nss, nr)\n",
    "    @. w_out[1, :] = clamp(w_out[1, :], -3.0, 0.0)\n",
    "    @. w_out[end, :] = clamp(abs(w_out[end, :]), 0.0, 3.0)\n",
    "\n",
    "    if p_cutoff > 0.0\n",
    "        w_out[findall(abs.(w_out) .< p_cutoff)] .= 0.0\n",
    "    end\n",
    "\n",
    "    w_out[nss-1:nss-1, :] .=\n",
    "        -sum(w_out[1:nss-2, :], dims = 1) .- sum(w_out[nss:nss, :], dims = 1)\n",
    "\n",
    "    w_cat_in = p[nr*(nss+1)+1:nr*(ns+1)]\n",
    "    w_cat_out = p[nr*(ns+1)+1:nr*(ns+nc+1)]*0\n",
    "    \n",
    "    #w_cat_out = - sign.(w_cat_in) .* abs.(w_cat_out);\n",
    "    w_cat_in = abs.(w_cat_in)\n",
    "    \n",
    "    \n",
    "    if p_cutoff > 0.0\n",
    "        w_cat_in[findall(abs.(w_cat_in) .< p_cutoff)] .= 0.0\n",
    "    end\n",
    "    \n",
    "    w_in_Ea = abs.(p[nr*(ns+nc+1)+1:nr*(ns+nc+2)].* (slope * 100.0))\n",
    "    w_in_Ea = clamp.(w_in_Ea, 0.0, 300.0)\n",
    "\n",
    "    w_in_b = abs.(p[nr*(ns+nc+2)+1:nr*(ns+nc+3)])\n",
    "\n",
    "\n",
    "    w_in = vcat(clamp.(-w_out, 0.0, 3.0), w_cat_in', w_in_Ea', w_in_b')\n",
    "    w_out = vcat(w_out, w_cat_out')\n",
    "    \n",
    "    return w_in, w_b, w_out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "d2ef21d4-97f1-465b-a27b-6214f9cdf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr_name1 = \"5s8r1c-02\"\n",
    "# ckpt_path1 = string(\"./results_catalyst/\", expr_name1, \"/checkpoint\")\n",
    "# @load string(ckpt_path1, \"/mymodel.bson\") p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47e16cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.01  0.0   0.0   0.15  0.0  0.03  83.95  0.08  7.27  -0.01   0.02   0.03  -0.15  0.11  0.0\n",
      "  0.01  0.08  0.04  0.0   0.0  0.07  80.93  0.06  7.79  -0.01  -0.08  -0.04   0.13  0.0   0.0\n",
      "  0.01  0.1   0.05  0.0   0.0  0.02  72.92  0.12  8.42  -0.01  -0.1   -0.05   0.12  0.04  0.0\n",
      "  0.0   0.01  0.06  0.0   0.0  0.06  68.25  0.11  8.21  -0.0   -0.01  -0.06   0.05  0.02  0.0\n",
      "  0.05  0.0   0.05  0.0   0.0  0.01  81.05  0.01  7.51  -0.05   0.0   -0.05  -0.0   0.1   0.0\n",
      " -0.0   0.0   0.0   0.01  0.0  0.02  75.51  0.14  7.25   0.0   -0.0    0.01  -0.01  0.0   0.0\n",
      " -0.0   0.04  0.05  0.0   0.0  0.0   80.93  0.11  8.31   0.0   -0.04  -0.05   0.07  0.02  0.0\n",
      "  0.09  0.03  0.0   0.0   0.0  0.03  84.52  0.05  7.3   -0.09  -0.03   0.03   0.06  0.02  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function display_p(p)\n",
    "    w_in, w_b, w_out = p2vec(p)\n",
    "    println(\"\\n species (column) reaction (row)\")\n",
    "    println(\"w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\")\n",
    "    show(stdout, \"text/plain\", round.(hcat(w_in', w_b, w_out'), digits = 2))\n",
    "    # println(\"\\n w_out\")\n",
    "    # show(stdout, \"text/plain\", round.(w_out', digits=3))\n",
    "    println(\"\\n\")\n",
    "end\n",
    "display_p(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efbee5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getsampletemp (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function getsampletemp(t, T0, beta)\n",
    "    \n",
    "    T = clamp.((T0 + beta / 60 * t), 0, 873.15)   # K/min to K/s\n",
    "\n",
    "    return T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fba0576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crnn! (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const R = -1.0 / 8.314e-3  # universal gas constant, kJ/mol*K\n",
    "@inbounds function crnn!(du, u, p, t)\n",
    "    logX = @. log(clamp(u, lb, 10.0))\n",
    "    T = getsampletemp(t, T0, beta)\n",
    "    w_in_x = w_in' * vcat(logX, R / T, log(T))\n",
    "    du .= w_out * (@. exp(w_in_x + w_b))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fd55e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[38;2;86;182;194mODEProblem\u001b[0m with uType \u001b[38;2;86;182;194mVector{Float64}\u001b[0m and tType \u001b[38;2;86;182;194mFloat64\u001b[0m. In-place: \u001b[38;2;86;182;194mtrue\u001b[0m\n",
       "timespan: (0.0, 1.0)\n",
       "u0: 6-element Vector{Float64}:\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tspan = [0.0, 1.0];\n",
    "u0 = zeros(ns);\n",
    "u0[1] = 1.0;\n",
    "prob = ODEProblem(crnn!, u0, tspan, p, abstol = lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0e559c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ForwardSensitivity{0, true, Val{:central}}(true, false)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition(u, t, integrator) = u[1] < lb * 10.0\n",
    "affect!(integrator) = terminate!(integrator)\n",
    "_cb = DiscreteCallback(condition, affect!)\n",
    "\n",
    "alg = TRBDF2();\n",
    "#alg = RK4();\n",
    "#alg = TRBDF2();\n",
    "#alg = AutoTsit5(Rosenbrock23(autodiff=false));\n",
    "#alg = Rosenbrock23(autodiff=false);\n",
    "#alg = AutoVern7(Rodas5P(autodiff = false));\n",
    "#sense = BacksolveAdjoint(checkpointing=true; autojacvec=ZygoteVJP());\n",
    "sense = ForwardSensitivity(autojacvec = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7353dd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred_n_ode (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pred_n_ode(p, i_exp, exp_data)\n",
    "    global T0, beta, cat_conc = l_exp_info[i_exp, :]\n",
    "    global w_in, w_b, w_out = p2vec(p)\n",
    "\n",
    "    ts = @view(exp_data[:, 1])\n",
    "    tspan = [ts[1], ts[end]]\n",
    "    u0[end] = cat_conc \n",
    "    sol = solve(\n",
    "        prob,\n",
    "        alg,\n",
    "        tspan = tspan,\n",
    "        p = p,\n",
    "        saveat = ts, \n",
    "        sensealg = sense,\n",
    "        maxiters = maxiters\n",
    "        # callback = _cb,\n",
    "    )\n",
    "\n",
    "    if sol.retcode == :Success\n",
    "        nothing\n",
    "    else\n",
    "        #@sprintf(\"solver failed beta: %.0f ocen: %.2f\", beta, exp(ocen))\n",
    "        @sprintf(\"solver failed beta: %.0f\", beta)\n",
    "    end\n",
    "    if length(sol.t) > length(ts)\n",
    "        # @show exp_data[:, 1]\n",
    "        # @show sol.t\n",
    "        return  sol[:, 1:length(ts)]\n",
    "    else\n",
    "        return sol\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4fd2705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_neuralode (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_neuralode(p, i_exp)\n",
    "    exp_data = l_exp_data[i_exp]\n",
    "    pred = Array(pred_n_ode(p, i_exp, exp_data))\n",
    "    masslist = sum(clamp.(@view(pred[1:end-1-nc, :]), 0, Inf), dims = 1)'\n",
    "    gaslist = clamp.(@views(pred[end-nc, :]), 0,  Inf)\n",
    "\n",
    "    loss = mae(masslist, @view(exp_data[1:length(masslist), 3])) + mae(gaslist, 1 .- @view(exp_data[1:length(masslist), 3]))\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "6c2a4f48-b669-4995-aa6d-1121364acdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function loss_neuralode(p, i_exp)\n",
    "#     exp_data = l_exp_data[i_exp]\n",
    "#     pred = Array(pred_n_ode(p, i_exp, exp_data))\n",
    "#     masslist = sum(clamp.(@view(pred[1:end-1-nc, :]), 0, Inf), dims = 1)'\n",
    "#     gaslist = clamp.(@views(pred[end-nc, :]), 0,  Inf)\n",
    "\n",
    "#     loss = mse(masslist, @view(exp_data[1:length(masslist), 3])) + mse(gaslist, 1 .- @view(exp_data[1:length(masslist), 3]))\n",
    "#     return loss\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4d8bb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16.619802 seconds (45.09 M allocations: 2.649 GiB, 6.86% gc time, 99.58% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1469355162922197"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time loss = loss_neuralode(p, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c402d1e9-0213-429d-9e0b-a1579411b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_sol(i_exp, sol, exp_data, Tlist, cap, sol0 = nothing)\n",
    "    T0, beta, cat = l_exp_info[i_exp, :]\n",
    "    #T0, beta = l_exp_info[i_exp, :]\n",
    "    ts = sol.t / 60.0\n",
    "    ind = length(ts)\n",
    "    plt = plot(\n",
    "        exp_data[:, 1] / 60.0,\n",
    "        exp_data[:, 3],\n",
    "        seriestype = :scatter,\n",
    "        label = \"Exp\",\n",
    "    )\n",
    "\n",
    "    plot!(\n",
    "        plt,\n",
    "        ts,\n",
    "        sum(clamp.(sol[1:end-1-nc, :], 0, Inf), dims = 1)',\n",
    "        lw = 3,\n",
    "        legend = :left,\n",
    "        label = \"CRNN\",\n",
    "    )\n",
    "    \n",
    "    if sol0 !== nothing\n",
    "        plot!(\n",
    "            plt,\n",
    "            sol0.t / 60,\n",
    "            sum(sol0[1:end-1-nc, :], dims = 1)',\n",
    "            label = \"initial model\",\n",
    "        )\n",
    "    end\n",
    "    xlabel!(plt, \"Time [min]\")\n",
    "    ylabel!(plt, \"Mass\")\n",
    "    ylims!(0,1.1)\n",
    "    title!(plt, cap)\n",
    "    exp_cond = string(\n",
    "        @sprintf(\n",
    "            #\"T0 = %.1f K \\n beta = %.2f K/min\",\n",
    "            \"T0 = %.1f K \\n beta = %.2f K/min \\n [catalyst] = %.2f\",\n",
    "            T0,\n",
    "            beta,\n",
    "            cat*100\n",
    "        )\n",
    "    )\n",
    "    annotate!(plt, exp_data[end, 1] / 60.0 * 0.85, 0.4, exp_cond)\n",
    "\n",
    "    plt2 = twinx()\n",
    "    plot!(\n",
    "        plt2,\n",
    "        exp_data[1:ind, 1] / 60,\n",
    "        Tlist,\n",
    "        lw = 2,\n",
    "        ls = :dash,\n",
    "        legend = :topright,\n",
    "        label = \"T\",\n",
    "    )\n",
    "    ylabel!(plt2, \"Temp\")\n",
    "\n",
    "    p2 = plot(ts, sol[1, :], lw = 2, legend = :right, label = \"Cellulose\")\n",
    "    for i = 2:ns-nc\n",
    "        plot!(p2, ts, sol[i, :], lw = 2, label = \"S$i\")\n",
    "    end\n",
    "    xlabel!(p2, \"Time [min]\")\n",
    "    ylabel!(p2, \"Mass\")\n",
    "\n",
    "    plt = plot(plt, p2, framestyle = :box, layout = @layout [a; b])\n",
    "    plot!(plt, size = (800, 800))\n",
    "    return plt\n",
    "end\n",
    "\n",
    "cbi = function (p, i_exp)\n",
    "    exp_data = l_exp_data[i_exp]\n",
    "    sol = pred_n_ode(p, i_exp, exp_data)\n",
    "    Tlist = similar(sol.t)\n",
    "    #T0, beta, ocen = l_exp_info[i_exp, :]\n",
    "    T0, beta = l_exp_info[i_exp, :]\n",
    "    for (i, t) in enumerate(sol.t)\n",
    "        Tlist[i] = getsampletemp(t, T0, beta)\n",
    "    end\n",
    "    value = l_exp[i_exp]\n",
    "    plt = plot_sol(i_exp, sol, exp_data, Tlist, \"exp_$value\")\n",
    "    png(plt, string(fig_path, \"/conditions/pred_exp_$value\"))\n",
    "    return false\n",
    "end\n",
    "\n",
    "\n",
    "function plot_loss(l_loss_train, l_loss_val; yscale = :log10)\n",
    "    plt_loss = plot(l_loss_train, yscale = yscale, label = \"train\")\n",
    "    plot!(plt_loss, l_loss_val, yscale = yscale, label = \"val\")\n",
    "    plt_grad = plot(list_grad, yscale = yscale, label = \"grad_norm\")\n",
    "    xlabel!(plt_loss, \"Epoch\")\n",
    "    ylabel!(plt_loss, \"Loss\")\n",
    "    xlabel!(plt_grad, \"Epoch\")\n",
    "    ylabel!(plt_grad, \"Gradient Norm\")\n",
    "    # ylims!(plt_loss, (-Inf, 1e0))\n",
    "    # ylims!(plt_grad, (-Inf, 1e3))\n",
    "    #plt_all = plot([plt_loss, plt_grad]..., legend = :top, framestyle=:box)\n",
    "    # plot!(\n",
    "    #     plt_all,\n",
    "    #     size=(800, 400),\n",
    "    #     xtickfontsize = 11,\n",
    "    #     ytickfontsize = 11,\n",
    "    #     xguidefontsize = 12,\n",
    "    #     yguidefontsize = 12,\n",
    "    # )\n",
    "    png(plt_loss, string(fig_path, \"/loss_plot\"))\n",
    "    png(plt_grad, string(fig_path, \"/grad_plot\"))\n",
    "end\n",
    "\n",
    "l_loss_train = []\n",
    "l_loss_val = []\n",
    "list_grad = []\n",
    "iter = 1\n",
    "cb = function (p, loss_train, loss_val, g_norm)\n",
    "    global l_loss_train, l_loss_val, list_grad, iter\n",
    "    push!(l_loss_train, loss_train)\n",
    "    push!(l_loss_val, loss_val)\n",
    "    push!(list_grad, g_norm)\n",
    "\n",
    "    #if iter % n_plot == 0\n",
    "    if all(x -> loss_val < x, l_loss_val[1:end-1])\n",
    "        display_p(p)\n",
    "        list_exp = randperm(n_exp)[1]\n",
    "        @printf(\n",
    "            \"Min Loss train: %.2e val: %.2e\",\n",
    "            minimum(l_loss_train),\n",
    "            minimum(l_loss_val)\n",
    "        )\n",
    "        println(\"\\n update plot \", l_exp[list_exp], \"\\n\")\n",
    "        for i_exp in list_exp\n",
    "            cbi(p, i_exp)\n",
    "        end\n",
    "        @save string(ckpt_path, \"/mymodel.bson\") p opt l_loss_train l_loss_val list_grad iter\n",
    "    end\n",
    "    \n",
    "    plot_loss(l_loss_train, l_loss_val; yscale = :log10)\n",
    "    iter += 1 \n",
    "end\n",
    "\n",
    "if is_restart\n",
    "    @load string(ckpt_path, \"/mymodel.bson\") p opt l_loss_train l_loss_val list_grad iter\n",
    "    iter += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "7ae2de53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                             ┫ 0/2.5k [00:00<00:-7, -0s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.04  0.01  0.06  0.0   0.0  0.08  70.3   0.08  7.1   -0.04  -0.01  -0.06   0.09  0.01  0.0\n",
      "  0.03  0.0   0.0   0.03  0.0  0.04  70.74  0.11  7.6   -0.03   0.0   -0.0   -0.03  0.06  0.0\n",
      "  0.02  0.04  0.02  0.0   0.0  0.02  79.11  0.05  7.66  -0.02  -0.04  -0.02   0.05  0.04  0.0\n",
      "  0.01  0.11  0.01  0.0   0.0  0.04  81.38  0.06  6.9   -0.01  -0.11  -0.01   0.1   0.03  0.0\n",
      " -0.0   0.03  0.0   0.04  0.0  0.01  81.15  0.07  7.44   0.0   -0.03   0.05  -0.04  0.02  0.0\n",
      "  0.01  0.02  0.0   0.0   0.0  0.06  71.12  0.16  7.07  -0.01  -0.02   0.01  -0.0   0.02  0.0\n",
      " -0.0   0.04  0.0   0.06  0.0  0.01  72.75  0.12  7.73   0.0   -0.04   0.05  -0.06  0.05  0.0\n",
      "  0.05  0.02  0.02  0.01  0.0  0.08  65.95  0.08  6.93  -0.05  -0.02  -0.02  -0.01  0.1   0.0\n",
      "\n",
      "Min Loss train: 7.56e-01 val: 7.27e-01\n",
      " update plot 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 7.56e-01 val: 7.27e-01 grad: 8.86e+00 lr: 2.0e-03 0.0%┣┫ 1/2.5k [00:26<Inf:Inf, InfGs/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.05  0.0   0.05  0.0   0.0  0.07  65.32  0.08  6.67  -0.05  -0.0   -0.05   0.09  0.01  0.0\n",
      "  0.04  0.0   0.0   0.01  0.0  0.03  65.64  0.12  7.14  -0.04   0.01  -0.0   -0.01  0.05  0.0\n",
      "  0.03  0.04  0.02  0.0   0.0  0.02  74.04  0.04  7.14  -0.03  -0.04  -0.02   0.06  0.04  0.0\n",
      "  0.02  0.12  0.01  0.0   0.0  0.05  76.51  0.05  6.4   -0.02  -0.12  -0.01   0.12  0.03  0.0\n",
      " -0.0   0.02  0.0   0.05  0.0  0.01  76.1   0.07  6.92   0.0   -0.02   0.06  -0.05  0.02  0.0\n",
      "  0.02  0.01  0.0   0.0   0.0  0.05  66.63  0.16  6.58  -0.02  -0.01   0.01   0.0   0.02  0.0\n",
      " -0.0   0.04  0.0   0.08  0.0  0.01  68.15  0.12  7.2    0.0   -0.04   0.06  -0.08  0.06  0.0\n",
      "  0.06  0.02  0.02  0.0   0.0  0.08  61.16  0.08  6.52  -0.06  -0.02  -0.02  -0.0   0.1   0.0\n",
      "\n",
      "Min Loss train: 6.35e-01 val: 5.98e-01\n",
      " update plot 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 6.35e-01 val: 5.98e-01 grad: 1.16e+01 lr: 2.0e-03 0.1%┣┫ 2/2.5k [00:27<18:32:08, 27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.05  0.0   0.04  0.0   0.0  0.08  65.76  0.08  6.67  -0.05   0.0   -0.04   0.08  0.01  0.0\n",
      "  0.05  0.0   0.0   0.01  0.0  0.03  65.72  0.12  7.17  -0.05   0.01   0.0   -0.01  0.05  0.0\n",
      "  0.04  0.04  0.02  0.0   0.0  0.03  74.92  0.04  7.1   -0.04  -0.04  -0.02   0.07  0.03  0.0\n",
      "  0.03  0.13  0.02  0.0   0.0  0.06  77.45  0.04  6.36  -0.03  -0.13  -0.02   0.15  0.03  0.0\n",
      " -0.0   0.02  0.0   0.07  0.0  0.01  76.61  0.07  6.93   0.0   -0.02   0.07  -0.07  0.01  0.0\n",
      "  0.03  0.0   0.0   0.0   0.0  0.05  67.24  0.15  6.56  -0.03  -0.0    0.02  -0.0   0.01  0.0\n",
      " -0.0   0.03  0.0   0.1   0.0  0.01  68.69  0.12  7.2    0.0   -0.03   0.07  -0.1   0.06  0.0\n",
      "  0.07  0.01  0.01  0.01  0.0  0.08  61.48  0.08  6.52  -0.07  -0.01  -0.01  -0.01  0.1   0.0\n",
      "\n",
      "Min Loss train: 5.64e-01 val: 5.23e-01\n",
      " update plot 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 5.64e-01 val: 5.23e-01 grad: 1.82e+01 lr: 2.0e-03 0.1%┣┫ 3/2.5k [00:28<09:40:49, 14s/it]\n",
      "Loss train: 5.80e-01 val: 5.77e-01 grad: 1.75e+01 lr: 2.0e-03 0.2%┣┫ 4/2.5k [00:29<06:36:02, 10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.06  0.0   0.04  0.0   0.0  0.07  67.71  0.08  6.83  -0.06   0.0   -0.04   0.1   0.0   0.0\n",
      "  0.06  0.0   0.0   0.02  0.0  0.03  67.84  0.11  7.31  -0.06   0.02   0.01  -0.02  0.05  0.0\n",
      "  0.05  0.06  0.02  0.0   0.0  0.02  76.64  0.04  7.32  -0.05  -0.06  -0.02   0.12  0.02  0.0\n",
      "  0.04  0.14  0.01  0.0   0.0  0.04  78.63  0.06  6.62  -0.04  -0.14  -0.01   0.18  0.01  0.0\n",
      " -0.0   0.02  0.0   0.07  0.0  0.02  79.16  0.06  7.06   0.0   -0.02   0.08  -0.07  0.01  0.0\n",
      "  0.03  0.01  0.0   0.0   0.0  0.06  69.51  0.15  6.68  -0.03  -0.01   0.02   0.01  0.01  0.0\n",
      " -0.0   0.03  0.0   0.11  0.0  0.02  71.19  0.11  7.32   0.0   -0.03   0.08  -0.11  0.06  0.0\n",
      "  0.07  0.01  0.0   0.02  0.0  0.07  63.51  0.07  6.64  -0.07  -0.01  -0.0   -0.02  0.1   0.0\n",
      "\n",
      "Min Loss train: 5.21e-01 val: 5.02e-01\n",
      " update plot 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 5.21e-01 val: 5.02e-01 grad: 1.29e+01 lr: 2.0e-03 0.2%┣┫ 5/2.5k [00:30<05:06:52, 7s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.07  0.0   0.04  0.0   0.0  0.07  66.8   0.07  6.72  -0.07   0.0   -0.04   0.11  0.0   0.0\n",
      "  0.06  0.0   0.0   0.01  0.0  0.02  66.76  0.11  7.21  -0.06   0.02   0.01  -0.01  0.05  0.0\n",
      "  0.06  0.07  0.02  0.0   0.0  0.01  75.33  0.04  7.22  -0.06  -0.07  -0.02   0.13  0.02  0.0\n",
      "  0.05  0.15  0.01  0.0   0.0  0.04  77.11  0.06  6.56  -0.05  -0.15  -0.01   0.2   0.01  0.0\n",
      " -0.0   0.01  0.0   0.08  0.0  0.03  78.11  0.06  6.94   0.0   -0.01   0.08  -0.08  0.01  0.0\n",
      "  0.04  0.01  0.0   0.0   0.0  0.06  68.62  0.15  6.56  -0.04  -0.01   0.02   0.02  0.01  0.0\n",
      " -0.0   0.02  0.0   0.12  0.0  0.03  70.3   0.1   7.19   0.0   -0.02   0.08  -0.12  0.06  0.0\n",
      "  0.08  0.01  0.0   0.01  0.0  0.07  62.58  0.07  6.54  -0.08  -0.01  -0.0   -0.01  0.1   0.0\n",
      "\n",
      "Min Loss train: 5.01e-01 val: 4.68e-01\n",
      " update plot 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 5.01e-01 val: 4.68e-01 grad: 1.68e+01 lr: 2.0e-03 0.2%┣┫ 6/2.5k [00:31<04:13:45, 6s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.08  0.0   0.04  0.0   0.0  0.07  67.24  0.07  6.69  -0.08   0.01  -0.04   0.11  0.0   0.0\n",
      "  0.06  0.0   0.0   0.02  0.0  0.02  66.84  0.11  7.21  -0.06   0.02   0.01  -0.02  0.05  0.0\n",
      "  0.07  0.08  0.02  0.0   0.0  0.01  75.41  0.04  7.23  -0.07  -0.08  -0.02   0.15  0.02  0.0\n",
      "  0.05  0.16  0.01  0.0   0.0  0.03  77.02  0.06  6.59  -0.05  -0.16  -0.01   0.21  0.0   0.0\n",
      " -0.0   0.01  0.0   0.09  0.0  0.03  78.25  0.06  6.95   0.0   -0.01   0.08  -0.09  0.01  0.0\n",
      "  0.04  0.01  0.0   0.0   0.0  0.06  69.06  0.14  6.54  -0.04  -0.01   0.02   0.02  0.01  0.0\n",
      " -0.0   0.02  0.0   0.13  0.0  0.03  70.34  0.11  7.21   0.0   -0.02   0.08  -0.13  0.07  0.0\n",
      "  0.08  0.01  0.0   0.02  0.0  0.07  62.65  0.07  6.55  -0.08  -0.01   0.0   -0.02  0.11  0.0\n",
      "\n",
      "Min Loss train: 4.82e-01 val: 4.53e-01\n",
      " update plot 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 4.82e-01 val: 4.53e-01 grad: 1.87e+01 lr: 2.0e-03 0.3%┣┫ 7/2.5k [00:31<03:37:44, 5s/it]\n",
      "Loss train: 4.96e-01 val: 4.98e-01 grad: 1.70e+01 lr: 2.0e-03 0.3%┣┫ 8/2.5k [00:32<03:12:47, 5s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.1   0.0   0.04  0.0   0.0  0.06  67.39  0.07  6.69  -0.1    0.01  -0.04   0.13  0.0   0.0\n",
      "  0.07  0.0   0.0   0.02  0.0  0.02  67.18  0.11  7.19  -0.07   0.02   0.01  -0.02  0.06  0.0\n",
      "  0.08  0.08  0.03  0.0   0.0  0.0   74.86  0.05  7.31  -0.08  -0.08  -0.03   0.17  0.02  0.0\n",
      "  0.04  0.16  0.02  0.0   0.0  0.02  76.4   0.07  6.68  -0.04  -0.16  -0.02   0.22  0.0   0.0\n",
      " -0.0   0.01  0.0   0.09  0.0  0.04  78.7   0.06  6.94   0.0   -0.01   0.09  -0.09  0.01  0.0\n",
      "  0.03  0.02  0.0   0.0   0.0  0.07  69.79  0.13  6.48  -0.03  -0.02   0.02   0.02  0.01  0.0\n",
      " -0.0   0.02  0.0   0.14  0.0  0.04  70.69  0.1   7.2    0.0   -0.02   0.08  -0.14  0.07  0.0\n",
      "  0.09  0.01  0.0   0.01  0.0  0.06  62.75  0.07  6.55  -0.09  -0.01  -0.0   -0.01  0.11  0.0\n",
      "\n",
      "Min Loss train: 4.49e-01 val: 4.06e-01\n",
      " update plot 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 4.49e-01 val: 4.06e-01 grad: 1.44e+01 lr: 2.0e-03 0.4%┣┫ 9/2.5k [00:34<02:54:46, 4s/it]\n",
      "Loss train: 4.35e-01 val: 4.13e-01 grad: 1.69e+01 lr: 2.0e-03 0.4%┣┫ 10/2.5k [00:35<02:40:14, 4s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.11  0.0   0.04  0.0   0.0  0.05  67.54  0.07  6.68  -0.11   0.01  -0.04   0.15  0.0   0.0\n",
      "  0.07  0.0   0.0   0.03  0.0  0.01  67.46  0.1   7.17  -0.07   0.03   0.01  -0.03  0.07  0.0\n",
      "  0.09  0.08  0.04  0.0   0.0  0.01  74.52  0.05  7.36  -0.09  -0.08  -0.04   0.18  0.02  0.0\n",
      "  0.04  0.16  0.02  0.0   0.0  0.02  76.15  0.07  6.72  -0.04  -0.16  -0.02   0.21  0.01  0.0\n",
      " -0.0   0.01  0.0   0.09  0.0  0.05  78.74  0.06  6.95   0.0   -0.01   0.09  -0.09  0.01  0.0\n",
      "  0.03  0.02  0.0   0.0   0.0  0.08  70.29  0.13  6.44  -0.03  -0.02   0.02   0.03  0.01  0.0\n",
      " -0.0   0.02  0.0   0.14  0.0  0.04  70.72  0.11  7.22   0.0   -0.02   0.08  -0.14  0.07  0.0\n",
      "  0.09  0.01  0.0   0.01  0.0  0.05  62.93  0.07  6.55  -0.09  -0.01   0.0   -0.01  0.11  0.0\n",
      "\n",
      "Min Loss train: 4.18e-01 val: 3.79e-01\n",
      " update plot 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 4.18e-01 val: 3.79e-01 grad: 1.50e+01 lr: 2.0e-03 0.4%┣┫ 11/2.5k [00:36<02:28:06, 4s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.12  0.0   0.04  0.0   0.0  0.05  67.54  0.07  6.68  -0.12   0.01  -0.04   0.15  0.0   0.0\n",
      "  0.07  0.0   0.0   0.04  0.0  0.01  67.53  0.1   7.17  -0.07   0.03   0.01  -0.04  0.07  0.0\n",
      "  0.09  0.08  0.04  0.0   0.0  0.0   74.3   0.05  7.38  -0.09  -0.08  -0.04   0.19  0.02  0.0\n",
      "  0.04  0.16  0.03  0.0   0.0  0.02  76.01  0.08  6.73  -0.04  -0.16  -0.03   0.22  0.01  0.0\n",
      " -0.0   0.01  0.0   0.09  0.0  0.05  78.81  0.06  6.95   0.0   -0.01   0.09  -0.09  0.01  0.0\n",
      "  0.03  0.02  0.0   0.0   0.0  0.08  70.47  0.12  6.42  -0.03  -0.02   0.02   0.03  0.01  0.0\n",
      " -0.0   0.02  0.0   0.14  0.0  0.05  70.76  0.11  7.23   0.0   -0.02   0.09  -0.14  0.07  0.0\n",
      "  0.09  0.01  0.0   0.02  0.0  0.04  63.03  0.07  6.54  -0.09  -0.01   0.01  -0.02  0.12  0.0\n",
      "\n",
      "Min Loss train: 4.04e-01 val: 3.68e-01\n",
      " update plot 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 4.04e-01 val: 3.68e-01 grad: 2.32e+01 lr: 2.0e-03 0.5%┣┫ 12/2.5k [00:37<02:18:44, 3s/it]\n",
      "Loss train: 4.09e-01 val: 4.05e-01 grad: 1.30e+01 lr: 2.0e-03 0.5%┣┫ 13/2.5k [00:38<02:10:20, 3s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.13  0.0   0.03  0.0   0.0  0.05  67.73  0.06  6.72  -0.13   0.01  -0.03   0.15  0.0   0.0\n",
      "  0.07  0.0   0.0   0.05  0.0  0.0   67.91  0.1   7.18  -0.07   0.03   0.01  -0.05  0.07  0.0\n",
      "  0.1   0.08  0.04  0.0   0.0  0.0   74.25  0.06  7.44  -0.1   -0.08  -0.04   0.2   0.03  0.0\n",
      "  0.04  0.16  0.03  0.0   0.0  0.01  76.18  0.08  6.78  -0.04  -0.16  -0.03   0.22  0.01  0.0\n",
      " -0.0   0.01  0.0   0.11  0.0  0.06  79.31  0.05  6.96   0.0   -0.01   0.1   -0.11  0.02  0.0\n",
      "  0.03  0.03  0.0   0.0   0.0  0.08  71.08  0.12  6.41  -0.03  -0.03   0.02   0.04  0.0   0.0\n",
      " -0.0   0.02  0.0   0.16  0.0  0.05  71.1   0.11  7.25   0.0   -0.02   0.09  -0.16  0.08  0.0\n",
      "  0.1   0.01  0.0   0.03  0.0  0.04  63.51  0.06  6.54  -0.1   -0.01   0.01  -0.03  0.12  0.0\n",
      "\n",
      "Min Loss train: 3.84e-01 val: 3.61e-01\n",
      " update plot 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 3.84e-01 val: 3.61e-01 grad: 1.50e+01 lr: 2.0e-03 0.6%┣┫ 14/2.5k [00:39<02:03:31, 3s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.13  0.0   0.03  0.0   0.0  0.04  67.02  0.06  6.65  -0.13   0.01  -0.03   0.15  0.01  0.0\n",
      "  0.07  0.0   0.0   0.06  0.0  0.0   67.21  0.1   7.11  -0.07   0.04   0.02  -0.06  0.08  0.0\n",
      "  0.1   0.09  0.04  0.0   0.0  0.0   73.18  0.06  7.39  -0.1   -0.09  -0.04   0.2   0.03  0.0\n",
      "  0.04  0.16  0.03  0.0   0.0  0.0   75.03  0.08  6.74  -0.04  -0.16  -0.03   0.22  0.02  0.0\n",
      " -0.0   0.01  0.0   0.11  0.0  0.06  78.49  0.05  6.89   0.0   -0.01   0.1   -0.11  0.02  0.0\n",
      "  0.03  0.03  0.0   0.0   0.0  0.08  70.49  0.12  6.33  -0.03  -0.03   0.02   0.04  0.0   0.0\n",
      " -0.0   0.02  0.0   0.16  0.0  0.05  70.25  0.11  7.19   0.0   -0.02   0.1   -0.16  0.08  0.0\n",
      "  0.1   0.01  0.0   0.03  0.0  0.03  62.82  0.06  6.48  -0.1   -0.01   0.02  -0.03  0.13  0.0\n",
      "\n",
      "Min Loss train: 3.70e-01 val: 3.34e-01\n",
      " update plot 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 3.70e-01 val: 3.34e-01 grad: 1.91e+01 lr: 2.0e-03 0.6%┣┫ 15/2.5k [00:40<01:57:47, 3s/it]\n",
      "Loss train: 3.63e-01 val: 3.41e-01 grad: 1.77e+01 lr: 2.0e-03 0.6%┣┫ 16/2.5k [00:41<01:52:13, 3s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.14  0.0   0.03  0.0   0.0  0.04  66.8   0.07  6.65  -0.14   0.01  -0.03   0.16  0.0   0.0\n",
      "  0.07  0.0   0.0   0.07  0.0  0.0   67.21  0.1   7.09  -0.07   0.04   0.02  -0.07  0.08  0.0\n",
      "  0.11  0.09  0.05  0.0   0.0  0.0   72.47  0.07  7.44  -0.11  -0.09  -0.05   0.21  0.03  0.0\n",
      "  0.04  0.16  0.04  0.0   0.0  0.01  74.52  0.09  6.77  -0.04  -0.16  -0.04   0.22  0.02  0.0\n",
      " -0.0   0.01  0.0   0.12  0.0  0.07  78.71  0.05  6.84   0.0   -0.01   0.11  -0.12  0.02  0.0\n",
      "  0.03  0.03  0.0   0.0   0.0  0.09  70.69  0.11  6.29  -0.03  -0.03   0.02   0.04  0.0   0.0\n",
      " -0.0   0.01  0.0   0.17  0.0  0.05  70.34  0.1   7.16   0.0   -0.01   0.1   -0.17  0.08  0.0\n",
      "  0.1   0.01  0.0   0.05  0.0  0.03  62.91  0.06  6.45  -0.1   -0.01   0.02  -0.05  0.14  0.0\n",
      "\n",
      "Min Loss train: 3.48e-01 val: 3.04e-01\n",
      " update plot 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 3.48e-01 val: 3.04e-01 grad: 1.78e+01 lr: 2.0e-03 0.7%┣┫ 17/2.5k [00:42<01:47:23, 3s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.15  0.0   0.03  0.0   0.0  0.03  66.74  0.06  6.63  -0.15   0.01  -0.03   0.16  0.0   0.0\n",
      "  0.07  0.0   0.0   0.07  0.0  0.0   67.04  0.1   7.07  -0.07   0.04   0.02  -0.07  0.09  0.0\n",
      "  0.12  0.09  0.05  0.0   0.0  0.0   72.18  0.07  7.43  -0.12  -0.09  -0.05   0.22  0.04  0.0\n",
      "  0.04  0.17  0.04  0.0   0.0  0.0   74.3   0.09  6.76  -0.04  -0.17  -0.04   0.23  0.03  0.0\n",
      " -0.0   0.01  0.0   0.13  0.0  0.07  78.48  0.05  6.83   0.0   -0.01   0.11  -0.13  0.02  0.0\n",
      "  0.03  0.03  0.0   0.0   0.0  0.09  70.76  0.11  6.25  -0.03  -0.03   0.02   0.04  0.0   0.0\n",
      " -0.0   0.01  0.0   0.18  0.0  0.05  70.0   0.11  7.16   0.0   -0.01   0.11  -0.18  0.09  0.0\n",
      "  0.1   0.01  0.0   0.05  0.0  0.02  62.77  0.06  6.44  -0.1   -0.01   0.02  -0.05  0.14  0.0\n",
      "\n",
      "Min Loss train: 3.39e-01 val: 2.92e-01\n",
      " update plot 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 3.39e-01 val: 2.92e-01 grad: 2.07e+01 lr: 2.0e-03 0.7%┣┫ 18/2.5k [00:43<01:43:43, 3s/it]\n",
      "Loss train: 3.35e-01 val: 3.06e-01 grad: 2.58e+01 lr: 2.0e-03 0.8%┣┫ 19/2.5k [00:43<01:39:52, 2s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.16  0.0   0.02  0.0   0.0  0.03  67.31  0.06  6.69  -0.16   0.01  -0.02   0.17  0.0   0.0\n",
      "  0.08  0.0   0.0   0.08  0.0  0.0   67.71  0.09  7.12  -0.08   0.05   0.02  -0.08  0.09  0.0\n",
      "  0.13  0.09  0.05  0.0   0.0  0.0   72.34  0.07  7.54  -0.13  -0.09  -0.05   0.23  0.04  0.0\n",
      "  0.04  0.17  0.05  0.0   0.0  0.0   74.6   0.09  6.84  -0.04  -0.17  -0.05   0.24  0.03  0.0\n",
      " -0.0   0.0   0.0   0.14  0.0  0.07  79.33  0.05  6.86   0.0   -0.0    0.12  -0.14  0.02  0.0\n",
      "  0.04  0.03  0.0   0.0   0.0  0.09  71.61  0.11  6.27  -0.04  -0.03   0.02   0.05  0.0   0.0\n",
      " -0.0   0.01  0.0   0.2   0.0  0.06  70.56  0.11  7.21   0.0   -0.01   0.12  -0.2   0.09  0.0\n",
      "  0.11  0.0   0.0   0.06  0.0  0.01  63.45  0.06  6.48  -0.11  -0.0    0.03  -0.06  0.15  0.0\n",
      "\n",
      "Min Loss train: 3.23e-01 val: 2.86e-01\n",
      " update plot 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 3.23e-01 val: 2.86e-01 grad: 1.79e+01 lr: 2.0e-03 0.8%┣┫ 20/2.5k [00:45<01:36:50, 2s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.16  0.0   0.02  0.0   0.0  0.02  67.08  0.06  6.65  -0.16   0.01  -0.02   0.17  0.0   0.0\n",
      "  0.08  0.0   0.0   0.09  0.0  0.0   67.39  0.09  7.09  -0.08   0.05   0.02  -0.09  0.09  0.0\n",
      "  0.14  0.09  0.05  0.0   0.0  0.0   71.96  0.07  7.51  -0.14  -0.09  -0.05   0.24  0.04  0.0\n",
      "  0.04  0.18  0.05  0.0   0.0  0.0   74.24  0.09  6.82  -0.04  -0.18  -0.05   0.24  0.03  0.0\n",
      " -0.0   0.0   0.0   0.15  0.0  0.08  78.92  0.05  6.84   0.0   -0.0    0.13  -0.15  0.02  0.0\n",
      "  0.04  0.03  0.0   0.0   0.0  0.1   71.44  0.11  6.23  -0.04  -0.03   0.02   0.05  0.0   0.0\n",
      " -0.0   0.01  0.0   0.2   0.0  0.06  70.08  0.11  7.2    0.0   -0.01   0.12  -0.2   0.09  0.0\n",
      "  0.11  0.0   0.0   0.07  0.0  0.01  63.15  0.06  6.45  -0.11  -0.0    0.03  -0.07  0.15  0.0\n",
      "\n",
      "Min Loss train: 3.14e-01 val: 2.68e-01\n",
      " update plot 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 3.14e-01 val: 2.68e-01 grad: 2.23e+01 lr: 2.0e-03 0.8%┣┫ 21/2.5k [00:45<01:33:56, 2s/it]\n",
      "Loss train: 3.12e-01 val: 2.72e-01 grad: 2.49e+01 lr: 2.0e-03 0.9%┣┫ 22/2.5k [00:46<01:31:05, 2s/it]\n",
      "Loss train: 3.02e-01 val: 2.69e-01 grad: 1.85e+01 lr: 2.0e-03 0.9%┣┫ 23/2.5k [00:47<01:28:24, 2s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.17  0.0   0.02  0.0   0.0  0.01  67.35  0.06  6.67  -0.17   0.01  -0.02   0.18  0.0   0.0\n",
      "  0.08  0.0   0.0   0.1   0.0  0.0   67.71  0.09  7.11  -0.08   0.05   0.02  -0.1   0.1   0.0\n",
      "  0.15  0.09  0.05  0.0   0.0  0.0   71.83  0.08  7.58  -0.15  -0.09  -0.05   0.25  0.05  0.0\n",
      "  0.03  0.18  0.06  0.0   0.0  0.01  74.37  0.09  6.85  -0.03  -0.18  -0.06   0.24  0.03  0.0\n",
      " -0.0   0.0   0.0   0.16  0.0  0.08  79.09  0.05  6.86   0.0   -0.0    0.14  -0.16  0.03  0.0\n",
      "  0.04  0.03  0.0   0.0   0.0  0.1   71.94  0.1   6.22  -0.04  -0.03   0.02   0.05  0.0   0.0\n",
      " -0.0   0.01  0.0   0.21  0.0  0.06  69.95  0.11  7.24   0.0   -0.01   0.13  -0.21  0.1   0.0\n",
      "  0.11  0.0   0.0   0.08  0.0  0.0   63.5   0.06  6.46  -0.11  -0.0    0.03  -0.08  0.16  0.0\n",
      "\n",
      "Min Loss train: 2.99e-01 val: 2.54e-01\n",
      " update plot 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 2.99e-01 val: 2.54e-01 grad: 2.52e+01 lr: 2.0e-03 1.0%┣┫ 24/2.5k [00:48<01:26:14, 2s/it]\n",
      "Loss train: 3.12e-01 val: 3.02e-01 grad: 2.10e+01 lr: 2.0e-03 1.0%┣┫ 25/2.5k [00:49<01:24:07, 2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.18  0.0   0.02  0.0   0.0  0.01  67.49  0.06  6.64  -0.18   0.01  -0.02   0.18  0.01  0.0\n",
      "  0.08  0.0   0.0   0.11  0.0  0.0   67.61  0.09  7.1   -0.08   0.06   0.02  -0.11  0.11  0.0\n",
      "  0.16  0.09  0.06  0.0   0.0  0.0   71.69  0.08  7.58  -0.16  -0.09  -0.06   0.25  0.06  0.0\n",
      "  0.03  0.19  0.07  0.0   0.0  0.01  74.18  0.09  6.85  -0.03  -0.19  -0.07   0.24  0.04  0.0\n",
      " -0.0   0.0   0.0   0.17  0.0  0.08  78.91  0.05  6.86   0.0   -0.0    0.14  -0.17  0.03  0.0\n",
      "  0.04  0.04  0.0   0.0   0.0  0.11  72.15  0.1   6.18  -0.04  -0.04   0.02   0.06  0.0   0.0\n",
      " -0.0   0.01  0.0   0.22  0.0  0.06  69.46  0.11  7.27   0.0   -0.01   0.13  -0.22  0.1   0.0\n",
      "  0.11  0.0   0.0   0.1   0.0  0.0   63.31  0.06  6.47  -0.11  -0.0    0.03  -0.1   0.17  0.0\n",
      "\n",
      "Min Loss train: 2.88e-01 val: 2.44e-01\n",
      " update plot 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 2.88e-01 val: 2.44e-01 grad: 1.77e+01 lr: 2.0e-03 1.0%┣┫ 26/2.5k [00:50<01:22:15, 2s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.19  0.0   0.02  0.0   0.0  0.0   66.47  0.06  6.56  -0.19   0.01  -0.02   0.19  0.01  0.0\n",
      "  0.08  0.0   0.0   0.11  0.0  0.0   66.64  0.09  7.01  -0.08   0.06   0.02  -0.11  0.11  0.0\n",
      "  0.16  0.09  0.06  0.0   0.0  0.0   70.53  0.08  7.49  -0.16  -0.09  -0.06   0.25  0.06  0.0\n",
      "  0.03  0.19  0.07  0.0   0.0  0.0   73.07  0.09  6.77  -0.03  -0.19  -0.07   0.24  0.04  0.0\n",
      " -0.0   0.0   0.0   0.17  0.0  0.09  77.9   0.05  6.75   0.0   -0.0    0.15  -0.17  0.03  0.0\n",
      "  0.04  0.04  0.0   0.0   0.0  0.11  71.23  0.1   6.09  -0.04  -0.04   0.01   0.06  0.0   0.0\n",
      " -0.0   0.01  0.0   0.22  0.0  0.06  68.51  0.11  7.17   0.0   -0.01   0.13  -0.22  0.1   0.0\n",
      "  0.11  0.0   0.0   0.1   0.0  0.0   62.37  0.06  6.39  -0.11   0.0    0.04  -0.1   0.17  0.0\n",
      "\n",
      "Min Loss train: 2.85e-01 val: 2.04e-01\n",
      " update plot 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 2.85e-01 val: 2.04e-01 grad: 2.20e+01 lr: 2.0e-03 1.1%┣┫ 27/2.5k [00:51<01:20:35, 2s/it]\n",
      "Loss train: 2.76e-01 val: 2.21e-01 grad: 2.50e+01 lr: 2.0e-03 1.1%┣┫ 28/2.5k [00:52<01:18:55, 2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.19  0.0   0.01  0.0   0.0  0.0   66.27  0.06  6.53  -0.19   0.01  -0.01   0.19  0.01  0.0\n",
      "  0.08  0.0   0.0   0.12  0.0  0.0   66.37  0.09  6.97  -0.08   0.06   0.02  -0.12  0.11  0.0\n",
      "  0.17  0.09  0.05  0.0   0.0  0.0   70.01  0.08  7.48  -0.17  -0.09  -0.05   0.25  0.06  0.0\n",
      "  0.02  0.19  0.07  0.0   0.0  0.01  72.2   0.1   6.78  -0.02  -0.19  -0.07   0.24  0.04  0.0\n",
      " -0.0   0.0   0.0   0.18  0.0  0.09  77.5   0.05  6.71   0.0   -0.0    0.15  -0.18  0.03  0.0\n",
      "  0.04  0.04  0.0   0.0   0.0  0.11  70.98  0.09  6.05  -0.04  -0.04   0.01   0.06  0.0   0.0\n",
      " -0.0   0.01  0.0   0.23  0.0  0.07  67.83  0.12  7.15   0.0   -0.01   0.14  -0.23  0.11  0.0\n",
      "  0.11  0.0   0.0   0.11  0.0  0.0   61.94  0.06  6.37  -0.11   0.0    0.04  -0.11  0.18  0.0\n",
      "\n",
      "Min Loss train: 2.76e-01 val: 2.00e-01\n",
      " update plot 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 2.76e-01 val: 2.00e-01 grad: 2.51e+01 lr: 2.0e-03 1.2%┣┫ 29/2.5k [00:53<01:17:42, 2s/it]\n",
      "Loss train: 2.89e-01 val: 2.69e-01 grad: 2.63e+01 lr: 2.0e-03 1.2%┣┫ 30/2.5k [00:54<01:16:14, 2s/it]\n",
      "Loss train: 2.70e-01 val: 2.01e-01 grad: 2.49e+01 lr: 2.0e-03 1.2%┣┫ 31/2.5k [00:55<01:14:59, 2s/it]\n",
      "Loss train: 2.66e-01 val: 2.11e-01 grad: 2.32e+01 lr: 2.0e-03 1.3%┣┫ 32/2.5k [00:56<01:13:45, 2s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.21  0.0   0.01  0.0   0.0  0.0   66.29  0.06  6.53  -0.21   0.01  -0.01   0.2   0.01  0.0\n",
      "  0.08  0.0   0.0   0.13  0.0  0.0   66.44  0.09  6.97  -0.08   0.07   0.03  -0.13  0.12  0.0\n",
      "  0.18  0.1   0.05  0.0   0.0  0.0   69.65  0.09  7.53  -0.18  -0.1   -0.05   0.26  0.07  0.0\n",
      "  0.02  0.2   0.08  0.0   0.0  0.02  72.0   0.1   6.78  -0.02  -0.2   -0.08   0.26  0.04  0.0\n",
      " -0.0   0.0   0.0   0.19  0.0  0.1   77.24  0.05  6.72   0.0   -0.0    0.16  -0.19  0.03  0.0\n",
      "  0.04  0.04  0.0   0.0   0.0  0.11  71.25  0.09  6.01  -0.04  -0.04   0.01   0.06  0.0   0.0\n",
      " -0.0   0.02  0.0   0.24  0.0  0.08  67.22  0.12  7.19   0.0   -0.02   0.15  -0.24  0.11  0.0\n",
      "  0.11  0.0   0.0   0.13  0.0  0.0   61.83  0.07  6.39  -0.11   0.01   0.04  -0.13  0.19  0.0\n",
      "\n",
      "Min Loss train: 2.64e-01 val: 1.87e-01\n",
      " update plot 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 2.64e-01 val: 1.87e-01 grad: 2.81e+01 lr: 2.0e-03 1.3%┣┫ 33/2.5k [00:57<01:12:47, 2s/it]\n",
      "Loss train: 2.65e-01 val: 2.32e-01 grad: 2.48e+01 lr: 2.0e-03 1.4%┣┫ 34/2.5k [00:58<01:11:51, 2s/it]\n",
      "Loss train: 2.59e-01 val: 2.21e-01 grad: 1.92e+01 lr: 2.0e-03 1.4%┣┫ 35/2.5k [00:59<01:10:50, 2s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.21  0.0   0.01  0.0   0.0  0.0   66.44  0.06  6.53  -0.21   0.01  -0.01   0.21  0.01  0.0\n",
      "  0.08  0.0   0.0   0.14  0.0  0.0   66.47  0.09  6.98  -0.08   0.07   0.03  -0.14  0.12  0.0\n",
      "  0.19  0.1   0.06  0.0   0.0  0.0   69.39  0.09  7.57  -0.19  -0.1   -0.06   0.27  0.08  0.0\n",
      "  0.02  0.21  0.08  0.0   0.0  0.03  71.86  0.1   6.79  -0.02  -0.21  -0.08   0.27  0.04  0.0\n",
      " -0.0   0.0   0.0   0.2   0.0  0.1   77.09  0.05  6.72   0.0   -0.0    0.17  -0.2   0.03  0.0\n",
      "  0.04  0.04  0.0   0.0   0.0  0.12  71.42  0.09  5.99  -0.04  -0.04   0.01   0.07  0.0   0.0\n",
      " -0.0   0.02  0.0   0.25  0.0  0.09  66.72  0.12  7.23   0.0   -0.02   0.16  -0.25  0.11  0.0\n",
      "  0.11  0.0   0.0   0.15  0.0  0.0   61.76  0.07  6.4   -0.11   0.01   0.04  -0.15  0.2   0.0\n",
      "\n",
      "Min Loss train: 2.58e-01 val: 1.76e-01\n",
      " update plot 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 2.58e-01 val: 1.76e-01 grad: 2.68e+01 lr: 2.0e-03 1.4%┣┫ 36/2.5k [01:00<01:10:10, 2s/it]\n",
      "Loss train: 2.52e-01 val: 1.98e-01 grad: 2.66e+01 lr: 2.0e-03 1.5%┣┫ 37/2.5k [01:01<01:09:09, 2s/it]\n",
      "Loss train: 2.57e-01 val: 2.31e-01 grad: 2.41e+01 lr: 2.0e-03 1.5%┣┫ 38/2.5k [01:02<01:08:13, 2s/it]\n",
      "Loss train: 2.44e-01 val: 1.85e-01 grad: 2.38e+01 lr: 2.0e-03 1.6%┣┫ 39/2.5k [01:02<01:07:22, 2s/it]\n",
      "Loss train: 2.40e-01 val: 1.88e-01 grad: 2.69e+01 lr: 2.0e-03 1.6%┣┫ 40/2.5k [01:03<01:06:42, 2s/it]\n",
      "Loss train: 2.37e-01 val: 1.93e-01 grad: 2.58e+01 lr: 2.0e-03 1.6%┣┫ 41/2.5k [01:04<01:06:02, 2s/it]\n",
      "Loss train: 2.33e-01 val: 1.93e-01 grad: 2.68e+01 lr: 2.0e-03 1.7%┣┫ 42/2.5k [01:05<01:05:25, 2s/it]\n",
      "Loss train: 2.37e-01 val: 2.16e-01 grad: 2.09e+01 lr: 2.0e-03 1.7%┣┫ 43/2.5k [01:06<01:04:48, 2s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.22  0.0   0.01  0.0   0.0  0.0   66.45  0.06  6.45  -0.22   0.0   -0.01   0.22  0.01  0.0\n",
      "  0.08  0.0   0.0   0.17  0.0  0.0   65.9   0.1   6.94  -0.08   0.08   0.03  -0.17  0.14  0.0\n",
      "  0.21  0.09  0.07  0.0   0.0  0.0   68.0   0.11  7.61  -0.21  -0.09  -0.07   0.29  0.09  0.0\n",
      "  0.01  0.2   0.1   0.0   0.0  0.05  70.96  0.1   6.74  -0.01  -0.2   -0.1    0.28  0.04  0.0\n",
      " -0.0   0.0   0.0   0.22  0.0  0.12  75.96  0.05  6.69   0.0   -0.0    0.19  -0.22  0.04  0.0\n",
      "  0.04  0.05  0.0   0.0   0.0  0.12  71.05  0.08  5.92  -0.04  -0.05   0.01   0.08  0.0   0.0\n",
      " -0.0   0.02  0.0   0.25  0.0  0.12  64.42  0.14  7.3    0.0   -0.02   0.18  -0.25  0.1   0.0\n",
      "  0.11  0.0   0.0   0.18  0.0  0.0   61.02  0.07  6.4   -0.11   0.03   0.04  -0.18  0.22  0.0\n",
      "\n",
      "Min Loss train: 2.33e-01 val: 1.71e-01\n",
      " update plot 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 2.39e-01 val: 1.71e-01 grad: 2.30e+01 lr: 2.0e-03 1.8%┣┫ 44/2.5k [01:08<01:04:22, 2s/it]\n",
      "Loss train: 2.32e-01 val: 2.19e-01 grad: 2.35e+01 lr: 2.0e-03 1.8%┣┫ 45/2.5k [01:09<01:03:58, 2s/it]\n",
      "Loss train: 2.21e-01 val: 1.89e-01 grad: 2.53e+01 lr: 2.0e-03 1.8%┣┫ 46/2.5k [01:10<01:03:29, 2s/it]\n",
      "Loss train: 2.23e-01 val: 2.12e-01 grad: 1.83e+01 lr: 2.0e-03 1.9%┣┫ 47/2.5k [01:11<01:03:03, 2s/it]\n",
      "Loss train: 2.15e-01 val: 1.93e-01 grad: 1.89e+01 lr: 2.0e-03 1.9%┣┫ 48/2.5k [01:12<01:02:40, 2s/it]\n",
      "Loss train: 2.16e-01 val: 1.97e-01 grad: 2.74e+01 lr: 2.0e-03 2.0%┣┫ 49/2.5k [01:13<01:02:20, 2s/it]\n",
      "Loss train: 2.15e-01 val: 2.20e-01 grad: 1.89e+01 lr: 2.0e-03 2.0%┣┫ 50/2.5k [01:14<01:01:58, 2s/it]\n",
      "Loss train: 2.08e-01 val: 2.05e-01 grad: 1.87e+01 lr: 2.0e-03 2.0%┣┫ 51/2.5k [01:16<01:01:39, 2s/it]\n",
      "Loss train: 2.14e-01 val: 2.22e-01 grad: 2.25e+01 lr: 2.0e-03 2.1%┣┫ 52/2.5k [01:17<01:01:23, 2s/it]\n",
      "Loss train: 2.02e-01 val: 2.15e-01 grad: 1.55e+01 lr: 2.0e-03 2.1%┣┫ 53/2.5k [01:18<01:01:07, 1s/it]\n",
      "Loss train: 2.10e-01 val: 2.08e-01 grad: 2.76e+01 lr: 2.0e-03 2.2%┣┫ 54/2.5k [01:19<01:00:49, 1s/it]\n",
      "Loss train: 2.67e-01 val: 2.69e-01 grad: 1.67e+01 lr: 2.0e-03 2.2%┣┫ 55/2.5k [01:20<01:00:27, 1s/it]\n",
      "Loss train: 2.03e-01 val: 2.05e-01 grad: 1.81e+01 lr: 2.0e-03 2.2%┣┫ 56/2.5k [01:21<01:00:03, 1s/it]\n",
      "Loss train: 2.28e-01 val: 2.03e-01 grad: 3.34e+01 lr: 2.0e-03 2.3%┣┫ 57/2.5k [01:22<59:46, 1s/it]\n",
      "Loss train: 2.10e-01 val: 2.20e-01 grad: 2.16e+01 lr: 2.0e-03 2.3%┣┫ 58/2.5k [01:23<59:33, 1s/it]\n",
      "Loss train: 2.13e-01 val: 2.26e-01 grad: 1.84e+01 lr: 2.0e-03 2.4%┣┫ 59/2.5k [01:25<59:19, 1s/it]\n",
      "Loss train: 1.99e-01 val: 2.06e-01 grad: 2.13e+01 lr: 2.0e-03 2.4%┣┫ 60/2.5k [01:26<59:06, 1s/it]\n",
      "Loss train: 2.01e-01 val: 2.30e-01 grad: 2.21e+01 lr: 2.0e-03 2.4%┣┫ 61/2.5k [01:27<58:58, 1s/it]\n",
      "Loss train: 1.88e-01 val: 2.23e-01 grad: 1.72e+01 lr: 2.0e-03 2.5%┣┫ 62/2.5k [01:28<58:49, 1s/it]\n",
      "Loss train: 1.85e-01 val: 2.31e-01 grad: 1.81e+01 lr: 2.0e-03 2.5%┣┫ 63/2.5k [01:30<58:43, 1s/it]\n",
      "Loss train: 1.82e-01 val: 2.32e-01 grad: 1.48e+01 lr: 2.0e-03 2.6%┣┫ 64/2.5k [01:31<58:36, 1s/it]\n",
      "Loss train: 1.81e-01 val: 2.33e-01 grad: 1.79e+01 lr: 2.0e-03 2.6%┣┫ 65/2.5k [01:32<58:33, 1s/it]\n",
      "Loss train: 1.78e-01 val: 2.38e-01 grad: 1.66e+01 lr: 2.0e-03 2.6%┣┫ 66/2.5k [01:34<58:24, 1s/it]\n",
      "Loss train: 1.79e-01 val: 2.34e-01 grad: 1.86e+01 lr: 2.0e-03 2.7%┣┫ 67/2.5k [01:35<58:17, 1s/it]\n",
      "Loss train: 1.82e-01 val: 2.45e-01 grad: 1.63e+01 lr: 2.0e-03 2.7%┣┫ 68/2.5k [01:36<58:14, 1s/it]\n",
      "Loss train: 1.75e-01 val: 2.34e-01 grad: 2.32e+01 lr: 2.0e-03 2.8%┣┫ 69/2.5k [01:38<58:08, 1s/it]\n",
      "Loss train: 1.80e-01 val: 2.31e-01 grad: 1.42e+01 lr: 2.0e-03 2.8%┣┫ 70/2.5k [01:39<58:02, 1s/it]\n",
      "Loss train: 1.80e-01 val: 2.20e-01 grad: 1.49e+01 lr: 2.0e-03 2.8%┣┫ 71/2.5k [01:40<58:02, 1s/it]\n",
      "Loss train: 1.74e-01 val: 2.34e-01 grad: 2.22e+01 lr: 2.0e-03 2.9%┣┫ 72/2.5k [01:42<58:03, 1s/it]\n",
      "Loss train: 1.76e-01 val: 2.31e-01 grad: 1.50e+01 lr: 2.0e-03 2.9%┣┫ 73/2.5k [01:43<57:52, 1s/it]\n",
      "Loss train: 1.82e-01 val: 2.18e-01 grad: 1.66e+01 lr: 2.0e-03 3.0%┣┫ 74/2.5k [01:44<57:47, 1s/it]\n",
      "Loss train: 1.82e-01 val: 2.40e-01 grad: 3.12e+01 lr: 2.0e-03 3.0%┣┫ 75/2.5k [01:46<57:40, 1s/it]\n",
      "Loss train: 2.83e-01 val: 2.89e-01 grad: 1.56e+01 lr: 2.0e-03 3.0%┣┫ 76/2.5k [01:47<57:25, 1s/it]\n",
      "Loss train: 2.00e-01 val: 2.11e-01 grad: 2.14e+01 lr: 2.0e-03 3.1%┣┫ 77/2.5k [01:48<57:10, 1s/it]\n",
      "Loss train: 2.18e-01 val: 2.09e-01 grad: 3.03e+01 lr: 2.0e-03 3.1%┣┫ 78/2.5k [01:49<57:00, 1s/it]\n",
      "Loss train: 2.13e-01 val: 2.28e-01 grad: 2.36e+01 lr: 2.0e-03 3.2%┣┫ 79/2.5k [01:50<56:48, 1s/it]\n",
      "Loss train: 1.93e-01 val: 2.04e-01 grad: 1.61e+01 lr: 2.0e-03 3.2%┣┫ 80/2.5k [01:51<56:36, 1s/it]\n",
      "Loss train: 1.81e-01 val: 2.03e-01 grad: 1.93e+01 lr: 2.0e-03 3.2%┣┫ 81/2.5k [01:52<56:26, 1s/it]\n",
      "Loss train: 1.77e-01 val: 2.23e-01 grad: 1.96e+01 lr: 2.0e-03 3.3%┣┫ 82/2.5k [01:53<56:14, 1s/it]\n",
      "Loss train: 1.74e-01 val: 2.19e-01 grad: 1.67e+01 lr: 2.0e-03 3.3%┣┫ 83/2.5k [01:54<56:03, 1s/it]\n",
      "Loss train: 1.75e-01 val: 2.34e-01 grad: 2.17e+01 lr: 2.0e-03 3.4%┣┫ 84/2.5k [01:55<55:54, 1s/it]\n",
      "Loss train: 1.67e-01 val: 2.39e-01 grad: 1.45e+01 lr: 2.0e-03 3.4%┣┫ 85/2.5k [01:56<55:44, 1s/it]\n",
      "Loss train: 1.65e-01 val: 2.42e-01 grad: 1.97e+01 lr: 2.0e-03 3.4%┣┫ 86/2.5k [01:58<55:45, 1s/it]\n",
      "Loss train: 1.71e-01 val: 2.44e-01 grad: 1.91e+01 lr: 2.0e-03 3.5%┣┫ 87/2.5k [01:59<55:37, 1s/it]\n",
      "Loss train: 1.67e-01 val: 2.39e-01 grad: 1.38e+01 lr: 2.0e-03 3.5%┣┫ 88/2.5k [02:00<55:26, 1s/it]\n",
      "Loss train: 1.68e-01 val: 2.30e-01 grad: 2.46e+01 lr: 2.0e-03 3.6%┣┫ 89/2.5k [02:01<55:21, 1s/it]\n",
      "Loss train: 1.81e-01 val: 2.49e-01 grad: 1.51e+01 lr: 2.0e-03 3.6%┣┫ 90/2.5k [02:02<55:15, 1s/it]\n",
      "Loss train: 1.63e-01 val: 2.34e-01 grad: 1.44e+01 lr: 2.0e-03 3.6%┣┫ 91/2.5k [02:04<55:08, 1s/it]\n",
      "Loss train: 1.73e-01 val: 2.15e-01 grad: 2.41e+01 lr: 2.0e-03 3.7%┣┫ 92/2.5k [02:05<55:05, 1s/it]\n",
      "Loss train: 1.71e-01 val: 2.20e-01 grad: 2.02e+01 lr: 2.0e-03 3.7%┣┫ 93/2.5k [02:06<54:55, 1s/it]\n",
      "Loss train: 1.64e-01 val: 2.19e-01 grad: 2.75e+01 lr: 2.0e-03 3.8%┣┫ 94/2.5k [02:07<54:51, 1s/it]\n",
      "Loss train: 1.83e-01 val: 2.37e-01 grad: 1.35e+01 lr: 2.0e-03 3.8%┣┫ 95/2.5k [02:08<54:41, 1s/it]\n",
      "Loss train: 1.66e-01 val: 2.21e-01 grad: 1.34e+01 lr: 2.0e-03 3.8%┣┫ 96/2.5k [02:09<54:33, 1s/it]\n",
      "Loss train: 1.67e-01 val: 2.17e-01 grad: 1.87e+01 lr: 2.0e-03 3.9%┣┫ 97/2.5k [02:10<54:26, 1s/it]\n",
      "Loss train: 1.69e-01 val: 2.37e-01 grad: 2.13e+01 lr: 2.0e-03 3.9%┣┫ 98/2.5k [02:12<54:21, 1s/it]\n",
      "Loss train: 1.59e-01 val: 2.30e-01 grad: 1.28e+01 lr: 2.0e-03 4.0%┣┫ 99/2.5k [02:13<54:13, 1s/it]\n",
      "Loss train: 1.61e-01 val: 2.26e-01 grad: 1.94e+01 lr: 2.0e-03 4.0%┣┫ 100/2.5k [02:14<54:09, 1s/it]\n",
      "Loss train: 1.60e-01 val: 2.32e-01 grad: 1.35e+01 lr: 2.0e-03 4.0%┣┫ 101/2.5k [02:15<54:08, 1s/it]\n",
      "Loss train: 1.63e-01 val: 2.21e-01 grad: 1.69e+01 lr: 2.0e-03 4.1%┣┫ 102/2.5k [02:17<54:04, 1s/it]\n",
      "Loss train: 1.61e-01 val: 2.33e-01 grad: 2.07e+01 lr: 2.0e-03 4.1%┣┫ 103/2.5k [02:18<53:59, 1s/it]\n",
      "Loss train: 1.58e-01 val: 2.38e-01 grad: 1.51e+01 lr: 2.0e-03 4.2%┣┫ 104/2.5k [02:19<53:54, 1s/it]\n",
      "Loss train: 1.61e-01 val: 2.29e-01 grad: 1.14e+01 lr: 2.0e-03 4.2%┣┫ 105/2.5k [02:20<53:49, 1s/it]\n",
      "Loss train: 1.57e-01 val: 2.36e-01 grad: 1.64e+01 lr: 2.0e-03 4.2%┣┫ 106/2.5k [02:21<53:44, 1s/it]\n",
      "Loss train: 1.58e-01 val: 2.33e-01 grad: 1.40e+01 lr: 2.0e-03 4.3%┣┫ 107/2.5k [02:22<53:36, 1s/it]\n",
      "Loss train: 1.59e-01 val: 2.28e-01 grad: 1.78e+01 lr: 2.0e-03 4.3%┣┫ 108/2.5k [02:24<53:32, 1s/it]\n",
      "Loss train: 1.66e-01 val: 2.50e-01 grad: 2.28e+01 lr: 2.0e-03 4.4%┣┫ 109/2.5k [02:25<53:30, 1s/it]\n",
      "Loss train: 1.59e-01 val: 2.40e-01 grad: 1.21e+01 lr: 2.0e-03 4.4%┣┫ 110/2.5k [02:26<53:22, 1s/it]\n",
      "Loss train: 1.59e-01 val: 2.33e-01 grad: 1.47e+01 lr: 2.0e-03 4.4%┣┫ 111/2.5k [02:27<53:18, 1s/it]\n",
      "Loss train: 1.53e-01 val: 2.31e-01 grad: 1.79e+01 lr: 2.0e-03 4.5%┣┫ 112/2.5k [02:28<53:14, 1s/it]\n",
      "Loss train: 1.54e-01 val: 2.32e-01 grad: 1.35e+01 lr: 2.0e-03 4.5%┣┫ 113/2.5k [02:30<53:10, 1s/it]\n",
      "Loss train: 1.54e-01 val: 2.22e-01 grad: 1.57e+01 lr: 2.0e-03 4.6%┣┫ 114/2.5k [02:31<53:06, 1s/it]\n",
      "Loss train: 1.65e-01 val: 2.19e-01 grad: 1.79e+01 lr: 2.0e-03 4.6%┣┫ 115/2.5k [02:32<53:03, 1s/it]\n",
      "Loss train: 1.59e-01 val: 2.20e-01 grad: 1.54e+01 lr: 2.0e-03 4.6%┣┫ 116/2.5k [02:33<52:57, 1s/it]\n",
      "Loss train: 1.58e-01 val: 2.12e-01 grad: 1.78e+01 lr: 2.0e-03 4.7%┣┫ 117/2.5k [02:34<52:54, 1s/it]\n",
      "Loss train: 1.68e-01 val: 2.44e-01 grad: 1.85e+01 lr: 2.0e-03 4.7%┣┫ 118/2.5k [02:36<52:49, 1s/it]\n",
      "Loss train: 1.52e-01 val: 2.25e-01 grad: 1.31e+01 lr: 2.0e-03 4.8%┣┫ 119/2.5k [02:37<52:42, 1s/it]\n",
      "Loss train: 1.51e-01 val: 2.29e-01 grad: 1.98e+01 lr: 2.0e-03 4.8%┣┫ 120/2.5k [02:38<52:37, 1s/it]\n",
      "Loss train: 1.51e-01 val: 2.33e-01 grad: 1.34e+01 lr: 2.0e-03 4.8%┣┫ 121/2.5k [02:39<52:31, 1s/it]\n",
      "Loss train: 1.48e-01 val: 2.26e-01 grad: 1.25e+01 lr: 2.0e-03 4.9%┣┫ 122/2.5k [02:40<52:26, 1s/it]\n",
      "Loss train: 1.50e-01 val: 2.21e-01 grad: 2.08e+01 lr: 2.0e-03 4.9%┣┫ 123/2.5k [02:41<52:22, 1s/it]\n",
      "Loss train: 1.68e-01 val: 2.24e-01 grad: 1.42e+01 lr: 2.0e-03 5.0%┣┫ 124/2.5k [02:42<52:15, 1s/it]\n",
      "Loss train: 1.70e-01 val: 2.08e-01 grad: 1.83e+01 lr: 2.0e-03 5.0%┣┫ 125/2.5k [02:43<52:07, 1s/it]\n",
      "Loss train: 1.65e-01 val: 2.21e-01 grad: 2.68e+01 lr: 2.0e-03 5.0%┣┫ 126/2.5k [02:44<52:02, 1s/it]\n",
      "Loss train: 1.63e-01 val: 2.27e-01 grad: 1.36e+01 lr: 2.0e-03 5.1%┣┫ 127/2.5k [02:45<51:56, 1s/it]\n",
      "Loss train: 1.60e-01 val: 2.11e-01 grad: 1.96e+01 lr: 2.0e-03 5.1%┣┫ 128/2.5k [02:47<51:53, 1s/it]\n",
      "Loss train: 1.64e-01 val: 2.09e-01 grad: 1.92e+01 lr: 2.0e-03 5.2%┣┫ 129/2.5k [02:48<51:47, 1s/it]\n",
      "Loss train: 1.50e-01 val: 2.26e-01 grad: 1.34e+01 lr: 2.0e-03 5.2%┣┫ 130/2.5k [02:49<51:41, 1s/it]\n",
      "Loss train: 1.53e-01 val: 2.31e-01 grad: 1.19e+01 lr: 2.0e-03 5.2%┣┫ 131/2.5k [02:50<51:37, 1s/it]\n",
      "Loss train: 1.48e-01 val: 2.27e-01 grad: 1.60e+01 lr: 2.0e-03 5.3%┣┫ 132/2.5k [02:51<51:35, 1s/it]\n",
      "Loss train: 1.49e-01 val: 2.23e-01 grad: 1.69e+01 lr: 2.0e-03 5.3%┣┫ 133/2.5k [02:52<51:32, 1s/it]\n",
      "Loss train: 1.46e-01 val: 2.31e-01 grad: 1.65e+01 lr: 2.0e-03 5.4%┣┫ 134/2.5k [02:54<51:28, 1s/it]\n",
      "Loss train: 1.52e-01 val: 2.32e-01 grad: 1.48e+01 lr: 2.0e-03 5.4%┣┫ 135/2.5k [02:55<51:26, 1s/it]\n",
      "Loss train: 1.51e-01 val: 2.17e-01 grad: 1.47e+01 lr: 2.0e-03 5.4%┣┫ 136/2.5k [02:56<51:21, 1s/it]\n",
      "Loss train: 1.52e-01 val: 2.12e-01 grad: 1.74e+01 lr: 2.0e-03 5.5%┣┫ 137/2.5k [02:57<51:18, 1s/it]\n",
      "Loss train: 1.50e-01 val: 2.30e-01 grad: 1.92e+01 lr: 2.0e-03 5.5%┣┫ 138/2.5k [02:58<51:13, 1s/it]\n",
      "Loss train: 1.53e-01 val: 2.02e-01 grad: 1.11e+01 lr: 2.0e-03 5.6%┣┫ 139/2.5k [02:59<51:08, 1s/it]\n",
      "Loss train: 1.53e-01 val: 2.05e-01 grad: 1.95e+01 lr: 2.0e-03 5.6%┣┫ 140/2.5k [03:00<51:04, 1s/it]\n",
      "Loss train: 1.83e-01 val: 1.75e-01 grad: 1.77e+01 lr: 2.0e-03 5.6%┣┫ 141/2.5k [03:02<51:00, 1s/it]\n",
      "Loss train: 1.70e-01 val: 1.91e-01 grad: 1.91e+01 lr: 2.0e-03 5.7%┣┫ 142/2.5k [03:03<50:55, 1s/it]\n",
      "Loss train: 1.52e-01 val: 2.19e-01 grad: 1.92e+01 lr: 2.0e-03 5.7%┣┫ 143/2.5k [03:04<50:52, 1s/it]\n",
      "Loss train: 1.51e-01 val: 2.25e-01 grad: 1.25e+01 lr: 2.0e-03 5.8%┣┫ 144/2.5k [03:05<50:47, 1s/it]\n",
      "Loss train: 1.48e-01 val: 2.11e-01 grad: 1.94e+01 lr: 2.0e-03 5.8%┣┫ 145/2.5k [03:06<50:43, 1s/it]\n",
      "Loss train: 1.55e-01 val: 2.25e-01 grad: 1.04e+01 lr: 2.0e-03 5.8%┣┫ 146/2.5k [03:07<50:39, 1s/it]\n",
      "Loss train: 1.45e-01 val: 2.14e-01 grad: 1.62e+01 lr: 2.0e-03 5.9%┣┫ 147/2.5k [03:08<50:33, 1s/it]\n",
      "Loss train: 1.46e-01 val: 2.16e-01 grad: 1.05e+01 lr: 2.0e-03 5.9%┣┫ 148/2.5k [03:09<50:28, 1s/it]\n",
      "Loss train: 1.43e-01 val: 2.08e-01 grad: 1.01e+01 lr: 2.0e-03 6.0%┣┫ 149/2.5k [03:10<50:23, 1s/it]\n",
      "Loss train: 1.59e-01 val: 1.98e-01 grad: 2.14e+01 lr: 2.0e-03 6.0%┣┫ 150/2.5k [03:12<50:20, 1s/it]\n",
      "Loss train: 1.50e-01 val: 2.09e-01 grad: 1.62e+01 lr: 2.0e-03 6.0%┣┫ 151/2.5k [03:13<50:15, 1s/it]\n",
      "Loss train: 1.44e-01 val: 2.19e-01 grad: 1.72e+01 lr: 2.0e-03 6.1%┣┫ 152/2.5k [03:14<50:11, 1s/it]\n",
      "Loss train: 1.40e-01 val: 2.22e-01 grad: 1.66e+01 lr: 2.0e-03 6.1%┣┫ 153/2.5k [03:15<50:06, 1s/it]\n",
      "Loss train: 1.48e-01 val: 2.14e-01 grad: 1.66e+01 lr: 2.0e-03 6.2%┣┫ 154/2.5k [03:16<50:03, 1s/it]\n",
      "Loss train: 1.47e-01 val: 2.25e-01 grad: 1.21e+01 lr: 2.0e-03 6.2%┣┫ 155/2.5k [03:17<49:58, 1s/it]\n",
      "Loss train: 1.54e-01 val: 2.08e-01 grad: 1.21e+01 lr: 2.0e-03 6.2%┣┫ 156/2.5k [03:18<49:54, 1s/it]\n",
      "Loss train: 1.42e-01 val: 2.28e-01 grad: 2.02e+01 lr: 2.0e-03 6.3%┣┫ 157/2.5k [03:19<49:49, 1s/it]\n",
      "Loss train: 1.41e-01 val: 2.29e-01 grad: 1.40e+01 lr: 2.0e-03 6.3%┣┫ 158/2.5k [03:20<49:44, 1s/it]\n",
      "Loss train: 1.40e-01 val: 2.21e-01 grad: 9.95e+00 lr: 2.0e-03 6.4%┣┫ 159/2.5k [03:21<49:39, 1s/it]\n",
      "Loss train: 1.46e-01 val: 2.09e-01 grad: 1.88e+01 lr: 2.0e-03 6.4%┣┫ 160/2.5k [03:22<49:36, 1s/it]\n",
      "Loss train: 1.51e-01 val: 1.98e-01 grad: 1.30e+01 lr: 2.0e-03 6.4%┣┫ 161/2.5k [03:23<49:31, 1s/it]\n",
      "Loss train: 1.46e-01 val: 2.14e-01 grad: 1.33e+01 lr: 2.0e-03 6.5%┣┫ 162/2.5k [03:24<49:26, 1s/it]\n",
      "Loss train: 1.41e-01 val: 2.15e-01 grad: 1.81e+01 lr: 2.0e-03 6.5%┣┫ 163/2.5k [03:25<49:21, 1s/it]\n",
      "Loss train: 1.41e-01 val: 2.23e-01 grad: 1.07e+01 lr: 2.0e-03 6.6%┣┫ 164/2.5k [03:26<49:17, 1s/it]\n",
      "Loss train: 1.37e-01 val: 2.14e-01 grad: 1.03e+01 lr: 2.0e-03 6.6%┣┫ 165/2.5k [03:27<49:12, 1s/it]\n",
      "Loss train: 1.42e-01 val: 2.27e-01 grad: 1.61e+01 lr: 2.0e-03 6.6%┣┫ 166/2.5k [03:28<49:09, 1s/it]\n",
      "Loss train: 1.40e-01 val: 2.12e-01 grad: 1.20e+01 lr: 2.0e-03 6.7%┣┫ 167/2.5k [03:30<49:05, 1s/it]\n",
      "Loss train: 1.36e-01 val: 2.13e-01 grad: 1.60e+01 lr: 2.0e-03 6.7%┣┫ 168/2.5k [03:31<49:01, 1s/it]\n",
      "Loss train: 1.42e-01 val: 2.20e-01 grad: 1.26e+01 lr: 2.0e-03 6.8%┣┫ 169/2.5k [03:32<48:57, 1s/it]\n",
      "Loss train: 1.57e-01 val: 1.72e-01 grad: 1.43e+01 lr: 2.0e-03 6.8%┣┫ 170/2.5k [03:33<48:54, 1s/it]\n",
      "Loss train: 1.62e-01 val: 2.08e-01 grad: 1.78e+01 lr: 2.0e-03 6.8%┣┫ 171/2.5k [03:34<48:51, 1s/it]\n",
      "Loss train: 1.46e-01 val: 2.05e-01 grad: 1.55e+01 lr: 2.0e-03 6.9%┣┫ 172/2.5k [03:35<48:47, 1s/it]\n",
      "Loss train: 1.38e-01 val: 2.20e-01 grad: 1.64e+01 lr: 2.0e-03 6.9%┣┫ 173/2.5k [03:36<48:42, 1s/it]\n",
      "Loss train: 1.36e-01 val: 2.20e-01 grad: 1.32e+01 lr: 2.0e-03 7.0%┣┫ 174/2.5k [03:37<48:37, 1s/it]\n",
      "Loss train: 1.33e-01 val: 2.05e-01 grad: 1.36e+01 lr: 2.0e-03 7.0%┣┫ 175/2.5k [03:38<48:33, 1s/it]\n",
      "Loss train: 1.45e-01 val: 2.19e-01 grad: 1.33e+01 lr: 2.0e-03 7.0%┣┫ 176/2.5k [03:39<48:31, 1s/it]\n",
      "Loss train: 1.34e-01 val: 2.04e-01 grad: 1.14e+01 lr: 2.0e-03 7.1%┣┫ 177/2.5k [03:40<48:27, 1s/it]\n",
      "Loss train: 1.35e-01 val: 2.06e-01 grad: 1.30e+01 lr: 2.0e-03 7.1%┣┫ 178/2.5k [03:41<48:23, 1s/it]\n",
      "Loss train: 1.32e-01 val: 2.01e-01 grad: 1.46e+01 lr: 2.0e-03 7.2%┣┫ 179/2.5k [03:42<48:19, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.92e-01 grad: 1.58e+01 lr: 2.0e-03 7.2%┣┫ 180/2.5k [03:43<48:16, 1s/it]\n",
      "Loss train: 1.47e-01 val: 2.14e-01 grad: 1.16e+01 lr: 2.0e-03 7.2%┣┫ 181/2.5k [03:45<48:13, 1s/it]\n",
      "Loss train: 1.33e-01 val: 1.95e-01 grad: 1.03e+01 lr: 2.0e-03 7.3%┣┫ 182/2.5k [03:46<48:09, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.95e-01 grad: 1.87e+01 lr: 2.0e-03 7.3%┣┫ 183/2.5k [03:47<48:05, 1s/it]\n",
      "Loss train: 1.36e-01 val: 2.15e-01 grad: 1.16e+01 lr: 2.0e-03 7.4%┣┫ 184/2.5k [03:48<48:01, 1s/it]\n",
      "Loss train: 1.30e-01 val: 2.07e-01 grad: 1.12e+01 lr: 2.0e-03 7.4%┣┫ 185/2.5k [03:49<47:58, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.87e-01 grad: 1.36e+01 lr: 2.0e-03 7.4%┣┫ 186/2.5k [03:50<47:56, 1s/it]\n",
      "Loss train: 1.52e-01 val: 2.14e-01 grad: 1.98e+01 lr: 2.0e-03 7.5%┣┫ 187/2.5k [03:51<47:54, 1s/it]\n",
      "Loss train: 1.46e-01 val: 1.94e-01 grad: 1.30e+01 lr: 2.0e-03 7.5%┣┫ 188/2.5k [03:52<47:50, 1s/it]\n",
      "Loss train: 1.55e-01 val: 1.84e-01 grad: 1.91e+01 lr: 2.0e-03 7.6%┣┫ 189/2.5k [03:53<47:47, 1s/it]\n",
      "Loss train: 1.49e-01 val: 2.07e-01 grad: 1.59e+01 lr: 2.0e-03 7.6%┣┫ 190/2.5k [03:54<47:44, 1s/it]\n",
      "Loss train: 1.34e-01 val: 1.88e-01 grad: 1.11e+01 lr: 2.0e-03 7.6%┣┫ 191/2.5k [03:55<47:41, 1s/it]\n",
      "Loss train: 1.39e-01 val: 1.97e-01 grad: 1.56e+01 lr: 2.0e-03 7.7%┣┫ 192/2.5k [03:57<47:38, 1s/it]\n",
      "Loss train: 1.43e-01 val: 1.86e-01 grad: 1.20e+01 lr: 2.0e-03 7.7%┣┫ 193/2.5k [03:58<47:35, 1s/it]\n",
      "Loss train: 1.38e-01 val: 2.08e-01 grad: 1.92e+01 lr: 2.0e-03 7.8%┣┫ 194/2.5k [03:59<47:31, 1s/it]\n",
      "Loss train: 1.40e-01 val: 2.18e-01 grad: 1.04e+01 lr: 2.0e-03 7.8%┣┫ 195/2.5k [04:00<47:27, 1s/it]\n",
      "Loss train: 1.34e-01 val: 1.99e-01 grad: 1.46e+01 lr: 2.0e-03 7.8%┣┫ 196/2.5k [04:01<47:22, 1s/it]\n",
      "Loss train: 1.29e-01 val: 2.07e-01 grad: 1.24e+01 lr: 2.0e-03 7.9%┣┫ 197/2.5k [04:02<47:19, 1s/it]\n",
      "Loss train: 1.28e-01 val: 2.07e-01 grad: 1.29e+01 lr: 2.0e-03 7.9%┣┫ 198/2.5k [04:03<47:15, 1s/it]\n",
      "Loss train: 1.30e-01 val: 2.03e-01 grad: 1.12e+01 lr: 2.0e-03 8.0%┣┫ 199/2.5k [04:04<47:12, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.95e-01 grad: 1.39e+01 lr: 2.0e-03 8.0%┣┫ 200/2.5k [04:05<47:08, 1s/it]\n",
      "Loss train: 1.28e-01 val: 2.03e-01 grad: 1.51e+01 lr: 2.0e-03 8.0%┣┫ 201/2.5k [04:06<47:06, 1s/it]\n",
      "Loss train: 1.37e-01 val: 2.16e-01 grad: 1.01e+01 lr: 2.0e-03 8.1%┣┫ 202/2.5k [04:07<47:02, 1s/it]\n",
      "Loss train: 1.26e-01 val: 2.02e-01 grad: 1.50e+01 lr: 2.0e-03 8.1%┣┫ 203/2.5k [04:08<46:59, 1s/it]\n",
      "Loss train: 1.30e-01 val: 1.99e-01 grad: 1.37e+01 lr: 2.0e-03 8.2%┣┫ 204/2.5k [04:09<46:56, 1s/it]\n",
      "Loss train: 1.30e-01 val: 2.06e-01 grad: 1.36e+01 lr: 2.0e-03 8.2%┣┫ 205/2.5k [04:10<46:52, 1s/it]\n",
      "Loss train: 1.27e-01 val: 2.05e-01 grad: 9.15e+00 lr: 2.0e-03 8.2%┣┫ 206/2.5k [04:11<46:50, 1s/it]\n",
      "Loss train: 1.25e-01 val: 1.96e-01 grad: 1.07e+01 lr: 2.0e-03 8.3%┣┫ 207/2.5k [04:12<46:46, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.87e-01 grad: 1.39e+01 lr: 2.0e-03 8.3%┣┫ 208/2.5k [04:13<46:43, 1s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.28  0.0   0.0   0.0   0.0  0.0    93.25  0.06   8.73  -0.28   0.01  -0.0    0.26  0.01  0.0\n",
      "  0.09  0.0   0.0   0.29  0.0  0.0    86.55  0.15  10.04  -0.09   0.15   0.04  -0.29  0.18  0.0\n",
      "  0.41  0.08  0.07  0.0   0.0  0.0    88.75  0.24  11.77  -0.41  -0.08  -0.07   0.4   0.15  0.0\n",
      " -0.0   0.33  0.16  0.0   0.0  0.02   86.38  0.16   9.9    0.0   -0.33  -0.16   0.47  0.02  0.0\n",
      " -0.0   0.0   0.0   0.34  0.0  0.14  103.79  0.03   8.82   0.0   -0.0    0.27  -0.34  0.07  0.0\n",
      "  0.05  0.06  0.03  0.0   0.0  0.17   99.65  0.05   7.63  -0.05  -0.06  -0.03   0.13  0.0   0.0\n",
      " -0.0   0.0   0.0   0.47  0.0  0.17   74.42  0.19  10.5    0.0    0.02   0.3   -0.47  0.15  0.0\n",
      "  0.11  0.0   0.0   0.39  0.0  0.0    76.39  0.18   9.97  -0.11   0.1    0.08  -0.39  0.33  0.0\n",
      "\n",
      "Min Loss train: 1.25e-01 val: 1.37e-01\n",
      " update plot 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 1.82e-01 val: 1.37e-01 grad: 1.86e+01 lr: 2.0e-03 8.4%┣┫ 209/2.5k [04:14<46:42, 1s/it]\n",
      "Loss train: 2.03e-01 val: 2.28e-01 grad: 1.94e+01 lr: 2.0e-03 8.4%┣┫ 210/2.5k [04:15<46:39, 1s/it]\n",
      "Loss train: 1.37e-01 val: 1.76e-01 grad: 1.50e+01 lr: 2.0e-03 8.4%┣┫ 211/2.5k [04:16<46:35, 1s/it]\n",
      "Loss train: 1.55e-01 val: 1.69e-01 grad: 2.44e+01 lr: 2.0e-03 8.5%┣┫ 212/2.5k [04:18<46:32, 1s/it]\n",
      "Loss train: 1.52e-01 val: 1.85e-01 grad: 1.65e+01 lr: 2.0e-03 8.5%┣┫ 213/2.5k [04:19<46:29, 1s/it]\n",
      "Loss train: 1.33e-01 val: 1.71e-01 grad: 1.45e+01 lr: 2.0e-03 8.6%┣┫ 214/2.5k [04:20<46:26, 1s/it]\n",
      "Loss train: 1.39e-01 val: 1.67e-01 grad: 1.75e+01 lr: 2.0e-03 8.6%┣┫ 215/2.5k [04:21<46:24, 1s/it]\n",
      "Loss train: 1.34e-01 val: 1.99e-01 grad: 1.11e+01 lr: 2.0e-03 8.6%┣┫ 216/2.5k [04:22<46:23, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.95e-01 grad: 1.04e+01 lr: 2.0e-03 8.7%┣┫ 217/2.5k [04:23<46:20, 1s/it]\n",
      "Loss train: 1.84e-01 val: 1.39e-01 grad: 1.52e+01 lr: 2.0e-03 8.7%┣┫ 218/2.5k [04:24<46:18, 1s/it]\n",
      "Loss train: 1.40e-01 val: 2.16e-01 grad: 1.60e+01 lr: 2.0e-03 8.8%┣┫ 219/2.5k [04:25<46:15, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.95e-01 grad: 1.20e+01 lr: 2.0e-03 8.8%┣┫ 220/2.5k [04:26<46:11, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.91e-01 grad: 1.46e+01 lr: 2.0e-03 8.8%┣┫ 221/2.5k [04:27<46:07, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.94e-01 grad: 1.63e+01 lr: 2.0e-03 8.9%┣┫ 222/2.5k [04:28<46:04, 1s/it]\n",
      "Loss train: 1.28e-01 val: 2.02e-01 grad: 8.98e+00 lr: 2.0e-03 8.9%┣┫ 223/2.5k [04:29<46:00, 1s/it]\n",
      "Loss train: 1.70e-01 val: 1.44e-01 grad: 1.33e+01 lr: 2.0e-03 9.0%┣┫ 224/2.5k [04:30<45:58, 1s/it]\n",
      "Loss train: 1.52e-01 val: 2.00e-01 grad: 1.54e+01 lr: 2.0e-03 9.0%┣┫ 225/2.5k [04:31<45:55, 1s/it]\n",
      "Loss train: 1.43e-01 val: 1.78e-01 grad: 1.44e+01 lr: 2.0e-03 9.0%┣┫ 226/2.5k [04:32<45:52, 1s/it]\n",
      "Loss train: 1.42e-01 val: 1.82e-01 grad: 1.95e+01 lr: 2.0e-03 9.1%┣┫ 227/2.5k [04:33<45:50, 1s/it]\n",
      "Loss train: 1.37e-01 val: 2.07e-01 grad: 1.32e+01 lr: 2.0e-03 9.1%┣┫ 228/2.5k [04:34<45:46, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.99e-01 grad: 1.28e+01 lr: 2.0e-03 9.2%┣┫ 229/2.5k [04:35<45:43, 1s/it]\n",
      "Loss train: 1.43e-01 val: 1.92e-01 grad: 1.97e+01 lr: 2.0e-03 9.2%┣┫ 230/2.5k [04:37<45:41, 1s/it]\n",
      "Loss train: 1.40e-01 val: 1.82e-01 grad: 1.63e+01 lr: 2.0e-03 9.2%┣┫ 231/2.5k [04:38<45:38, 1s/it]\n",
      "Loss train: 1.34e-01 val: 2.02e-01 grad: 1.25e+01 lr: 2.0e-03 9.3%┣┫ 232/2.5k [04:39<45:35, 1s/it]\n",
      "Loss train: 1.36e-01 val: 2.07e-01 grad: 1.35e+01 lr: 2.0e-03 9.3%┣┫ 233/2.5k [04:40<45:33, 1s/it]\n",
      "Loss train: 1.25e-01 val: 2.05e-01 grad: 1.31e+01 lr: 2.0e-03 9.4%┣┫ 234/2.5k [04:41<45:30, 1s/it]\n",
      "Loss train: 1.26e-01 val: 1.99e-01 grad: 1.26e+01 lr: 2.0e-03 9.4%┣┫ 235/2.5k [04:42<45:27, 1s/it]\n",
      "Loss train: 1.30e-01 val: 2.05e-01 grad: 1.22e+01 lr: 2.0e-03 9.4%┣┫ 236/2.5k [04:43<45:25, 1s/it]\n",
      "Loss train: 1.36e-01 val: 2.03e-01 grad: 2.02e+01 lr: 2.0e-03 9.5%┣┫ 237/2.5k [04:44<45:22, 1s/it]\n",
      "Loss train: 1.69e-01 val: 1.83e-01 grad: 1.37e+01 lr: 2.0e-03 9.5%┣┫ 238/2.5k [04:45<45:19, 1s/it]\n",
      "Loss train: 1.46e-01 val: 1.78e-01 grad: 1.59e+01 lr: 2.0e-03 9.6%┣┫ 239/2.5k [04:46<45:16, 1s/it]\n",
      "Loss train: 1.27e-01 val: 2.03e-01 grad: 1.63e+01 lr: 2.0e-03 9.6%┣┫ 240/2.5k [04:47<45:14, 1s/it]\n",
      "Loss train: 1.22e-01 val: 2.00e-01 grad: 9.93e+00 lr: 2.0e-03 9.6%┣┫ 241/2.5k [04:48<45:12, 1s/it]\n",
      "Loss train: 1.24e-01 val: 2.01e-01 grad: 1.50e+01 lr: 2.0e-03 9.7%┣┫ 242/2.5k [04:49<45:10, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.99e-01 grad: 1.26e+01 lr: 2.0e-03 9.7%┣┫ 243/2.5k [04:50<45:07, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.87e-01 grad: 1.31e+01 lr: 2.0e-03 9.8%┣┫ 244/2.5k [04:51<45:04, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.94e-01 grad: 1.34e+01 lr: 2.0e-03 9.8%┣┫ 245/2.5k [04:52<45:02, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.87e-01 grad: 9.66e+00 lr: 2.0e-03 9.8%┣┫ 246/2.5k [04:53<45:00, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.89e-01 grad: 1.33e+01 lr: 2.0e-03 9.9%┣┫ 247/2.5k [04:55<44:58, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.79e-01 grad: 1.10e+01 lr: 2.0e-03 9.9%┣┫ 248/2.5k [04:56<44:55, 1s/it]\n",
      "Loss train: 1.43e-01 val: 1.94e-01 grad: 1.91e+01 lr: 2.0e-03 10.0%┣┫ 249/2.5k [04:57<44:53, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.79e-01 grad: 1.17e+01 lr: 2.0e-03 10.0%┣┫ 250/2.5k [04:58<44:50, 1s/it]\n",
      "Loss train: 1.34e-01 val: 1.87e-01 grad: 1.45e+01 lr: 2.0e-03 10.0%┣┫ 251/2.5k [04:59<44:48, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.79e-01 grad: 1.13e+01 lr: 2.0e-03 10.1%┣┫ 252/2.5k [05:00<44:46, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.77e-01 grad: 1.82e+01 lr: 2.0e-03 10.1%┣┫ 253/2.5k [05:01<44:43, 1s/it]\n",
      "Loss train: 1.34e-01 val: 1.98e-01 grad: 1.15e+01 lr: 2.0e-03 10.2%┣┫ 254/2.5k [05:02<44:41, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.69e-01 grad: 1.01e+01 lr: 2.0e-03 10.2%┣┫ 255/2.5k [05:03<44:39, 1s/it]\n",
      "Loss train: 1.39e-01 val: 1.94e-01 grad: 1.73e+01 lr: 2.0e-03 10.2%┣┫ 256/2.5k [05:04<44:36, 1s/it]\n",
      "Loss train: 1.26e-01 val: 1.81e-01 grad: 1.15e+01 lr: 2.0e-03 10.3%┣┫ 257/2.5k [05:05<44:33, 1s/it]\n",
      "Loss train: 1.35e-01 val: 1.77e-01 grad: 1.56e+01 lr: 2.0e-03 10.3%┣┫ 258/2.5k [05:06<44:30, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.94e-01 grad: 1.44e+01 lr: 2.0e-03 10.4%┣┫ 259/2.5k [05:07<44:28, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.80e-01 grad: 1.06e+01 lr: 2.0e-03 10.4%┣┫ 260/2.5k [05:08<44:25, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.78e-01 grad: 1.47e+01 lr: 2.0e-03 10.4%┣┫ 261/2.5k [05:09<44:22, 1s/it]\n",
      "Loss train: 1.22e-01 val: 1.89e-01 grad: 1.41e+01 lr: 2.0e-03 10.5%┣┫ 262/2.5k [05:10<44:20, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.84e-01 grad: 1.17e+01 lr: 2.0e-03 10.5%┣┫ 263/2.5k [05:11<44:18, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.80e-01 grad: 1.18e+01 lr: 2.0e-03 10.6%┣┫ 264/2.5k [05:12<44:15, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.82e-01 grad: 1.32e+01 lr: 2.0e-03 10.6%┣┫ 265/2.5k [05:13<44:13, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.78e-01 grad: 1.13e+01 lr: 2.0e-03 10.6%┣┫ 266/2.5k [05:14<44:10, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.76e-01 grad: 1.32e+01 lr: 2.0e-03 10.7%┣┫ 267/2.5k [05:15<44:08, 1s/it]\n",
      "Loss train: 1.56e-01 val: 1.91e-01 grad: 2.16e+01 lr: 2.0e-03 10.7%┣┫ 268/2.5k [05:16<44:06, 1s/it]\n",
      "Loss train: 1.30e-01 val: 1.77e-01 grad: 1.48e+01 lr: 2.0e-03 10.8%┣┫ 269/2.5k [05:18<44:04, 1s/it]\n",
      "Loss train: 1.51e-01 val: 1.59e-01 grad: 1.60e+01 lr: 2.0e-03 10.8%┣┫ 270/2.5k [05:19<44:01, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.74e-01 grad: 1.69e+01 lr: 2.0e-03 10.8%┣┫ 271/2.5k [05:20<43:59, 1s/it]\n",
      "Loss train: 1.45e-01 val: 1.98e-01 grad: 1.21e+01 lr: 2.0e-03 10.9%┣┫ 272/2.5k [05:21<43:56, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.46e-01 grad: 1.38e+01 lr: 2.0e-03 10.9%┣┫ 273/2.5k [05:22<43:54, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.73e-01 grad: 1.81e+01 lr: 2.0e-03 11.0%┣┫ 274/2.5k [05:23<43:52, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.75e-01 grad: 1.43e+01 lr: 2.0e-03 11.0%┣┫ 275/2.5k [05:24<43:49, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.82e-01 grad: 1.86e+01 lr: 2.0e-03 11.0%┣┫ 276/2.5k [05:25<43:47, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.95e-01 grad: 1.42e+01 lr: 2.0e-03 11.1%┣┫ 277/2.5k [05:26<43:45, 1s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.29  0.0   0.0   0.0   0.0  0.0   102.19  0.06   9.64  -0.29   0.02   0.0    0.26  0.01  0.0\n",
      "  0.09  0.0   0.0   0.3   0.0  0.0    93.61  0.16  11.25  -0.09   0.16   0.04  -0.3   0.18  0.0\n",
      "  0.43  0.07  0.07  0.0   0.0  0.0    97.51  0.26  13.27  -0.43  -0.07  -0.07   0.41  0.17  0.0\n",
      " -0.0   0.37  0.18  0.0   0.0  0.05   91.85  0.17  11.17   0.0   -0.37  -0.18   0.51  0.04  0.0\n",
      " -0.0   0.0   0.0   0.35  0.0  0.14  114.2   0.02   9.67   0.0   -0.0    0.28  -0.35  0.08  0.0\n",
      "  0.05  0.06  0.03  0.0   0.0  0.17  109.54  0.05   8.38  -0.05  -0.06  -0.03   0.13  0.0   0.0\n",
      " -0.0   0.0   0.0   0.49  0.0  0.19   78.43  0.2   11.73   0.0    0.01   0.31  -0.49  0.17  0.0\n",
      "  0.12  0.0   0.0   0.43  0.0  0.0    81.79  0.21  11.36  -0.12   0.12   0.09  -0.43  0.34  0.0\n",
      "\n",
      "Min Loss train: 1.14e-01 val: 1.03e-01\n",
      " update plot 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 1.89e-01 val: 1.03e-01 grad: 1.36e+01 lr: 2.0e-03 11.1%┣┫ 278/2.5k [05:27<43:43, 1s/it]\n",
      "Loss train: 1.49e-01 val: 2.14e-01 grad: 2.39e+01 lr: 2.0e-03 11.2%┣┫ 279/2.5k [05:28<43:42, 1s/it]\n",
      "Loss train: 1.59e-01 val: 1.87e-01 grad: 1.30e+01 lr: 2.0e-03 11.2%┣┫ 280/2.5k [05:29<43:39, 1s/it]\n",
      "Loss train: 1.65e-01 val: 1.74e-01 grad: 1.59e+01 lr: 2.0e-03 11.2%┣┫ 281/2.5k [05:30<43:37, 1s/it]\n",
      "Loss train: 1.53e-01 val: 1.63e-01 grad: 1.69e+01 lr: 2.0e-03 11.3%┣┫ 282/2.5k [05:31<43:35, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.89e-01 grad: 1.72e+01 lr: 2.0e-03 11.3%┣┫ 283/2.5k [05:32<43:32, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.81e-01 grad: 1.08e+01 lr: 2.0e-03 11.4%┣┫ 284/2.5k [05:33<43:30, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.64e-01 grad: 8.70e+00 lr: 2.0e-03 11.4%┣┫ 285/2.5k [05:34<43:28, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.76e-01 grad: 1.55e+01 lr: 2.0e-03 11.4%┣┫ 286/2.5k [05:35<43:26, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.69e-01 grad: 1.02e+01 lr: 2.0e-03 11.5%┣┫ 287/2.5k [05:36<43:24, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.83e-01 grad: 1.71e+01 lr: 2.0e-03 11.5%┣┫ 288/2.5k [05:38<43:22, 1s/it]\n",
      "Loss train: 1.30e-01 val: 1.85e-01 grad: 1.17e+01 lr: 2.0e-03 11.6%┣┫ 289/2.5k [05:39<43:19, 1s/it]\n",
      "Loss train: 1.31e-01 val: 1.72e-01 grad: 1.59e+01 lr: 2.0e-03 11.6%┣┫ 290/2.5k [05:40<43:17, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.87e-01 grad: 1.44e+01 lr: 2.0e-03 11.6%┣┫ 291/2.5k [05:41<43:15, 1s/it]\n",
      "Loss train: 1.26e-01 val: 1.98e-01 grad: 1.14e+01 lr: 2.0e-03 11.7%┣┫ 292/2.5k [05:42<43:13, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.82e-01 grad: 1.00e+01 lr: 2.0e-03 11.7%┣┫ 293/2.5k [05:43<43:10, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.74e-01 grad: 1.69e+01 lr: 2.0e-03 11.8%┣┫ 294/2.5k [05:44<43:07, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.95e-01 grad: 1.39e+01 lr: 2.0e-03 11.8%┣┫ 295/2.5k [05:45<43:05, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.71e-01 grad: 1.05e+01 lr: 2.0e-03 11.8%┣┫ 296/2.5k [05:46<43:03, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.89e-01 grad: 2.19e+01 lr: 2.0e-03 11.9%┣┫ 297/2.5k [05:47<43:01, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.75e-01 grad: 1.36e+01 lr: 2.0e-03 11.9%┣┫ 298/2.5k [05:48<42:58, 1s/it]\n",
      "Loss train: 1.30e-01 val: 1.73e-01 grad: 1.59e+01 lr: 2.0e-03 12.0%┣┫ 299/2.5k [05:49<42:56, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.89e-01 grad: 1.38e+01 lr: 2.0e-03 12.0%┣┫ 300/2.5k [05:50<42:53, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.87e-01 grad: 1.11e+01 lr: 2.0e-03 12.0%┣┫ 301/2.5k [05:51<42:51, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.82e-01 grad: 1.10e+01 lr: 2.0e-03 12.1%┣┫ 302/2.5k [05:52<42:48, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.81e-01 grad: 1.27e+01 lr: 2.0e-03 12.1%┣┫ 303/2.5k [05:53<42:46, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.72e-01 grad: 1.32e+01 lr: 2.0e-03 12.2%┣┫ 304/2.5k [05:54<42:44, 1s/it]\n",
      "Loss train: 1.76e-01 val: 1.21e-01 grad: 1.32e+01 lr: 2.0e-03 12.2%┣┫ 305/2.5k [05:55<42:43, 1s/it]\n",
      "Loss train: 1.65e-01 val: 2.03e-01 grad: 2.13e+01 lr: 2.0e-03 12.2%┣┫ 306/2.5k [05:56<42:41, 1s/it]\n",
      "Loss train: 1.40e-01 val: 1.70e-01 grad: 1.26e+01 lr: 2.0e-03 12.3%┣┫ 307/2.5k [05:57<42:39, 1s/it]\n",
      "Loss train: 1.46e-01 val: 1.55e-01 grad: 1.61e+01 lr: 2.0e-03 12.3%┣┫ 308/2.5k [05:58<42:37, 1s/it]\n",
      "Loss train: 1.29e-01 val: 1.60e-01 grad: 1.68e+01 lr: 2.0e-03 12.4%┣┫ 309/2.5k [05:59<42:34, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.92e-01 grad: 1.25e+01 lr: 2.0e-03 12.4%┣┫ 310/2.5k [06:00<42:32, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.75e-01 grad: 1.01e+01 lr: 2.0e-03 12.4%┣┫ 311/2.5k [06:01<42:29, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.62e-01 grad: 1.51e+01 lr: 2.0e-03 12.5%┣┫ 312/2.5k [06:02<42:27, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.86e-01 grad: 1.61e+01 lr: 2.0e-03 12.5%┣┫ 313/2.5k [06:03<42:25, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.75e-01 grad: 1.76e+01 lr: 2.0e-03 12.6%┣┫ 314/2.5k [06:04<42:24, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.82e-01 grad: 1.36e+01 lr: 2.0e-03 12.6%┣┫ 315/2.5k [06:05<42:21, 1s/it]\n",
      "Loss train: 1.22e-01 val: 1.71e-01 grad: 1.02e+01 lr: 2.0e-03 12.6%┣┫ 316/2.5k [06:06<42:19, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.71e-01 grad: 1.33e+01 lr: 2.0e-03 12.7%┣┫ 317/2.5k [06:07<42:17, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.77e-01 grad: 1.28e+01 lr: 2.0e-03 12.7%┣┫ 318/2.5k [06:08<42:15, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.82e-01 grad: 1.09e+01 lr: 2.0e-03 12.8%┣┫ 319/2.5k [06:09<42:12, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.74e-01 grad: 1.22e+01 lr: 2.0e-03 12.8%┣┫ 320/2.5k [06:10<42:10, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.75e-01 grad: 1.28e+01 lr: 2.0e-03 12.8%┣┫ 321/2.5k [06:11<42:08, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.82e-01 grad: 1.23e+01 lr: 2.0e-03 12.9%┣┫ 322/2.5k [06:12<42:06, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.71e-01 grad: 1.08e+01 lr: 2.0e-03 12.9%┣┫ 323/2.5k [06:13<42:03, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.67e-01 grad: 1.33e+01 lr: 2.0e-03 13.0%┣┫ 324/2.5k [06:14<42:01, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.74e-01 grad: 1.40e+01 lr: 2.0e-03 13.0%┣┫ 325/2.5k [06:15<41:59, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.61e-01 grad: 1.36e+01 lr: 2.0e-03 13.0%┣┫ 326/2.5k [06:16<41:56, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.69e-01 grad: 1.37e+01 lr: 2.0e-03 13.1%┣┫ 327/2.5k [06:17<41:54, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.77e-01 grad: 1.71e+01 lr: 2.0e-03 13.1%┣┫ 328/2.5k [06:18<41:53, 1s/it]\n",
      "Loss train: 1.25e-01 val: 1.60e-01 grad: 9.69e+00 lr: 2.0e-03 13.2%┣┫ 329/2.5k [06:19<41:50, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.65e-01 grad: 1.50e+01 lr: 2.0e-03 13.2%┣┫ 330/2.5k [06:20<41:48, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.76e-01 grad: 1.10e+01 lr: 2.0e-03 13.2%┣┫ 331/2.5k [06:21<41:46, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.67e-01 grad: 9.74e+00 lr: 2.0e-03 13.3%┣┫ 332/2.5k [06:22<41:44, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.66e-01 grad: 1.69e+01 lr: 2.0e-03 13.3%┣┫ 333/2.5k [06:23<41:42, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.89e-01 grad: 1.39e+01 lr: 2.0e-03 13.4%┣┫ 334/2.5k [06:24<41:40, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.70e-01 grad: 9.44e+00 lr: 2.0e-03 13.4%┣┫ 335/2.5k [06:25<41:38, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.64e-01 grad: 1.86e+01 lr: 2.0e-03 13.4%┣┫ 336/2.5k [06:26<41:36, 1s/it]\n",
      "Loss train: 1.54e-01 val: 1.60e-01 grad: 1.19e+01 lr: 2.0e-03 13.5%┣┫ 337/2.5k [06:27<41:34, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.73e-01 grad: 1.50e+01 lr: 2.0e-03 13.5%┣┫ 338/2.5k [06:28<41:32, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.73e-01 grad: 1.44e+01 lr: 2.0e-03 13.6%┣┫ 339/2.5k [06:30<41:31, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.82e-01 grad: 1.22e+01 lr: 2.0e-03 13.6%┣┫ 340/2.5k [06:31<41:28, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.82e-01 grad: 9.73e+00 lr: 2.0e-03 13.6%┣┫ 341/2.5k [06:31<41:26, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.53e-01 grad: 8.65e+00 lr: 2.0e-03 13.7%┣┫ 342/2.5k [06:32<41:24, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.70e-01 grad: 1.79e+01 lr: 2.0e-03 13.7%┣┫ 343/2.5k [06:33<41:22, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.66e-01 grad: 1.36e+01 lr: 2.0e-03 13.8%┣┫ 344/2.5k [06:34<41:19, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.76e-01 grad: 1.55e+01 lr: 2.0e-03 13.8%┣┫ 345/2.5k [06:36<41:18, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.68e-01 grad: 1.23e+01 lr: 2.0e-03 13.8%┣┫ 346/2.5k [06:37<41:16, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.76e-01 grad: 1.45e+01 lr: 2.0e-03 13.9%┣┫ 347/2.5k [06:38<41:14, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.81e-01 grad: 1.08e+01 lr: 2.0e-03 13.9%┣┫ 348/2.5k [06:38<41:11, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.64e-01 grad: 1.17e+01 lr: 2.0e-03 14.0%┣┫ 349/2.5k [06:39<41:09, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.72e-01 grad: 1.36e+01 lr: 2.0e-03 14.0%┣┫ 350/2.5k [06:40<41:07, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.50e-01 grad: 1.14e+01 lr: 2.0e-03 14.0%┣┫ 351/2.5k [06:42<41:05, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.73e-01 grad: 2.06e+01 lr: 2.0e-03 14.1%┣┫ 352/2.5k [06:43<41:04, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.46e-01 grad: 1.30e+01 lr: 2.0e-03 14.1%┣┫ 353/2.5k [06:44<41:01, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.54e-01 grad: 1.60e+01 lr: 2.0e-03 14.2%┣┫ 354/2.5k [06:45<41:00, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.69e-01 grad: 1.25e+01 lr: 2.0e-03 14.2%┣┫ 355/2.5k [06:46<40:58, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.69e-01 grad: 1.13e+01 lr: 2.0e-03 14.2%┣┫ 356/2.5k [06:47<40:57, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.60e-01 grad: 1.45e+01 lr: 2.0e-03 14.3%┣┫ 357/2.5k [06:48<40:54, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.71e-01 grad: 1.51e+01 lr: 2.0e-03 14.3%┣┫ 358/2.5k [06:49<40:53, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.75e-01 grad: 1.08e+01 lr: 2.0e-03 14.4%┣┫ 359/2.5k [06:50<40:51, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.55e-01 grad: 1.27e+01 lr: 2.0e-03 14.4%┣┫ 360/2.5k [06:51<40:49, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.75e-01 grad: 1.93e+01 lr: 2.0e-03 14.4%┣┫ 361/2.5k [06:52<40:47, 1s/it]\n",
      "Loss train: 1.30e-01 val: 1.59e-01 grad: 1.20e+01 lr: 2.0e-03 14.5%┣┫ 362/2.5k [06:53<40:46, 1s/it]\n",
      "Loss train: 1.26e-01 val: 1.53e-01 grad: 1.39e+01 lr: 2.0e-03 14.5%┣┫ 363/2.5k [06:54<40:43, 1s/it]\n",
      "Loss train: 1.25e-01 val: 1.56e-01 grad: 1.63e+01 lr: 2.0e-03 14.6%┣┫ 364/2.5k [06:55<40:42, 1s/it]\n",
      "Loss train: 1.29e-01 val: 1.85e-01 grad: 1.10e+01 lr: 2.0e-03 14.6%┣┫ 365/2.5k [06:56<40:40, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.75e-01 grad: 9.87e+00 lr: 2.0e-03 14.6%┣┫ 366/2.5k [06:57<40:39, 1s/it]\n",
      "Loss train: 1.22e-01 val: 1.45e-01 grad: 1.17e+01 lr: 2.0e-03 14.7%┣┫ 367/2.5k [06:58<40:37, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.58e-01 grad: 1.97e+01 lr: 2.0e-03 14.7%┣┫ 368/2.5k [06:59<40:35, 1s/it]\n",
      "Loss train: 1.50e-01 val: 1.86e-01 grad: 1.04e+01 lr: 2.0e-03 14.8%┣┫ 369/2.5k [07:00<40:33, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.65e-01 grad: 1.44e+01 lr: 2.0e-03 14.8%┣┫ 370/2.5k [07:01<40:31, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.57e-01 grad: 1.36e+01 lr: 2.0e-03 14.8%┣┫ 371/2.5k [07:02<40:30, 1s/it]\n",
      "Loss train: 1.33e-01 val: 1.92e-01 grad: 1.51e+01 lr: 2.0e-03 14.9%┣┫ 372/2.5k [07:03<40:28, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.82e-01 grad: 1.09e+01 lr: 2.0e-03 14.9%┣┫ 373/2.5k [07:04<40:26, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.59e-01 grad: 1.06e+01 lr: 2.0e-03 15.0%┣┫ 374/2.5k [07:05<40:24, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.61e-01 grad: 1.43e+01 lr: 2.0e-03 15.0%┣┫ 375/2.5k [07:06<40:22, 1s/it]\n",
      "Loss train: 1.45e-01 val: 1.88e-01 grad: 2.03e+01 lr: 2.0e-03 15.0%┣┫ 376/2.5k [07:07<40:20, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.65e-01 grad: 1.10e+01 lr: 2.0e-03 15.1%┣┫ 377/2.5k [07:08<40:17, 1s/it]\n",
      "Loss train: 1.42e-01 val: 1.48e-01 grad: 1.50e+01 lr: 2.0e-03 15.1%┣┫ 378/2.5k [07:09<40:16, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.67e-01 grad: 1.52e+01 lr: 2.0e-03 15.2%┣┫ 379/2.5k [07:10<40:14, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.83e-01 grad: 1.17e+01 lr: 2.0e-03 15.2%┣┫ 380/2.5k [07:11<40:12, 1s/it]\n",
      "Loss train: 1.06e-01 val: 1.57e-01 grad: 8.19e+00 lr: 2.0e-03 15.2%┣┫ 381/2.5k [07:12<40:10, 1s/it]\n",
      "Loss train: 1.29e-01 val: 1.34e-01 grad: 1.43e+01 lr: 2.0e-03 15.3%┣┫ 382/2.5k [07:13<40:09, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.62e-01 grad: 1.60e+01 lr: 2.0e-03 15.3%┣┫ 383/2.5k [07:14<40:06, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.64e-01 grad: 1.08e+01 lr: 2.0e-03 15.4%┣┫ 384/2.5k [07:15<40:04, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.61e-01 grad: 1.47e+01 lr: 2.0e-03 15.4%┣┫ 385/2.5k [07:16<40:03, 1s/it]\n",
      "Loss train: 1.06e-01 val: 1.67e-01 grad: 1.23e+01 lr: 2.0e-03 15.4%┣┫ 386/2.5k [07:17<40:01, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.61e-01 grad: 1.38e+01 lr: 2.0e-03 15.5%┣┫ 387/2.5k [07:18<39:59, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.51e-01 grad: 1.51e+01 lr: 2.0e-03 15.5%┣┫ 388/2.5k [07:19<39:58, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.71e-01 grad: 1.26e+01 lr: 2.0e-03 15.6%┣┫ 389/2.5k [07:20<39:56, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.67e-01 grad: 1.86e+01 lr: 2.0e-03 15.6%┣┫ 390/2.5k [07:21<39:54, 1s/it]\n",
      "Loss train: 1.33e-01 val: 1.45e-01 grad: 1.28e+01 lr: 2.0e-03 15.6%┣┫ 391/2.5k [07:23<39:53, 1s/it]\n",
      "Loss train: 1.33e-01 val: 1.34e-01 grad: 1.65e+01 lr: 2.0e-03 15.7%┣┫ 392/2.5k [07:24<39:51, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.50e-01 grad: 1.68e+01 lr: 2.0e-03 15.7%┣┫ 393/2.5k [07:25<39:50, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.90e-01 grad: 1.29e+01 lr: 2.0e-03 15.8%┣┫ 394/2.5k [07:26<39:48, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.87e-01 grad: 9.27e+00 lr: 2.0e-03 15.8%┣┫ 395/2.5k [07:27<39:46, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.61e-01 grad: 1.41e+01 lr: 2.0e-03 15.8%┣┫ 396/2.5k [07:28<39:44, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.73e-01 grad: 1.31e+01 lr: 2.0e-03 15.9%┣┫ 397/2.5k [07:29<39:43, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.75e-01 grad: 1.08e+01 lr: 2.0e-03 15.9%┣┫ 398/2.5k [07:30<39:41, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.72e-01 grad: 2.19e+01 lr: 2.0e-03 16.0%┣┫ 399/2.5k [07:31<39:39, 1s/it]\n",
      "Loss train: 1.57e-01 val: 1.63e-01 grad: 1.21e+01 lr: 2.0e-03 16.0%┣┫ 400/2.5k [07:32<39:37, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.56e-01 grad: 1.35e+01 lr: 2.0e-03 16.0%┣┫ 401/2.5k [07:33<39:36, 1s/it]\n",
      "Loss train: 1.36e-01 val: 1.46e-01 grad: 1.48e+01 lr: 2.0e-03 16.1%┣┫ 402/2.5k [07:34<39:35, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.62e-01 grad: 1.47e+01 lr: 2.0e-03 16.1%┣┫ 403/2.5k [07:35<39:33, 1s/it]\n",
      "Loss train: 1.44e-01 val: 1.93e-01 grad: 1.46e+01 lr: 2.0e-03 16.2%┣┫ 404/2.5k [07:36<39:32, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.66e-01 grad: 1.05e+01 lr: 2.0e-03 16.2%┣┫ 405/2.5k [07:37<39:30, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.56e-01 grad: 1.88e+01 lr: 2.0e-03 16.2%┣┫ 406/2.5k [07:38<39:28, 1s/it]\n",
      "Loss train: 1.39e-01 val: 1.55e-01 grad: 1.31e+01 lr: 2.0e-03 16.3%┣┫ 407/2.5k [07:39<39:26, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.67e-01 grad: 1.40e+01 lr: 2.0e-03 16.3%┣┫ 408/2.5k [07:40<39:24, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.70e-01 grad: 1.43e+01 lr: 2.0e-03 16.4%┣┫ 409/2.5k [07:41<39:23, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.64e-01 grad: 1.15e+01 lr: 2.0e-03 16.4%┣┫ 410/2.5k [07:42<39:22, 1s/it]\n",
      "Loss train: 1.06e-01 val: 1.60e-01 grad: 1.34e+01 lr: 2.0e-03 16.4%┣┫ 411/2.5k [07:43<39:20, 1s/it]\n",
      "Loss train: 1.06e-01 val: 1.63e-01 grad: 1.36e+01 lr: 2.0e-03 16.5%┣┫ 412/2.5k [07:44<39:18, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.66e-01 grad: 1.12e+01 lr: 2.0e-03 16.5%┣┫ 413/2.5k [07:45<39:16, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.56e-01 grad: 1.02e+01 lr: 2.0e-03 16.6%┣┫ 414/2.5k [07:46<39:14, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.58e-01 grad: 2.11e+01 lr: 2.0e-03 16.6%┣┫ 415/2.5k [07:47<39:13, 1s/it]\n",
      "Loss train: 1.46e-01 val: 1.54e-01 grad: 1.37e+01 lr: 2.0e-03 16.6%┣┫ 416/2.5k [07:48<39:11, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.56e-01 grad: 1.44e+01 lr: 2.0e-03 16.7%┣┫ 417/2.5k [07:49<39:10, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.56e-01 grad: 1.33e+01 lr: 2.0e-03 16.7%┣┫ 418/2.5k [07:50<39:08, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.80e-01 grad: 1.49e+01 lr: 2.0e-03 16.8%┣┫ 419/2.5k [07:51<39:07, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.78e-01 grad: 8.55e+00 lr: 2.0e-03 16.8%┣┫ 420/2.5k [07:52<39:05, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.49e-01 grad: 1.48e+01 lr: 2.0e-03 16.8%┣┫ 421/2.5k [07:53<39:04, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.72e-01 grad: 1.26e+01 lr: 2.0e-03 16.9%┣┫ 422/2.5k [07:54<39:02, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.56e-01 grad: 9.11e+00 lr: 2.0e-03 16.9%┣┫ 423/2.5k [07:55<39:00, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.73e-01 grad: 1.76e+01 lr: 2.0e-03 17.0%┣┫ 424/2.5k [07:57<38:59, 1s/it]\n",
      "Loss train: 1.30e-01 val: 1.56e-01 grad: 1.37e+01 lr: 2.0e-03 17.0%┣┫ 425/2.5k [07:58<38:57, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.52e-01 grad: 1.39e+01 lr: 2.0e-03 17.0%┣┫ 426/2.5k [07:59<38:56, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.55e-01 grad: 1.32e+01 lr: 2.0e-03 17.1%┣┫ 427/2.5k [08:00<38:54, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.62e-01 grad: 1.36e+01 lr: 2.0e-03 17.1%┣┫ 428/2.5k [08:01<38:52, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.72e-01 grad: 1.22e+01 lr: 2.0e-03 17.2%┣┫ 429/2.5k [08:02<38:51, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.65e-01 grad: 9.47e+00 lr: 2.0e-03 17.2%┣┫ 430/2.5k [08:03<38:49, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.52e-01 grad: 1.41e+01 lr: 2.0e-03 17.2%┣┫ 431/2.5k [08:04<38:47, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.74e-01 grad: 2.17e+01 lr: 2.0e-03 17.3%┣┫ 432/2.5k [08:05<38:46, 1s/it]\n",
      "Loss train: 1.77e-01 val: 1.69e-01 grad: 1.13e+01 lr: 2.0e-03 17.3%┣┫ 433/2.5k [08:06<38:45, 1s/it]\n",
      "Loss train: 1.31e-01 val: 1.57e-01 grad: 1.56e+01 lr: 2.0e-03 17.4%┣┫ 434/2.5k [08:07<38:43, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.52e-01 grad: 1.39e+01 lr: 2.0e-03 17.4%┣┫ 435/2.5k [08:08<38:41, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.62e-01 grad: 1.39e+01 lr: 2.0e-03 17.4%┣┫ 436/2.5k [08:09<38:40, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.65e-01 grad: 1.37e+01 lr: 2.0e-03 17.5%┣┫ 437/2.5k [08:10<38:38, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.68e-01 grad: 9.60e+00 lr: 2.0e-03 17.5%┣┫ 438/2.5k [08:11<38:37, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.56e-01 grad: 1.48e+01 lr: 2.0e-03 17.6%┣┫ 439/2.5k [08:12<38:35, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.61e-01 grad: 1.53e+01 lr: 2.0e-03 17.6%┣┫ 440/2.5k [08:13<38:34, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.57e-01 grad: 9.81e+00 lr: 2.0e-03 17.6%┣┫ 441/2.5k [08:14<38:32, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.31e-01 grad: 1.22e+01 lr: 2.0e-03 17.7%┣┫ 442/2.5k [08:15<38:31, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.54e-01 grad: 1.98e+01 lr: 2.0e-03 17.7%┣┫ 443/2.5k [08:16<38:29, 1s/it]\n",
      "Loss train: 1.29e-01 val: 1.66e-01 grad: 1.16e+01 lr: 2.0e-03 17.8%┣┫ 444/2.5k [08:17<38:27, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.51e-01 grad: 1.21e+01 lr: 2.0e-03 17.8%┣┫ 445/2.5k [08:18<38:26, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.44e-01 grad: 1.46e+01 lr: 2.0e-03 17.8%┣┫ 446/2.5k [08:19<38:24, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.66e-01 grad: 1.14e+01 lr: 2.0e-03 17.9%┣┫ 447/2.5k [08:20<38:23, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.61e-01 grad: 9.54e+00 lr: 2.0e-03 17.9%┣┫ 448/2.5k [08:21<38:21, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.42e-01 grad: 1.33e+01 lr: 2.0e-03 18.0%┣┫ 449/2.5k [08:22<38:20, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.55e-01 grad: 1.33e+01 lr: 2.0e-03 18.0%┣┫ 450/2.5k [08:23<38:19, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.56e-01 grad: 1.39e+01 lr: 2.0e-03 18.0%┣┫ 451/2.5k [08:25<38:17, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.43e-01 grad: 1.18e+01 lr: 2.0e-03 18.1%┣┫ 452/2.5k [08:26<38:16, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.54e-01 grad: 2.08e+01 lr: 2.0e-03 18.1%┣┫ 453/2.5k [08:27<38:15, 1s/it]\n",
      "Loss train: 1.32e-01 val: 1.62e-01 grad: 1.10e+01 lr: 2.0e-03 18.2%┣┫ 454/2.5k [08:28<38:13, 1s/it]\n",
      "Loss train: 1.22e-01 val: 1.41e-01 grad: 1.21e+01 lr: 2.0e-03 18.2%┣┫ 455/2.5k [08:29<38:11, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.37e-01 grad: 1.27e+01 lr: 2.0e-03 18.2%┣┫ 456/2.5k [08:30<38:10, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.64e-01 grad: 1.49e+01 lr: 2.0e-03 18.3%┣┫ 457/2.5k [08:31<38:08, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.60e-01 grad: 1.10e+01 lr: 2.0e-03 18.3%┣┫ 458/2.5k [08:32<38:07, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.58e-01 grad: 1.18e+01 lr: 2.0e-03 18.4%┣┫ 459/2.5k [08:33<38:05, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.43e-01 grad: 1.09e+01 lr: 2.0e-03 18.4%┣┫ 460/2.5k [08:34<38:04, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.50e-01 grad: 1.32e+01 lr: 2.0e-03 18.4%┣┫ 461/2.5k [08:35<38:02, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.61e-01 grad: 1.44e+01 lr: 2.0e-03 18.5%┣┫ 462/2.5k [08:36<38:01, 1s/it]\n",
      "Loss train: 9.93e-02 val: 1.56e-01 grad: 1.33e+01 lr: 2.0e-03 18.5%┣┫ 463/2.5k [08:37<38:00, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.54e-01 grad: 1.32e+01 lr: 2.0e-03 18.6%┣┫ 464/2.5k [08:38<37:59, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.62e-01 grad: 1.59e+01 lr: 2.0e-03 18.6%┣┫ 465/2.5k [08:39<37:58, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.55e-01 grad: 1.04e+01 lr: 2.0e-03 18.6%┣┫ 466/2.5k [08:40<37:56, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.40e-01 grad: 1.23e+01 lr: 2.0e-03 18.7%┣┫ 467/2.5k [08:41<37:55, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.42e-01 grad: 1.36e+01 lr: 2.0e-03 18.7%┣┫ 468/2.5k [08:42<37:53, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.57e-01 grad: 1.49e+01 lr: 2.0e-03 18.8%┣┫ 469/2.5k [08:44<37:52, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.58e-01 grad: 1.06e+01 lr: 2.0e-03 18.8%┣┫ 470/2.5k [08:45<37:50, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.48e-01 grad: 1.03e+01 lr: 2.0e-03 18.8%┣┫ 471/2.5k [08:46<37:49, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.43e-01 grad: 1.30e+01 lr: 2.0e-03 18.9%┣┫ 472/2.5k [08:47<37:48, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.54e-01 grad: 1.29e+01 lr: 2.0e-03 18.9%┣┫ 473/2.5k [08:48<37:46, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.69e-01 grad: 1.09e+01 lr: 2.0e-03 19.0%┣┫ 474/2.5k [08:49<37:45, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.52e-01 grad: 1.35e+01 lr: 2.0e-03 19.0%┣┫ 475/2.5k [08:50<37:43, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.60e-01 grad: 1.42e+01 lr: 2.0e-03 19.0%┣┫ 476/2.5k [08:51<37:42, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.58e-01 grad: 1.31e+01 lr: 2.0e-03 19.1%┣┫ 477/2.5k [08:52<37:41, 1s/it]\n",
      "Loss train: 9.97e-02 val: 1.61e-01 grad: 1.28e+01 lr: 2.0e-03 19.1%┣┫ 478/2.5k [08:53<37:40, 1s/it]\n",
      "Loss train: 9.92e-02 val: 1.55e-01 grad: 1.29e+01 lr: 2.0e-03 19.2%┣┫ 479/2.5k [08:54<37:38, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.57e-01 grad: 2.02e+01 lr: 2.0e-03 19.2%┣┫ 480/2.5k [08:55<37:37, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.50e-01 grad: 1.19e+01 lr: 2.0e-03 19.2%┣┫ 481/2.5k [08:56<37:36, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.38e-01 grad: 1.38e+01 lr: 2.0e-03 19.3%┣┫ 482/2.5k [08:57<37:35, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.32e-01 grad: 1.48e+01 lr: 2.0e-03 19.3%┣┫ 483/2.5k [08:58<37:33, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.52e-01 grad: 1.38e+01 lr: 2.0e-03 19.4%┣┫ 484/2.5k [09:00<37:32, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.60e-01 grad: 1.02e+01 lr: 2.0e-03 19.4%┣┫ 485/2.5k [09:01<37:31, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.51e-01 grad: 1.19e+01 lr: 2.0e-03 19.4%┣┫ 486/2.5k [09:02<37:29, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.45e-01 grad: 1.39e+01 lr: 2.0e-03 19.5%┣┫ 487/2.5k [09:03<37:28, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.42e-01 grad: 1.43e+01 lr: 2.0e-03 19.5%┣┫ 488/2.5k [09:04<37:27, 1s/it]\n",
      "Loss train: 9.99e-02 val: 1.41e-01 grad: 1.23e+01 lr: 2.0e-03 19.6%┣┫ 489/2.5k [09:05<37:26, 1s/it]\n",
      "Loss train: 9.87e-02 val: 1.43e-01 grad: 1.39e+01 lr: 2.0e-03 19.6%┣┫ 490/2.5k [09:06<37:24, 1s/it]\n",
      "Loss train: 9.98e-02 val: 1.51e-01 grad: 1.30e+01 lr: 2.0e-03 19.6%┣┫ 491/2.5k [09:07<37:23, 1s/it]\n",
      "Loss train: 9.78e-02 val: 1.48e-01 grad: 1.41e+01 lr: 2.0e-03 19.7%┣┫ 492/2.5k [09:08<37:22, 1s/it]\n",
      "Loss train: 9.80e-02 val: 1.45e-01 grad: 1.83e+01 lr: 2.0e-03 19.7%┣┫ 493/2.5k [09:09<37:21, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.60e-01 grad: 1.16e+01 lr: 2.0e-03 19.8%┣┫ 494/2.5k [09:10<37:19, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.39e-01 grad: 1.17e+01 lr: 2.0e-03 19.8%┣┫ 495/2.5k [09:11<37:18, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.39e-01 grad: 1.40e+01 lr: 2.0e-03 19.8%┣┫ 496/2.5k [09:12<37:16, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.50e-01 grad: 1.44e+01 lr: 2.0e-03 19.9%┣┫ 497/2.5k [09:13<37:15, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.50e-01 grad: 1.16e+01 lr: 2.0e-03 19.9%┣┫ 498/2.5k [09:15<37:14, 1s/it]\n",
      "Loss train: 9.96e-02 val: 1.50e-01 grad: 1.19e+01 lr: 2.0e-03 20.0%┣┫ 499/2.5k [09:16<37:12, 1s/it]\n",
      "Loss train: 9.98e-02 val: 1.44e-01 grad: 1.32e+01 lr: 1.0e-03 20.0%┣┫ 500/2.5k [09:17<37:11, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.39e-01 grad: 1.34e+01 lr: 1.0e-03 20.0%┣┫ 501/2.5k [09:18<37:10, 1s/it]\n",
      "Loss train: 9.80e-02 val: 1.45e-01 grad: 1.25e+01 lr: 1.0e-03 20.1%┣┫ 502/2.5k [09:19<37:08, 1s/it]\n",
      "Loss train: 9.79e-02 val: 1.52e-01 grad: 1.21e+01 lr: 1.0e-03 20.1%┣┫ 503/2.5k [09:20<37:07, 1s/it]\n",
      "Loss train: 9.75e-02 val: 1.51e-01 grad: 1.13e+01 lr: 1.0e-03 20.2%┣┫ 504/2.5k [09:21<37:06, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.49e-01 grad: 1.22e+01 lr: 1.0e-03 20.2%┣┫ 505/2.5k [09:22<37:04, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.47e-01 grad: 1.82e+01 lr: 1.0e-03 20.2%┣┫ 506/2.5k [09:23<37:03, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.54e-01 grad: 1.25e+01 lr: 1.0e-03 20.3%┣┫ 507/2.5k [09:24<37:02, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.48e-01 grad: 1.15e+01 lr: 1.0e-03 20.3%┣┫ 508/2.5k [09:25<37:01, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.45e-01 grad: 1.40e+01 lr: 1.0e-03 20.4%┣┫ 509/2.5k [09:26<37:00, 1s/it]\n",
      "Loss train: 9.95e-02 val: 1.44e-01 grad: 1.38e+01 lr: 1.0e-03 20.4%┣┫ 510/2.5k [09:27<36:58, 1s/it]\n",
      "Loss train: 9.91e-02 val: 1.42e-01 grad: 1.38e+01 lr: 1.0e-03 20.4%┣┫ 511/2.5k [09:28<36:57, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.50e-01 grad: 1.14e+01 lr: 1.0e-03 20.5%┣┫ 512/2.5k [09:30<36:56, 1s/it]\n",
      "Loss train: 9.76e-02 val: 1.46e-01 grad: 1.18e+01 lr: 1.0e-03 20.5%┣┫ 513/2.5k [09:31<36:55, 1s/it]\n",
      "Loss train: 9.74e-02 val: 1.46e-01 grad: 1.22e+01 lr: 1.0e-03 20.6%┣┫ 514/2.5k [09:32<36:54, 1s/it]\n",
      "Loss train: 9.75e-02 val: 1.47e-01 grad: 1.21e+01 lr: 1.0e-03 20.6%┣┫ 515/2.5k [09:33<36:53, 1s/it]\n",
      "Loss train: 9.76e-02 val: 1.43e-01 grad: 1.43e+01 lr: 1.0e-03 20.6%┣┫ 516/2.5k [09:34<36:52, 1s/it]\n",
      "Loss train: 9.81e-02 val: 1.41e-01 grad: 1.36e+01 lr: 1.0e-03 20.7%┣┫ 517/2.5k [09:35<36:51, 1s/it]\n",
      "Loss train: 9.93e-02 val: 1.51e-01 grad: 1.41e+01 lr: 1.0e-03 20.7%┣┫ 518/2.5k [09:36<36:50, 1s/it]\n",
      "Loss train: 9.71e-02 val: 1.48e-01 grad: 1.07e+01 lr: 1.0e-03 20.8%┣┫ 519/2.5k [09:37<36:48, 1s/it]\n",
      "Loss train: 9.65e-02 val: 1.47e-01 grad: 1.22e+01 lr: 1.0e-03 20.8%┣┫ 520/2.5k [09:39<36:50, 1s/it]\n",
      "Loss train: 9.86e-02 val: 1.43e-01 grad: 1.76e+01 lr: 1.0e-03 20.8%┣┫ 521/2.5k [09:41<36:51, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.49e-01 grad: 1.38e+01 lr: 1.0e-03 20.9%┣┫ 522/2.5k [09:42<36:50, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.48e-01 grad: 1.30e+01 lr: 1.0e-03 20.9%┣┫ 523/2.5k [09:43<36:49, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.47e-01 grad: 1.26e+01 lr: 1.0e-03 21.0%┣┫ 524/2.5k [09:44<36:47, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.47e-01 grad: 1.15e+01 lr: 1.0e-03 21.0%┣┫ 525/2.5k [09:45<36:46, 1s/it]\n",
      "Loss train: 9.95e-02 val: 1.44e-01 grad: 1.19e+01 lr: 1.0e-03 21.0%┣┫ 526/2.5k [09:46<36:44, 1s/it]\n",
      "Loss train: 9.82e-02 val: 1.45e-01 grad: 1.42e+01 lr: 1.0e-03 21.1%┣┫ 527/2.5k [09:47<36:43, 1s/it]\n",
      "Loss train: 9.83e-02 val: 1.41e-01 grad: 1.38e+01 lr: 1.0e-03 21.1%┣┫ 528/2.5k [09:48<36:42, 1s/it]\n",
      "Loss train: 9.77e-02 val: 1.43e-01 grad: 1.40e+01 lr: 1.0e-03 21.2%┣┫ 529/2.5k [09:49<36:40, 1s/it]\n",
      "Loss train: 9.85e-02 val: 1.47e-01 grad: 1.30e+01 lr: 1.0e-03 21.2%┣┫ 530/2.5k [09:50<36:39, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.42e-01 grad: 1.06e+01 lr: 1.0e-03 21.2%┣┫ 531/2.5k [09:52<36:38, 1s/it]\n",
      "Loss train: 9.66e-02 val: 1.43e-01 grad: 1.41e+01 lr: 1.0e-03 21.3%┣┫ 532/2.5k [09:53<36:37, 1s/it]\n",
      "Loss train: 9.63e-02 val: 1.42e-01 grad: 1.40e+01 lr: 1.0e-03 21.3%┣┫ 533/2.5k [09:54<36:36, 1s/it]\n",
      "Loss train: 9.92e-02 val: 1.48e-01 grad: 1.24e+01 lr: 1.0e-03 21.4%┣┫ 534/2.5k [09:55<36:35, 1s/it]\n",
      "Loss train: 9.76e-02 val: 1.38e-01 grad: 1.19e+01 lr: 1.0e-03 21.4%┣┫ 535/2.5k [09:56<36:34, 1s/it]\n",
      "Loss train: 9.66e-02 val: 1.40e-01 grad: 1.26e+01 lr: 1.0e-03 21.4%┣┫ 536/2.5k [09:57<36:33, 1s/it]\n",
      "Loss train: 9.86e-02 val: 1.48e-01 grad: 1.29e+01 lr: 1.0e-03 21.5%┣┫ 537/2.5k [09:59<36:32, 1s/it]\n",
      "Loss train: 9.81e-02 val: 1.47e-01 grad: 1.54e+01 lr: 1.0e-03 21.5%┣┫ 538/2.5k [10:00<36:31, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.48e-01 grad: 1.11e+01 lr: 1.0e-03 21.6%┣┫ 539/2.5k [10:01<36:30, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.42e-01 grad: 1.20e+01 lr: 1.0e-03 21.6%┣┫ 540/2.5k [10:02<36:28, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.38e-01 grad: 1.22e+01 lr: 1.0e-03 21.6%┣┫ 541/2.5k [10:03<36:27, 1s/it]\n",
      "Loss train: 9.89e-02 val: 1.44e-01 grad: 1.42e+01 lr: 1.0e-03 21.7%┣┫ 542/2.5k [10:04<36:25, 1s/it]\n",
      "Loss train: 9.95e-02 val: 1.47e-01 grad: 1.34e+01 lr: 1.0e-03 21.7%┣┫ 543/2.5k [10:05<36:24, 1s/it]\n",
      "Loss train: 9.72e-02 val: 1.42e-01 grad: 1.22e+01 lr: 1.0e-03 21.8%┣┫ 544/2.5k [10:06<36:23, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.42e-01 grad: 1.18e+01 lr: 1.0e-03 21.8%┣┫ 545/2.5k [10:07<36:21, 1s/it]\n",
      "Loss train: 9.65e-02 val: 1.41e-01 grad: 1.34e+01 lr: 1.0e-03 21.8%┣┫ 546/2.5k [10:08<36:20, 1s/it]\n",
      "Loss train: 9.66e-02 val: 1.44e-01 grad: 1.35e+01 lr: 1.0e-03 21.9%┣┫ 547/2.5k [10:09<36:19, 1s/it]\n",
      "Loss train: 9.62e-02 val: 1.42e-01 grad: 1.19e+01 lr: 1.0e-03 21.9%┣┫ 548/2.5k [10:10<36:18, 1s/it]\n",
      "Loss train: 9.68e-02 val: 1.38e-01 grad: 1.27e+01 lr: 1.0e-03 22.0%┣┫ 549/2.5k [10:12<36:17, 1s/it]\n",
      "Loss train: 9.67e-02 val: 1.39e-01 grad: 1.37e+01 lr: 1.0e-03 22.0%┣┫ 550/2.5k [10:13<36:16, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.49e-01 grad: 1.34e+01 lr: 1.0e-03 22.0%┣┫ 551/2.5k [10:14<36:16, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.43e-01 grad: 1.03e+01 lr: 1.0e-03 22.1%┣┫ 552/2.5k [10:15<36:15, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.38e-01 grad: 1.40e+01 lr: 1.0e-03 22.1%┣┫ 553/2.5k [10:16<36:14, 1s/it]\n",
      "Loss train: 9.72e-02 val: 1.45e-01 grad: 1.55e+01 lr: 1.0e-03 22.2%┣┫ 554/2.5k [10:17<36:12, 1s/it]\n",
      "Loss train: 9.82e-02 val: 1.41e-01 grad: 1.39e+01 lr: 1.0e-03 22.2%┣┫ 555/2.5k [10:18<36:11, 1s/it]\n",
      "Loss train: 9.66e-02 val: 1.43e-01 grad: 1.31e+01 lr: 1.0e-03 22.2%┣┫ 556/2.5k [10:19<36:09, 1s/it]\n",
      "Loss train: 9.90e-02 val: 1.48e-01 grad: 1.12e+01 lr: 1.0e-03 22.3%┣┫ 557/2.5k [10:20<36:08, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.41e-01 grad: 1.17e+01 lr: 1.0e-03 22.3%┣┫ 558/2.5k [10:21<36:07, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.40e-01 grad: 1.29e+01 lr: 1.0e-03 22.4%┣┫ 559/2.5k [10:23<36:06, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.41e-01 grad: 1.24e+01 lr: 1.0e-03 22.4%┣┫ 560/2.5k [10:24<36:04, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.40e-01 grad: 1.30e+01 lr: 1.0e-03 22.4%┣┫ 561/2.5k [10:25<36:04, 1s/it]\n",
      "Loss train: 9.53e-02 val: 1.43e-01 grad: 1.34e+01 lr: 1.0e-03 22.5%┣┫ 562/2.5k [10:26<36:02, 1s/it]\n",
      "Loss train: 9.52e-02 val: 1.41e-01 grad: 1.39e+01 lr: 1.0e-03 22.5%┣┫ 563/2.5k [10:27<36:01, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.17e-01 grad: 1.07e+01 lr: 1.0e-03 22.6%┣┫ 564/2.5k [10:28<36:00, 1s/it]\n",
      "Loss train: 9.97e-02 val: 1.46e-01 grad: 1.67e+01 lr: 1.0e-03 22.6%┣┫ 565/2.5k [10:29<35:59, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.45e-01 grad: 1.34e+01 lr: 1.0e-03 22.6%┣┫ 566/2.5k [10:30<35:57, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.38e-01 grad: 1.41e+01 lr: 1.0e-03 22.7%┣┫ 567/2.5k [10:31<35:55, 1s/it]\n",
      "Loss train: 9.96e-02 val: 1.44e-01 grad: 1.39e+01 lr: 1.0e-03 22.7%┣┫ 568/2.5k [10:32<35:54, 1s/it]\n",
      "Loss train: 9.82e-02 val: 1.42e-01 grad: 1.16e+01 lr: 1.0e-03 22.8%┣┫ 569/2.5k [10:33<35:52, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.39e-01 grad: 1.37e+01 lr: 1.0e-03 22.8%┣┫ 570/2.5k [10:34<35:51, 1s/it]\n",
      "Loss train: 9.65e-02 val: 1.40e-01 grad: 1.40e+01 lr: 1.0e-03 22.8%┣┫ 571/2.5k [10:35<35:50, 1s/it]\n",
      "Loss train: 9.71e-02 val: 1.41e-01 grad: 1.20e+01 lr: 1.0e-03 22.9%┣┫ 572/2.5k [10:36<35:48, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.40e-01 grad: 1.17e+01 lr: 1.0e-03 22.9%┣┫ 573/2.5k [10:37<35:47, 1s/it]\n",
      "Loss train: 9.53e-02 val: 1.40e-01 grad: 1.39e+01 lr: 1.0e-03 23.0%┣┫ 574/2.5k [10:38<35:45, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.43e-01 grad: 1.33e+01 lr: 1.0e-03 23.0%┣┫ 575/2.5k [10:39<35:45, 1s/it]\n",
      "Loss train: 9.49e-02 val: 1.41e-01 grad: 1.20e+01 lr: 1.0e-03 23.0%┣┫ 576/2.5k [10:41<35:44, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.45e-01 grad: 1.14e+01 lr: 1.0e-03 23.1%┣┫ 577/2.5k [10:42<35:43, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.40e-01 grad: 1.11e+01 lr: 1.0e-03 23.1%┣┫ 578/2.5k [10:43<35:42, 1s/it]\n",
      "Loss train: 9.84e-02 val: 1.49e-01 grad: 1.59e+01 lr: 1.0e-03 23.2%┣┫ 579/2.5k [10:44<35:41, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.40e-01 grad: 1.38e+01 lr: 1.0e-03 23.2%┣┫ 580/2.5k [10:45<35:39, 1s/it]\n",
      "Loss train: 9.89e-02 val: 1.47e-01 grad: 1.31e+01 lr: 1.0e-03 23.2%┣┫ 581/2.5k [10:46<35:38, 1s/it]\n",
      "Loss train: 9.64e-02 val: 1.45e-01 grad: 1.21e+01 lr: 1.0e-03 23.3%┣┫ 582/2.5k [10:47<35:37, 1s/it]\n",
      "Loss train: 9.59e-02 val: 1.42e-01 grad: 1.37e+01 lr: 1.0e-03 23.3%┣┫ 583/2.5k [10:48<35:36, 1s/it]\n",
      "Loss train: 9.78e-02 val: 1.45e-01 grad: 1.37e+01 lr: 1.0e-03 23.4%┣┫ 584/2.5k [10:49<35:34, 1s/it]\n",
      "Loss train: 9.60e-02 val: 1.42e-01 grad: 1.01e+01 lr: 1.0e-03 23.4%┣┫ 585/2.5k [10:51<35:33, 1s/it]\n",
      "Loss train: 9.76e-02 val: 1.34e-01 grad: 1.29e+01 lr: 1.0e-03 23.4%┣┫ 586/2.5k [10:52<35:32, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.45e-01 grad: 1.25e+01 lr: 1.0e-03 23.5%┣┫ 587/2.5k [10:53<35:31, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.41e-01 grad: 1.21e+01 lr: 1.0e-03 23.5%┣┫ 588/2.5k [10:54<35:30, 1s/it]\n",
      "Loss train: 9.46e-02 val: 1.43e-01 grad: 1.31e+01 lr: 1.0e-03 23.6%┣┫ 589/2.5k [10:55<35:29, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.44e-01 grad: 1.18e+01 lr: 1.0e-03 23.6%┣┫ 590/2.5k [10:56<35:27, 1s/it]\n",
      "Loss train: 9.68e-02 val: 1.36e-01 grad: 1.80e+01 lr: 1.0e-03 23.6%┣┫ 591/2.5k [10:57<35:27, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.47e-01 grad: 1.18e+01 lr: 1.0e-03 23.7%┣┫ 592/2.5k [10:59<35:26, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.49e-01 grad: 9.96e+00 lr: 1.0e-03 23.7%┣┫ 593/2.5k [11:00<35:25, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.39e-01 grad: 1.10e+01 lr: 1.0e-03 23.8%┣┫ 594/2.5k [11:01<35:23, 1s/it]\n",
      "Loss train: 9.94e-02 val: 1.38e-01 grad: 1.38e+01 lr: 1.0e-03 23.8%┣┫ 595/2.5k [11:02<35:22, 1s/it]\n",
      "Loss train: 9.80e-02 val: 1.36e-01 grad: 1.38e+01 lr: 1.0e-03 23.8%┣┫ 596/2.5k [11:03<35:20, 1s/it]\n",
      "Loss train: 9.62e-02 val: 1.40e-01 grad: 1.28e+01 lr: 1.0e-03 23.9%┣┫ 597/2.5k [11:04<35:19, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.39e-01 grad: 1.16e+01 lr: 1.0e-03 23.9%┣┫ 598/2.5k [11:05<35:17, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.42e-01 grad: 1.24e+01 lr: 1.0e-03 24.0%┣┫ 599/2.5k [11:06<35:16, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.41e-01 grad: 1.20e+01 lr: 1.0e-03 24.0%┣┫ 600/2.5k [11:07<35:15, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.38e-01 grad: 1.30e+01 lr: 1.0e-03 24.0%┣┫ 601/2.5k [11:08<35:14, 1s/it]\n",
      "Loss train: 9.43e-02 val: 1.42e-01 grad: 1.34e+01 lr: 1.0e-03 24.1%┣┫ 602/2.5k [11:09<35:12, 1s/it]\n",
      "Loss train: 9.41e-02 val: 1.41e-01 grad: 1.19e+01 lr: 1.0e-03 24.1%┣┫ 603/2.5k [11:10<35:11, 1s/it]\n",
      "Loss train: 9.39e-02 val: 1.40e-01 grad: 1.34e+01 lr: 1.0e-03 24.2%┣┫ 604/2.5k [11:11<35:10, 1s/it]\n",
      "Loss train: 9.63e-02 val: 1.38e-01 grad: 1.72e+01 lr: 1.0e-03 24.2%┣┫ 605/2.5k [11:12<35:09, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.45e-01 grad: 1.33e+01 lr: 1.0e-03 24.2%┣┫ 606/2.5k [11:13<35:08, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.43e-01 grad: 1.07e+01 lr: 1.0e-03 24.3%┣┫ 607/2.5k [11:14<35:07, 1s/it]\n",
      "Loss train: 9.94e-02 val: 1.34e-01 grad: 1.15e+01 lr: 1.0e-03 24.3%┣┫ 608/2.5k [11:15<35:05, 1s/it]\n",
      "Loss train: 9.87e-02 val: 1.35e-01 grad: 1.28e+01 lr: 1.0e-03 24.4%┣┫ 609/2.5k [11:16<35:03, 1s/it]\n",
      "Loss train: 9.71e-02 val: 1.42e-01 grad: 1.39e+01 lr: 1.0e-03 24.4%┣┫ 610/2.5k [11:17<35:02, 1s/it]\n",
      "Loss train: 9.61e-02 val: 1.38e-01 grad: 1.07e+01 lr: 1.0e-03 24.4%┣┫ 611/2.5k [11:19<35:01, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.37e-01 grad: 1.29e+01 lr: 1.0e-03 24.5%┣┫ 612/2.5k [11:20<35:00, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.34e-01 grad: 1.20e+01 lr: 1.0e-03 24.5%┣┫ 613/2.5k [11:21<34:59, 1s/it]\n",
      "Loss train: 9.45e-02 val: 1.40e-01 grad: 1.30e+01 lr: 1.0e-03 24.6%┣┫ 614/2.5k [11:22<34:58, 1s/it]\n",
      "Loss train: 9.73e-02 val: 1.46e-01 grad: 1.11e+01 lr: 1.0e-03 24.6%┣┫ 615/2.5k [11:23<34:57, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.37e-01 grad: 1.27e+01 lr: 1.0e-03 24.6%┣┫ 616/2.5k [11:24<34:56, 1s/it]\n",
      "Loss train: 9.36e-02 val: 1.39e-01 grad: 1.36e+01 lr: 1.0e-03 24.7%┣┫ 617/2.5k [11:25<34:54, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.43e-01 grad: 1.35e+01 lr: 1.0e-03 24.7%┣┫ 618/2.5k [11:26<34:53, 1s/it]\n",
      "Loss train: 9.46e-02 val: 1.42e-01 grad: 9.80e+00 lr: 1.0e-03 24.8%┣┫ 619/2.5k [11:27<34:52, 1s/it]\n",
      "Loss train: 9.43e-02 val: 1.36e-01 grad: 1.15e+01 lr: 1.0e-03 24.8%┣┫ 620/2.5k [11:29<34:51, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.39e-01 grad: 1.36e+01 lr: 1.0e-03 24.8%┣┫ 621/2.5k [11:30<34:50, 1s/it]\n",
      "Loss train: 9.74e-02 val: 1.43e-01 grad: 1.35e+01 lr: 1.0e-03 24.9%┣┫ 622/2.5k [11:31<34:49, 1s/it]\n",
      "Loss train: 9.80e-02 val: 1.46e-01 grad: 1.18e+01 lr: 1.0e-03 24.9%┣┫ 623/2.5k [11:32<34:47, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.38e-01 grad: 1.39e+01 lr: 1.0e-03 25.0%┣┫ 624/2.5k [11:33<34:46, 1s/it]\n",
      "Loss train: 9.40e-02 val: 1.38e-01 grad: 1.31e+01 lr: 1.0e-03 25.0%┣┫ 625/2.5k [11:34<34:45, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.42e-01 grad: 1.13e+01 lr: 1.0e-03 25.0%┣┫ 626/2.5k [11:35<34:44, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.45e-01 grad: 9.84e+00 lr: 1.0e-03 25.1%┣┫ 627/2.5k [11:36<34:43, 1s/it]\n",
      "Loss train: 9.65e-02 val: 1.36e-01 grad: 1.52e+01 lr: 1.0e-03 25.1%┣┫ 628/2.5k [11:37<34:42, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.49e-01 grad: 1.40e+01 lr: 1.0e-03 25.2%┣┫ 629/2.5k [11:38<34:40, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.41e-01 grad: 9.80e+00 lr: 1.0e-03 25.2%┣┫ 630/2.5k [11:39<34:39, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.24e-01 grad: 1.40e+01 lr: 1.0e-03 25.2%┣┫ 631/2.5k [11:40<34:37, 1s/it]\n",
      "Loss train: 9.92e-02 val: 1.36e-01 grad: 1.36e+01 lr: 1.0e-03 25.3%┣┫ 632/2.5k [11:41<34:36, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.42e-01 grad: 1.18e+01 lr: 1.0e-03 25.3%┣┫ 633/2.5k [11:43<34:35, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.52e-01 grad: 1.02e+01 lr: 1.0e-03 25.4%┣┫ 634/2.5k [11:44<34:34, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.45e-01 grad: 9.92e+00 lr: 1.0e-03 25.4%┣┫ 635/2.5k [11:45<34:33, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.34e-01 grad: 1.29e+01 lr: 1.0e-03 25.4%┣┫ 636/2.5k [11:46<34:31, 1s/it]\n",
      "Loss train: 9.67e-02 val: 1.51e-01 grad: 1.22e+01 lr: 1.0e-03 25.5%┣┫ 637/2.5k [11:47<34:30, 1s/it]\n",
      "Loss train: 9.41e-02 val: 1.48e-01 grad: 9.39e+00 lr: 1.0e-03 25.5%┣┫ 638/2.5k [11:48<34:29, 1s/it]\n",
      "Loss train: 9.45e-02 val: 1.42e-01 grad: 1.37e+01 lr: 1.0e-03 25.6%┣┫ 639/2.5k [11:49<34:28, 1s/it]\n",
      "Loss train: 9.37e-02 val: 1.48e-01 grad: 1.34e+01 lr: 1.0e-03 25.6%┣┫ 640/2.5k [11:50<34:27, 1s/it]\n",
      "Loss train: 9.33e-02 val: 1.43e-01 grad: 1.37e+01 lr: 1.0e-03 25.6%┣┫ 641/2.5k [11:51<34:26, 1s/it]\n",
      "Loss train: 9.77e-02 val: 1.49e-01 grad: 1.57e+01 lr: 1.0e-03 25.7%┣┫ 642/2.5k [11:52<34:25, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.50e-01 grad: 8.85e+00 lr: 1.0e-03 25.7%┣┫ 643/2.5k [11:53<34:24, 1s/it]\n",
      "Loss train: 9.74e-02 val: 1.38e-01 grad: 1.03e+01 lr: 1.0e-03 25.8%┣┫ 644/2.5k [11:54<34:22, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.31e-01 grad: 1.28e+01 lr: 1.0e-03 25.8%┣┫ 645/2.5k [11:55<34:21, 1s/it]\n",
      "Loss train: 9.71e-02 val: 1.34e-01 grad: 1.30e+01 lr: 1.0e-03 25.8%┣┫ 646/2.5k [11:57<34:20, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.44e-01 grad: 1.32e+01 lr: 1.0e-03 25.9%┣┫ 647/2.5k [11:58<34:19, 1s/it]\n",
      "Loss train: 9.87e-02 val: 1.50e-01 grad: 9.85e+00 lr: 1.0e-03 25.9%┣┫ 648/2.5k [11:59<34:17, 1s/it]\n",
      "Loss train: 9.39e-02 val: 1.42e-01 grad: 1.13e+01 lr: 1.0e-03 26.0%┣┫ 649/2.5k [12:00<34:16, 1s/it]\n",
      "Loss train: 9.31e-02 val: 1.43e-01 grad: 1.35e+01 lr: 1.0e-03 26.0%┣┫ 650/2.5k [12:01<34:15, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.46e-01 grad: 1.19e+01 lr: 1.0e-03 26.0%┣┫ 651/2.5k [12:02<34:14, 1s/it]\n",
      "Loss train: 9.32e-02 val: 1.39e-01 grad: 1.07e+01 lr: 1.0e-03 26.1%┣┫ 652/2.5k [12:03<34:13, 1s/it]\n",
      "Loss train: 9.36e-02 val: 1.43e-01 grad: 1.40e+01 lr: 1.0e-03 26.1%┣┫ 653/2.5k [12:04<34:12, 1s/it]\n",
      "Loss train: 9.43e-02 val: 1.44e-01 grad: 1.08e+01 lr: 1.0e-03 26.2%┣┫ 654/2.5k [12:06<34:11, 1s/it]\n",
      "Loss train: 9.46e-02 val: 1.36e-01 grad: 1.28e+01 lr: 1.0e-03 26.2%┣┫ 655/2.5k [12:07<34:10, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.31e-01 grad: 1.21e+01 lr: 1.0e-03 26.2%┣┫ 656/2.5k [12:08<34:09, 1s/it]\n",
      "Loss train: 9.41e-02 val: 1.44e-01 grad: 1.19e+01 lr: 1.0e-03 26.3%┣┫ 657/2.5k [12:09<34:07, 1s/it]\n",
      "Loss train: 9.85e-02 val: 1.48e-01 grad: 9.06e+00 lr: 1.0e-03 26.3%┣┫ 658/2.5k [12:10<34:06, 1s/it]\n",
      "Loss train: 9.29e-02 val: 1.42e-01 grad: 1.93e+01 lr: 1.0e-03 26.4%┣┫ 659/2.5k [12:11<34:05, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.44e-01 grad: 1.07e+01 lr: 1.0e-03 26.4%┣┫ 660/2.5k [12:12<34:03, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.30e-01 grad: 1.02e+01 lr: 1.0e-03 26.4%┣┫ 661/2.5k [12:13<34:02, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.41e-01 grad: 1.20e+01 lr: 1.0e-03 26.5%┣┫ 662/2.5k [12:14<34:00, 1s/it]\n",
      "Loss train: 9.77e-02 val: 1.46e-01 grad: 1.30e+01 lr: 1.0e-03 26.5%┣┫ 663/2.5k [12:15<33:59, 1s/it]\n",
      "Loss train: 9.58e-02 val: 1.43e-01 grad: 1.33e+01 lr: 1.0e-03 26.6%┣┫ 664/2.5k [12:16<33:58, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.50e-01 grad: 1.22e+01 lr: 1.0e-03 26.6%┣┫ 665/2.5k [12:17<33:56, 1s/it]\n",
      "Loss train: 9.62e-02 val: 1.44e-01 grad: 8.52e+00 lr: 1.0e-03 26.6%┣┫ 666/2.5k [12:18<33:55, 1s/it]\n",
      "Loss train: 9.97e-02 val: 1.29e-01 grad: 1.29e+01 lr: 1.0e-03 26.7%┣┫ 667/2.5k [12:19<33:54, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.35e-01 grad: 1.25e+01 lr: 1.0e-03 26.7%┣┫ 668/2.5k [12:20<33:52, 1s/it]\n",
      "Loss train: 9.44e-02 val: 1.45e-01 grad: 1.27e+01 lr: 1.0e-03 26.8%┣┫ 669/2.5k [12:21<33:51, 1s/it]\n",
      "Loss train: 9.30e-02 val: 1.42e-01 grad: 1.04e+01 lr: 1.0e-03 26.8%┣┫ 670/2.5k [12:22<33:50, 1s/it]\n",
      "Loss train: 9.43e-02 val: 1.45e-01 grad: 1.01e+01 lr: 1.0e-03 26.8%┣┫ 671/2.5k [12:23<33:49, 1s/it]\n",
      "Loss train: 9.22e-02 val: 1.39e-01 grad: 1.16e+01 lr: 1.0e-03 26.9%┣┫ 672/2.5k [12:24<33:48, 1s/it]\n",
      "Loss train: 9.38e-02 val: 1.37e-01 grad: 1.37e+01 lr: 1.0e-03 26.9%┣┫ 673/2.5k [12:26<33:47, 1s/it]\n",
      "Loss train: 9.30e-02 val: 1.41e-01 grad: 1.31e+01 lr: 1.0e-03 27.0%┣┫ 674/2.5k [12:27<33:46, 1s/it]\n",
      "Loss train: 9.44e-02 val: 1.46e-01 grad: 1.27e+01 lr: 1.0e-03 27.0%┣┫ 675/2.5k [12:28<33:45, 1s/it]\n",
      "Loss train: 9.39e-02 val: 1.34e-01 grad: 1.08e+01 lr: 1.0e-03 27.0%┣┫ 676/2.5k [12:29<33:43, 1s/it]\n",
      "Loss train: 9.94e-02 val: 1.34e-01 grad: 1.71e+01 lr: 1.0e-03 27.1%┣┫ 677/2.5k [12:30<33:42, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.41e-01 grad: 1.15e+01 lr: 1.0e-03 27.1%┣┫ 678/2.5k [12:31<33:41, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.30e-01 grad: 1.17e+01 lr: 1.0e-03 27.2%┣┫ 679/2.5k [12:32<33:40, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.28e-01 grad: 1.27e+01 lr: 1.0e-03 27.2%┣┫ 680/2.5k [12:33<33:39, 1s/it]\n",
      "Loss train: 9.53e-02 val: 1.44e-01 grad: 1.15e+01 lr: 1.0e-03 27.2%┣┫ 681/2.5k [12:34<33:38, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.50e-01 grad: 8.60e+00 lr: 1.0e-03 27.3%┣┫ 682/2.5k [12:35<33:36, 1s/it]\n",
      "Loss train: 9.42e-02 val: 1.38e-01 grad: 1.13e+01 lr: 1.0e-03 27.3%┣┫ 683/2.5k [12:36<33:35, 1s/it]\n",
      "Loss train: 9.31e-02 val: 1.40e-01 grad: 1.19e+01 lr: 1.0e-03 27.4%┣┫ 684/2.5k [12:37<33:34, 1s/it]\n",
      "Loss train: 9.31e-02 val: 1.36e-01 grad: 1.33e+01 lr: 1.0e-03 27.4%┣┫ 685/2.5k [12:39<33:33, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.35e-01 grad: 1.33e+01 lr: 1.0e-03 27.4%┣┫ 686/2.5k [12:40<33:32, 1s/it]\n",
      "Loss train: 9.28e-02 val: 1.41e-01 grad: 1.17e+01 lr: 1.0e-03 27.5%┣┫ 687/2.5k [12:41<33:31, 1s/it]\n",
      "Loss train: 9.35e-02 val: 1.42e-01 grad: 9.95e+00 lr: 1.0e-03 27.5%┣┫ 688/2.5k [12:42<33:29, 1s/it]\n",
      "Loss train: 9.92e-02 val: 1.22e-01 grad: 1.04e+01 lr: 1.0e-03 27.6%┣┫ 689/2.5k [12:43<33:28, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.37e-01 grad: 1.86e+01 lr: 1.0e-03 27.6%┣┫ 690/2.5k [12:44<33:27, 1s/it]\n",
      "Loss train: 1.22e-01 val: 1.35e-01 grad: 1.47e+01 lr: 1.0e-03 27.6%┣┫ 691/2.5k [12:45<33:26, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.35e-01 grad: 1.25e+01 lr: 1.0e-03 27.7%┣┫ 692/2.5k [12:46<33:24, 1s/it]\n",
      "Loss train: 9.78e-02 val: 1.42e-01 grad: 1.33e+01 lr: 1.0e-03 27.7%┣┫ 693/2.5k [12:47<33:23, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.33e-01 grad: 1.33e+01 lr: 1.0e-03 27.8%┣┫ 694/2.5k [12:48<33:22, 1s/it]\n",
      "Loss train: 9.41e-02 val: 1.39e-01 grad: 1.21e+01 lr: 1.0e-03 27.8%┣┫ 695/2.5k [12:49<33:21, 1s/it]\n",
      "Loss train: 9.63e-02 val: 1.44e-01 grad: 1.06e+01 lr: 1.0e-03 27.8%┣┫ 696/2.5k [12:50<33:19, 1s/it]\n",
      "Loss train: 9.32e-02 val: 1.43e-01 grad: 9.80e+00 lr: 1.0e-03 27.9%┣┫ 697/2.5k [12:51<33:18, 1s/it]\n",
      "Loss train: 9.71e-02 val: 1.32e-01 grad: 1.33e+01 lr: 1.0e-03 27.9%┣┫ 698/2.5k [12:52<33:17, 1s/it]\n",
      "Loss train: 9.22e-02 val: 1.42e-01 grad: 1.22e+01 lr: 1.0e-03 28.0%┣┫ 699/2.5k [12:53<33:15, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.49e-01 grad: 9.16e+00 lr: 1.0e-03 28.0%┣┫ 700/2.5k [12:55<33:15, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.37e-01 grad: 1.15e+01 lr: 1.0e-03 28.0%┣┫ 701/2.5k [12:56<33:14, 1s/it]\n",
      "Loss train: 9.72e-02 val: 1.30e-01 grad: 1.36e+01 lr: 1.0e-03 28.1%┣┫ 702/2.5k [12:57<33:14, 1s/it]\n",
      "Loss train: 9.53e-02 val: 1.43e-01 grad: 1.21e+01 lr: 1.0e-03 28.1%┣┫ 703/2.5k [12:59<33:13, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.45e-01 grad: 9.52e+00 lr: 1.0e-03 28.2%┣┫ 704/2.5k [13:00<33:12, 1s/it]\n",
      "Loss train: 9.49e-02 val: 1.34e-01 grad: 1.11e+01 lr: 1.0e-03 28.2%┣┫ 705/2.5k [13:01<33:11, 1s/it]\n",
      "Loss train: 9.21e-02 val: 1.38e-01 grad: 1.32e+01 lr: 1.0e-03 28.2%┣┫ 706/2.5k [13:02<33:10, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.37e-01 grad: 1.33e+01 lr: 1.0e-03 28.3%┣┫ 707/2.5k [13:03<33:09, 1s/it]\n",
      "Loss train: 9.42e-02 val: 1.32e-01 grad: 1.71e+01 lr: 1.0e-03 28.3%┣┫ 708/2.5k [13:04<33:08, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.47e-01 grad: 1.02e+01 lr: 1.0e-03 28.4%┣┫ 709/2.5k [13:05<33:07, 1s/it]\n",
      "Loss train: 9.90e-02 val: 1.38e-01 grad: 1.12e+01 lr: 1.0e-03 28.4%┣┫ 710/2.5k [13:07<33:06, 1s/it]\n",
      "Loss train: 9.90e-02 val: 1.37e-01 grad: 1.18e+01 lr: 1.0e-03 28.4%┣┫ 711/2.5k [13:08<33:05, 1s/it]\n",
      "Loss train: 9.66e-02 val: 1.47e-01 grad: 1.17e+01 lr: 1.0e-03 28.5%┣┫ 712/2.5k [13:09<33:04, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.47e-01 grad: 8.90e+00 lr: 1.0e-03 28.5%┣┫ 713/2.5k [13:10<33:04, 1s/it]\n",
      "Loss train: 9.40e-02 val: 1.38e-01 grad: 1.25e+01 lr: 1.0e-03 28.6%┣┫ 714/2.5k [13:12<33:03, 1s/it]\n",
      "Loss train: 9.58e-02 val: 1.46e-01 grad: 1.20e+01 lr: 1.0e-03 28.6%┣┫ 715/2.5k [13:13<33:02, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.39e-01 grad: 1.30e+01 lr: 1.0e-03 28.6%┣┫ 716/2.5k [13:14<33:01, 1s/it]\n",
      "Loss train: 9.40e-02 val: 1.44e-01 grad: 1.17e+01 lr: 1.0e-03 28.7%┣┫ 717/2.5k [13:15<33:00, 1s/it]\n",
      "Loss train: 9.22e-02 val: 1.40e-01 grad: 9.05e+00 lr: 1.0e-03 28.7%┣┫ 718/2.5k [13:16<33:00, 1s/it]\n",
      "Loss train: 9.25e-02 val: 1.29e-01 grad: 1.24e+01 lr: 1.0e-03 28.8%┣┫ 719/2.5k [13:18<32:59, 1s/it]\n",
      "Loss train: 9.49e-02 val: 1.30e-01 grad: 1.19e+01 lr: 1.0e-03 28.8%┣┫ 720/2.5k [13:19<32:59, 1s/it]\n",
      "Loss train: 9.36e-02 val: 1.31e-01 grad: 1.11e+01 lr: 1.0e-03 28.8%┣┫ 721/2.5k [13:21<32:58, 1s/it]\n",
      "Loss train: 9.32e-02 val: 1.40e-01 grad: 1.13e+01 lr: 1.0e-03 28.9%┣┫ 722/2.5k [13:22<32:58, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.41e-01 grad: 1.65e+01 lr: 1.0e-03 28.9%┣┫ 723/2.5k [13:24<32:58, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.40e-01 grad: 1.05e+01 lr: 1.0e-03 29.0%┣┫ 724/2.5k [13:25<32:58, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.18e-01 grad: 1.17e+01 lr: 1.0e-03 29.0%┣┫ 725/2.5k [13:27<32:58, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.21e-01 grad: 1.29e+01 lr: 1.0e-03 29.0%┣┫ 726/2.5k [13:28<32:57, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.36e-01 grad: 1.31e+01 lr: 1.0e-03 29.1%┣┫ 727/2.5k [13:29<32:57, 1s/it]\n",
      "Loss train: 9.90e-02 val: 1.48e-01 grad: 1.12e+01 lr: 1.0e-03 29.1%┣┫ 728/2.5k [13:30<32:55, 1s/it]\n",
      "Loss train: 9.42e-02 val: 1.42e-01 grad: 9.00e+00 lr: 1.0e-03 29.2%┣┫ 729/2.5k [13:32<32:54, 1s/it]\n",
      "Loss train: 9.39e-02 val: 1.38e-01 grad: 1.09e+01 lr: 1.0e-03 29.2%┣┫ 730/2.5k [13:33<32:53, 1s/it]\n",
      "Loss train: 9.70e-02 val: 1.30e-01 grad: 1.16e+01 lr: 1.0e-03 29.2%┣┫ 731/2.5k [13:34<32:52, 1s/it]\n",
      "Loss train: 9.28e-02 val: 1.38e-01 grad: 1.27e+01 lr: 1.0e-03 29.3%┣┫ 732/2.5k [13:35<32:50, 1s/it]\n",
      "Loss train: 9.44e-02 val: 1.46e-01 grad: 1.24e+01 lr: 1.0e-03 29.3%┣┫ 733/2.5k [13:36<32:49, 1s/it]\n",
      "Loss train: 9.19e-02 val: 1.38e-01 grad: 1.20e+01 lr: 1.0e-03 29.4%┣┫ 734/2.5k [13:37<32:48, 1s/it]\n",
      "Loss train: 9.25e-02 val: 1.45e-01 grad: 1.07e+01 lr: 1.0e-03 29.4%┣┫ 735/2.5k [13:38<32:47, 1s/it]\n",
      "Loss train: 9.26e-02 val: 1.31e-01 grad: 9.99e+00 lr: 1.0e-03 29.4%┣┫ 736/2.5k [13:39<32:46, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.40e-01 grad: 1.83e+01 lr: 1.0e-03 29.5%┣┫ 737/2.5k [13:40<32:45, 1s/it]\n",
      "Loss train: 1.28e-01 val: 1.30e-01 grad: 1.38e+01 lr: 1.0e-03 29.5%┣┫ 738/2.5k [13:42<32:44, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.30e-01 grad: 1.43e+01 lr: 1.0e-03 29.6%┣┫ 739/2.5k [13:43<32:43, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.24e-01 grad: 1.30e+01 lr: 1.0e-03 29.6%┣┫ 740/2.5k [13:44<32:42, 1s/it]\n",
      "Loss train: 9.53e-02 val: 1.37e-01 grad: 1.17e+01 lr: 1.0e-03 29.6%┣┫ 741/2.5k [13:45<32:40, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.58e-01 grad: 1.10e+01 lr: 1.0e-03 29.7%┣┫ 742/2.5k [13:46<32:39, 1s/it]\n",
      "Loss train: 9.39e-02 val: 1.48e-01 grad: 7.48e+00 lr: 1.0e-03 29.7%┣┫ 743/2.5k [13:47<32:38, 1s/it]\n",
      "Loss train: 9.31e-02 val: 1.40e-01 grad: 1.27e+01 lr: 1.0e-03 29.8%┣┫ 744/2.5k [13:48<32:36, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.31e-01 grad: 1.26e+01 lr: 1.0e-03 29.8%┣┫ 745/2.5k [13:49<32:35, 1s/it]\n",
      "Loss train: 9.16e-02 val: 1.41e-01 grad: 1.31e+01 lr: 1.0e-03 29.8%┣┫ 746/2.5k [13:50<32:34, 1s/it]\n",
      "Loss train: 9.12e-02 val: 1.42e-01 grad: 1.21e+01 lr: 1.0e-03 29.9%┣┫ 747/2.5k [13:51<32:33, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.53e-01 grad: 1.99e+01 lr: 1.0e-03 29.9%┣┫ 748/2.5k [13:52<32:32, 1s/it]\n",
      "Loss train: 1.22e-01 val: 1.37e-01 grad: 1.21e+01 lr: 1.0e-03 30.0%┣┫ 749/2.5k [13:53<32:31, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.22e-01 grad: 1.04e+01 lr: 1.0e-03 30.0%┣┫ 750/2.5k [13:55<32:30, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.27e-01 grad: 1.21e+01 lr: 1.0e-03 30.0%┣┫ 751/2.5k [13:56<32:29, 1s/it]\n",
      "Loss train: 9.50e-02 val: 1.44e-01 grad: 1.25e+01 lr: 1.0e-03 30.1%┣┫ 752/2.5k [13:57<32:28, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.57e-01 grad: 1.07e+01 lr: 1.0e-03 30.1%┣┫ 753/2.5k [13:58<32:27, 1s/it]\n",
      "Loss train: 9.39e-02 val: 1.47e-01 grad: 7.86e+00 lr: 1.0e-03 30.2%┣┫ 754/2.5k [13:59<32:25, 1s/it]\n",
      "Loss train: 9.42e-02 val: 1.38e-01 grad: 1.27e+01 lr: 1.0e-03 30.2%┣┫ 755/2.5k [14:00<32:24, 1s/it]\n",
      "Loss train: 9.17e-02 val: 1.45e-01 grad: 1.31e+01 lr: 1.0e-03 30.2%┣┫ 756/2.5k [14:01<32:23, 1s/it]\n",
      "Loss train: 9.14e-02 val: 1.45e-01 grad: 1.17e+01 lr: 1.0e-03 30.3%┣┫ 757/2.5k [14:02<32:21, 1s/it]\n",
      "Loss train: 9.07e-02 val: 1.39e-01 grad: 1.12e+01 lr: 1.0e-03 30.3%┣┫ 758/2.5k [14:03<32:20, 1s/it]\n",
      "Loss train: 9.19e-02 val: 1.38e-01 grad: 1.45e+01 lr: 1.0e-03 30.4%┣┫ 759/2.5k [14:04<32:19, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.33e-01 grad: 1.29e+01 lr: 1.0e-03 30.4%┣┫ 760/2.5k [14:05<32:18, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.43e-01 grad: 1.15e+01 lr: 1.0e-03 30.4%┣┫ 761/2.5k [14:06<32:17, 1s/it]\n",
      "Loss train: 9.21e-02 val: 1.33e-01 grad: 1.26e+01 lr: 1.0e-03 30.5%┣┫ 762/2.5k [14:08<32:16, 1s/it]\n",
      "Loss train: 9.17e-02 val: 1.30e-01 grad: 1.27e+01 lr: 1.0e-03 30.5%┣┫ 763/2.5k [14:09<32:14, 1s/it]\n",
      "Loss train: 9.61e-02 val: 1.42e-01 grad: 1.08e+01 lr: 1.0e-03 30.6%┣┫ 764/2.5k [14:10<32:13, 1s/it]\n",
      "Loss train: 9.13e-02 val: 1.26e-01 grad: 1.02e+01 lr: 1.0e-03 30.6%┣┫ 765/2.5k [14:11<32:12, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.43e-01 grad: 1.56e+01 lr: 1.0e-03 30.6%┣┫ 766/2.5k [14:12<32:11, 1s/it]\n",
      "Loss train: 1.13e-01 val: 1.27e-01 grad: 1.12e+01 lr: 1.0e-03 30.7%┣┫ 767/2.5k [14:13<32:10, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.21e-01 grad: 1.16e+01 lr: 1.0e-03 30.7%┣┫ 768/2.5k [14:14<32:08, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.29e-01 grad: 1.19e+01 lr: 1.0e-03 30.8%┣┫ 769/2.5k [14:15<32:07, 1s/it]\n",
      "Loss train: 9.41e-02 val: 1.37e-01 grad: 1.14e+01 lr: 1.0e-03 30.8%┣┫ 770/2.5k [14:16<32:06, 1s/it]\n",
      "Loss train: 9.73e-02 val: 1.46e-01 grad: 1.01e+01 lr: 1.0e-03 30.8%┣┫ 771/2.5k [14:17<32:04, 1s/it]\n",
      "Loss train: 9.25e-02 val: 1.39e-01 grad: 1.04e+01 lr: 1.0e-03 30.9%┣┫ 772/2.5k [14:18<32:03, 1s/it]\n",
      "Loss train: 9.37e-02 val: 1.34e-01 grad: 1.27e+01 lr: 1.0e-03 30.9%┣┫ 773/2.5k [14:19<32:02, 1s/it]\n",
      "Loss train: 9.11e-02 val: 1.40e-01 grad: 1.29e+01 lr: 1.0e-03 31.0%┣┫ 774/2.5k [14:20<32:00, 1s/it]\n",
      "Loss train: 9.17e-02 val: 1.35e-01 grad: 1.28e+01 lr: 1.0e-03 31.0%┣┫ 775/2.5k [14:21<31:59, 1s/it]\n",
      "Loss train: 9.35e-02 val: 1.46e-01 grad: 1.09e+01 lr: 1.0e-03 31.0%┣┫ 776/2.5k [14:22<31:57, 1s/it]\n",
      "Loss train: 9.06e-02 val: 1.35e-01 grad: 1.10e+01 lr: 1.0e-03 31.1%┣┫ 777/2.5k [14:23<31:56, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.48e-01 grad: 1.52e+01 lr: 1.0e-03 31.1%┣┫ 778/2.5k [14:24<31:55, 1s/it]\n",
      "Loss train: 9.99e-02 val: 1.34e-01 grad: 1.28e+01 lr: 1.0e-03 31.2%┣┫ 779/2.5k [14:25<31:54, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.30e-01 grad: 1.30e+01 lr: 1.0e-03 31.2%┣┫ 780/2.5k [14:26<31:53, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.44e-01 grad: 1.25e+01 lr: 1.0e-03 31.2%┣┫ 781/2.5k [14:27<31:51, 1s/it]\n",
      "Loss train: 9.21e-02 val: 1.34e-01 grad: 8.89e+00 lr: 1.0e-03 31.3%┣┫ 782/2.5k [14:28<31:50, 1s/it]\n",
      "Loss train: 9.11e-02 val: 1.32e-01 grad: 1.08e+01 lr: 1.0e-03 31.3%┣┫ 783/2.5k [14:29<31:49, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.31e-01 grad: 1.28e+01 lr: 1.0e-03 31.4%┣┫ 784/2.5k [14:30<31:48, 1s/it]\n",
      "Loss train: 9.00e-02 val: 1.33e-01 grad: 1.28e+01 lr: 1.0e-03 31.4%┣┫ 785/2.5k [14:32<31:46, 1s/it]\n",
      "Loss train: 9.15e-02 val: 1.26e-01 grad: 1.28e+01 lr: 1.0e-03 31.4%┣┫ 786/2.5k [14:33<31:45, 1s/it]\n",
      "Loss train: 9.02e-02 val: 1.31e-01 grad: 1.16e+01 lr: 1.0e-03 31.5%┣┫ 787/2.5k [14:34<31:44, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.42e-01 grad: 1.24e+01 lr: 1.0e-03 31.5%┣┫ 788/2.5k [14:35<31:43, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.42e-01 grad: 1.58e+01 lr: 1.0e-03 31.6%┣┫ 789/2.5k [14:36<31:42, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.32e-01 grad: 9.27e+00 lr: 1.0e-03 31.6%┣┫ 790/2.5k [14:37<31:40, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.20e-01 grad: 9.62e+00 lr: 1.0e-03 31.6%┣┫ 791/2.5k [14:38<31:39, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.21e-01 grad: 1.23e+01 lr: 1.0e-03 31.7%┣┫ 792/2.5k [14:39<31:38, 1s/it]\n",
      "Loss train: 9.61e-02 val: 1.29e-01 grad: 1.21e+01 lr: 1.0e-03 31.7%┣┫ 793/2.5k [14:40<31:37, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.48e-01 grad: 1.09e+01 lr: 1.0e-03 31.8%┣┫ 794/2.5k [14:41<31:35, 1s/it]\n",
      "Loss train: 9.37e-02 val: 1.39e-01 grad: 8.94e+00 lr: 1.0e-03 31.8%┣┫ 795/2.5k [14:42<31:34, 1s/it]\n",
      "Loss train: 9.29e-02 val: 1.36e-01 grad: 1.04e+01 lr: 1.0e-03 31.8%┣┫ 796/2.5k [14:43<31:32, 1s/it]\n",
      "Loss train: 9.40e-02 val: 1.27e-01 grad: 1.28e+01 lr: 1.0e-03 31.9%┣┫ 797/2.5k [14:44<31:31, 1s/it]\n",
      "Loss train: 9.18e-02 val: 1.34e-01 grad: 1.12e+01 lr: 1.0e-03 31.9%┣┫ 798/2.5k [14:45<31:30, 1s/it]\n",
      "Loss train: 9.07e-02 val: 1.39e-01 grad: 1.23e+01 lr: 1.0e-03 32.0%┣┫ 799/2.5k [14:46<31:29, 1s/it]\n",
      "Loss train: 9.17e-02 val: 1.40e-01 grad: 1.19e+01 lr: 1.0e-03 32.0%┣┫ 800/2.5k [14:47<31:28, 1s/it]\n",
      "Loss train: 8.95e-02 val: 1.33e-01 grad: 9.78e+00 lr: 1.0e-03 32.0%┣┫ 801/2.5k [14:48<31:27, 1s/it]\n",
      "Loss train: 9.91e-02 val: 1.38e-01 grad: 2.02e+01 lr: 1.0e-03 32.1%┣┫ 802/2.5k [14:50<31:26, 1s/it]\n",
      "Loss train: 1.20e-01 val: 1.27e-01 grad: 1.37e+01 lr: 1.0e-03 32.1%┣┫ 803/2.5k [14:51<31:24, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.28e-01 grad: 1.23e+01 lr: 1.0e-03 32.2%┣┫ 804/2.5k [14:52<31:23, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.35e-01 grad: 1.19e+01 lr: 1.0e-03 32.2%┣┫ 805/2.5k [14:53<31:22, 1s/it]\n",
      "Loss train: 9.35e-02 val: 1.33e-01 grad: 1.27e+01 lr: 1.0e-03 32.2%┣┫ 806/2.5k [14:54<31:21, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.34e-01 grad: 1.28e+01 lr: 1.0e-03 32.3%┣┫ 807/2.5k [14:55<31:20, 1s/it]\n",
      "Loss train: 9.38e-02 val: 1.36e-01 grad: 1.26e+01 lr: 1.0e-03 32.3%┣┫ 808/2.5k [14:56<31:18, 1s/it]\n",
      "Loss train: 9.12e-02 val: 1.34e-01 grad: 9.30e+00 lr: 1.0e-03 32.4%┣┫ 809/2.5k [14:57<31:17, 1s/it]\n",
      "Loss train: 9.03e-02 val: 1.31e-01 grad: 1.23e+01 lr: 1.0e-03 32.4%┣┫ 810/2.5k [14:58<31:16, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.26e-01 grad: 1.27e+01 lr: 1.0e-03 32.4%┣┫ 811/2.5k [14:59<31:15, 1s/it]\n",
      "Loss train: 8.90e-02 val: 1.31e-01 grad: 1.19e+01 lr: 1.0e-03 32.5%┣┫ 812/2.5k [15:00<31:14, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.32e-01 grad: 1.11e+01 lr: 1.0e-03 32.5%┣┫ 813/2.5k [15:01<31:13, 1s/it]\n",
      "Loss train: 8.86e-02 val: 1.28e-01 grad: 1.26e+01 lr: 1.0e-03 32.6%┣┫ 814/2.5k [15:03<31:12, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.11e-01 grad: 1.04e+01 lr: 1.0e-03 32.6%┣┫ 815/2.5k [15:04<31:11, 1s/it]\n",
      "Loss train: 1.48e-01 val: 1.52e-01 grad: 1.94e+01 lr: 1.0e-03 32.6%┣┫ 816/2.5k [15:05<31:10, 1s/it]\n",
      "Loss train: 1.15e-01 val: 1.23e-01 grad: 1.71e+01 lr: 1.0e-03 32.7%┣┫ 817/2.5k [15:06<31:09, 1s/it]\n",
      "Loss train: 1.84e-01 val: 1.88e-01 grad: 1.37e+01 lr: 1.0e-03 32.7%┣┫ 818/2.5k [15:07<31:07, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.27e-01 grad: 1.96e+01 lr: 1.0e-03 32.8%┣┫ 819/2.5k [15:08<31:06, 1s/it]\n",
      "Loss train: 1.16e-01 val: 1.47e-01 grad: 1.12e+01 lr: 1.0e-03 32.8%┣┫ 820/2.5k [15:09<31:05, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.40e-01 grad: 1.08e+01 lr: 1.0e-03 32.8%┣┫ 821/2.5k [15:10<31:04, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.40e-01 grad: 1.16e+01 lr: 1.0e-03 32.9%┣┫ 822/2.5k [15:11<31:03, 1s/it]\n",
      "Loss train: 9.88e-02 val: 1.27e-01 grad: 1.29e+01 lr: 1.0e-03 32.9%┣┫ 823/2.5k [15:12<31:01, 1s/it]\n",
      "Loss train: 9.60e-02 val: 1.27e-01 grad: 1.12e+01 lr: 1.0e-03 33.0%┣┫ 824/2.5k [15:13<31:00, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.37e-01 grad: 1.12e+01 lr: 1.0e-03 33.0%┣┫ 825/2.5k [15:14<30:59, 1s/it]\n",
      "Loss train: 9.37e-02 val: 1.24e-01 grad: 1.04e+01 lr: 1.0e-03 33.0%┣┫ 826/2.5k [15:15<30:57, 1s/it]\n",
      "Loss train: 9.44e-02 val: 1.20e-01 grad: 1.12e+01 lr: 1.0e-03 33.1%┣┫ 827/2.5k [15:16<30:56, 1s/it]\n",
      "Loss train: 9.30e-02 val: 1.29e-01 grad: 1.23e+01 lr: 1.0e-03 33.1%┣┫ 828/2.5k [15:17<30:55, 1s/it]\n",
      "Loss train: 9.18e-02 val: 1.22e-01 grad: 1.05e+01 lr: 1.0e-03 33.2%┣┫ 829/2.5k [15:19<30:54, 1s/it]\n",
      "Loss train: 9.17e-02 val: 1.22e-01 grad: 1.22e+01 lr: 1.0e-03 33.2%┣┫ 830/2.5k [15:20<30:53, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.39e-01 grad: 1.11e+01 lr: 1.0e-03 33.2%┣┫ 831/2.5k [15:21<30:51, 1s/it]\n",
      "Loss train: 9.03e-02 val: 1.27e-01 grad: 9.35e+00 lr: 1.0e-03 33.3%┣┫ 832/2.5k [15:22<30:50, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.34e-01 grad: 1.77e+01 lr: 1.0e-03 33.3%┣┫ 833/2.5k [15:23<30:49, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.27e-01 grad: 1.27e+01 lr: 1.0e-03 33.4%┣┫ 834/2.5k [15:24<30:48, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.28e-01 grad: 1.16e+01 lr: 1.0e-03 33.4%┣┫ 835/2.5k [15:25<30:47, 1s/it]\n",
      "Loss train: 9.61e-02 val: 1.34e-01 grad: 1.22e+01 lr: 1.0e-03 33.4%┣┫ 836/2.5k [15:26<30:45, 1s/it]\n",
      "Loss train: 9.48e-02 val: 1.35e-01 grad: 1.25e+01 lr: 1.0e-03 33.5%┣┫ 837/2.5k [15:27<30:44, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.30e-01 grad: 1.07e+01 lr: 1.0e-03 33.5%┣┫ 838/2.5k [15:28<30:42, 1s/it]\n",
      "Loss train: 9.28e-02 val: 1.22e-01 grad: 1.26e+01 lr: 1.0e-03 33.6%┣┫ 839/2.5k [15:29<30:41, 1s/it]\n",
      "Loss train: 9.12e-02 val: 1.27e-01 grad: 1.25e+01 lr: 1.0e-03 33.6%┣┫ 840/2.5k [15:30<30:40, 1s/it]\n",
      "Loss train: 9.61e-02 val: 1.37e-01 grad: 1.01e+01 lr: 1.0e-03 33.6%┣┫ 841/2.5k [15:31<30:38, 1s/it]\n",
      "Loss train: 9.02e-02 val: 1.29e-01 grad: 9.06e+00 lr: 1.0e-03 33.7%┣┫ 842/2.5k [15:32<30:37, 1s/it]\n",
      "Loss train: 8.92e-02 val: 1.29e-01 grad: 1.28e+01 lr: 1.0e-03 33.7%┣┫ 843/2.5k [15:33<30:36, 1s/it]\n",
      "Loss train: 9.16e-02 val: 1.22e-01 grad: 1.14e+01 lr: 1.0e-03 33.8%┣┫ 844/2.5k [15:34<30:35, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.26e-01 grad: 1.25e+01 lr: 1.0e-03 33.8%┣┫ 845/2.5k [15:35<30:33, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.42e-01 grad: 1.31e+01 lr: 1.0e-03 33.8%┣┫ 846/2.5k [15:36<30:32, 1s/it]\n",
      "Loss train: 9.25e-02 val: 1.35e-01 grad: 8.50e+00 lr: 1.0e-03 33.9%┣┫ 847/2.5k [15:37<30:31, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.26e-01 grad: 1.24e+01 lr: 1.0e-03 33.9%┣┫ 848/2.5k [15:38<30:30, 1s/it]\n",
      "Loss train: 9.22e-02 val: 1.22e-01 grad: 1.13e+01 lr: 1.0e-03 34.0%┣┫ 849/2.5k [15:39<30:28, 1s/it]\n",
      "Loss train: 8.99e-02 val: 1.31e-01 grad: 1.12e+01 lr: 1.0e-03 34.0%┣┫ 850/2.5k [15:40<30:27, 1s/it]\n",
      "Loss train: 9.24e-02 val: 1.35e-01 grad: 9.52e+00 lr: 1.0e-03 34.0%┣┫ 851/2.5k [15:41<30:26, 1s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.3   0.0   0.0   0.0   0.0  0.0   140.17  0.06  13.33  -0.3    0.02  -0.0    0.27  0.01  0.0\n",
      "  0.1   0.0   0.0   0.31  0.0  0.02  123.3   0.19  16.16  -0.1    0.2    0.03  -0.31  0.18  0.0\n",
      "  0.57  0.11  0.04  0.0   0.0  0.04  129.74  0.35  19.98  -0.57  -0.11  -0.04   0.48  0.24  0.0\n",
      " -0.0   0.42  0.27  0.0   0.0  0.04  114.42  0.21  16.2    0.0   -0.42  -0.27   0.46  0.23  0.0\n",
      " -0.0   0.0   0.0   0.36  0.0  0.14  157.5   0.02  13.27   0.0   -0.0    0.28  -0.36  0.08  0.0\n",
      "  0.05  0.06  0.03  0.0   0.0  0.17  150.87  0.05  11.52  -0.05  -0.06  -0.03   0.13  0.0   0.0\n",
      " -0.0   0.0   0.0   0.65  0.0  0.25   96.18  0.24  16.91   0.0    0.06   0.32  -0.65  0.26  0.0\n",
      "  0.1   0.0   0.0   0.57  0.0  0.02  104.83  0.28  16.85  -0.1    0.18   0.1   -0.57  0.39  0.0\n",
      "\n",
      "Min Loss train: 8.86e-02 val: 8.77e-02\n",
      " update plot 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss train: 1.22e-01 val: 8.77e-02 grad: 1.03e+01 lr: 1.0e-03 34.1%┣┫ 852/2.5k [15:42<30:25, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.33e-01 grad: 1.82e+01 lr: 1.0e-03 34.1%┣┫ 853/2.5k [15:43<30:24, 1s/it]\n",
      "Loss train: 1.12e-01 val: 1.27e-01 grad: 1.11e+01 lr: 1.0e-03 34.2%┣┫ 854/2.5k [15:44<30:22, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.17e-01 grad: 1.30e+01 lr: 1.0e-03 34.2%┣┫ 855/2.5k [15:45<30:21, 1s/it]\n",
      "Loss train: 9.65e-02 val: 1.24e-01 grad: 1.26e+01 lr: 1.0e-03 34.2%┣┫ 856/2.5k [15:46<30:20, 1s/it]\n",
      "Loss train: 9.15e-02 val: 1.35e-01 grad: 1.09e+01 lr: 1.0e-03 34.3%┣┫ 857/2.5k [15:47<30:19, 1s/it]\n",
      "Loss train: 9.26e-02 val: 1.37e-01 grad: 8.92e+00 lr: 1.0e-03 34.3%┣┫ 858/2.5k [15:48<30:17, 1s/it]\n",
      "Loss train: 9.06e-02 val: 1.33e-01 grad: 8.82e+00 lr: 1.0e-03 34.4%┣┫ 859/2.5k [15:50<30:16, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.16e-01 grad: 1.27e+01 lr: 1.0e-03 34.4%┣┫ 860/2.5k [15:51<30:15, 1s/it]\n",
      "Loss train: 8.92e-02 val: 1.34e-01 grad: 1.22e+01 lr: 1.0e-03 34.4%┣┫ 861/2.5k [15:52<30:14, 1s/it]\n",
      "Loss train: 9.35e-02 val: 1.42e-01 grad: 9.48e+00 lr: 1.0e-03 34.5%┣┫ 862/2.5k [15:53<30:12, 1s/it]\n",
      "Loss train: 8.97e-02 val: 1.35e-01 grad: 9.53e+00 lr: 1.0e-03 34.5%┣┫ 863/2.5k [15:54<30:11, 1s/it]\n",
      "Loss train: 9.04e-02 val: 1.39e-01 grad: 1.36e+01 lr: 1.0e-03 34.6%┣┫ 864/2.5k [15:55<30:10, 1s/it]\n",
      "Loss train: 9.32e-02 val: 1.26e-01 grad: 1.20e+01 lr: 1.0e-03 34.6%┣┫ 865/2.5k [15:56<30:09, 1s/it]\n",
      "Loss train: 9.14e-02 val: 1.27e-01 grad: 1.06e+01 lr: 1.0e-03 34.6%┣┫ 866/2.5k [15:57<30:07, 1s/it]\n",
      "Loss train: 8.95e-02 val: 1.38e-01 grad: 1.17e+01 lr: 1.0e-03 34.7%┣┫ 867/2.5k [15:58<30:06, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.13e-01 grad: 9.15e+00 lr: 1.0e-03 34.7%┣┫ 868/2.5k [15:59<30:05, 1s/it]\n",
      "Loss train: 1.10e-01 val: 1.41e-01 grad: 1.89e+01 lr: 1.0e-03 34.8%┣┫ 869/2.5k [16:00<30:04, 1s/it]\n",
      "Loss train: 1.18e-01 val: 1.22e-01 grad: 1.14e+01 lr: 1.0e-03 34.8%┣┫ 870/2.5k [16:01<30:03, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.21e-01 grad: 1.23e+01 lr: 1.0e-03 34.8%┣┫ 871/2.5k [16:02<30:02, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.26e-01 grad: 1.16e+01 lr: 1.0e-03 34.9%┣┫ 872/2.5k [16:03<30:00, 1s/it]\n",
      "Loss train: 9.81e-02 val: 1.34e-01 grad: 1.11e+01 lr: 1.0e-03 34.9%┣┫ 873/2.5k [16:04<29:59, 1s/it]\n",
      "Loss train: 9.33e-02 val: 1.36e-01 grad: 1.26e+01 lr: 1.0e-03 35.0%┣┫ 874/2.5k [16:05<29:58, 1s/it]\n",
      "Loss train: 9.22e-02 val: 1.37e-01 grad: 1.10e+01 lr: 1.0e-03 35.0%┣┫ 875/2.5k [16:06<29:57, 1s/it]\n",
      "Loss train: 9.19e-02 val: 1.36e-01 grad: 1.08e+01 lr: 1.0e-03 35.0%┣┫ 876/2.5k [16:07<29:56, 1s/it]\n",
      "Loss train: 9.03e-02 val: 1.36e-01 grad: 1.19e+01 lr: 1.0e-03 35.1%┣┫ 877/2.5k [16:08<29:54, 1s/it]\n",
      "Loss train: 8.96e-02 val: 1.34e-01 grad: 1.02e+01 lr: 1.0e-03 35.1%┣┫ 878/2.5k [16:09<29:53, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.35e-01 grad: 1.24e+01 lr: 1.0e-03 35.2%┣┫ 879/2.5k [16:10<29:52, 1s/it]\n",
      "Loss train: 8.85e-02 val: 1.31e-01 grad: 1.19e+01 lr: 1.0e-03 35.2%┣┫ 880/2.5k [16:11<29:50, 1s/it]\n",
      "Loss train: 8.81e-02 val: 1.28e-01 grad: 1.25e+01 lr: 1.0e-03 35.2%┣┫ 881/2.5k [16:12<29:49, 1s/it]\n",
      "Loss train: 9.04e-02 val: 1.25e-01 grad: 1.84e+01 lr: 1.0e-03 35.3%┣┫ 882/2.5k [16:13<29:48, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.41e-01 grad: 1.06e+01 lr: 1.0e-03 35.3%┣┫ 883/2.5k [16:14<29:46, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.29e-01 grad: 1.04e+01 lr: 1.0e-03 35.4%┣┫ 884/2.5k [16:15<29:45, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.22e-01 grad: 1.11e+01 lr: 1.0e-03 35.4%┣┫ 885/2.5k [16:16<29:44, 1s/it]\n",
      "Loss train: 9.86e-02 val: 1.25e-01 grad: 1.12e+01 lr: 1.0e-03 35.4%┣┫ 886/2.5k [16:17<29:43, 1s/it]\n",
      "Loss train: 9.37e-02 val: 1.33e-01 grad: 1.12e+01 lr: 1.0e-03 35.5%┣┫ 887/2.5k [16:19<29:41, 1s/it]\n",
      "Loss train: 9.21e-02 val: 1.37e-01 grad: 1.00e+01 lr: 1.0e-03 35.5%┣┫ 888/2.5k [16:20<29:40, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.31e-01 grad: 1.01e+01 lr: 1.0e-03 35.6%┣┫ 889/2.5k [16:21<29:39, 1s/it]\n",
      "Loss train: 9.59e-02 val: 1.20e-01 grad: 1.23e+01 lr: 1.0e-03 35.6%┣┫ 890/2.5k [16:22<29:38, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.28e-01 grad: 1.12e+01 lr: 1.0e-03 35.6%┣┫ 891/2.5k [16:23<29:36, 1s/it]\n",
      "Loss train: 8.92e-02 val: 1.32e-01 grad: 1.23e+01 lr: 1.0e-03 35.7%┣┫ 892/2.5k [16:24<29:35, 1s/it]\n",
      "Loss train: 9.05e-02 val: 1.37e-01 grad: 1.10e+01 lr: 1.0e-03 35.7%┣┫ 893/2.5k [16:25<29:34, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.32e-01 grad: 1.02e+01 lr: 1.0e-03 35.8%┣┫ 894/2.5k [16:26<29:33, 1s/it]\n",
      "Loss train: 9.18e-02 val: 1.30e-01 grad: 1.53e+01 lr: 1.0e-03 35.8%┣┫ 895/2.5k [16:27<29:32, 1s/it]\n",
      "Loss train: 9.73e-02 val: 1.32e-01 grad: 1.22e+01 lr: 1.0e-03 35.8%┣┫ 896/2.5k [16:28<29:30, 1s/it]\n",
      "Loss train: 9.21e-02 val: 1.31e-01 grad: 1.08e+01 lr: 1.0e-03 35.9%┣┫ 897/2.5k [16:29<29:29, 1s/it]\n",
      "Loss train: 9.24e-02 val: 1.25e-01 grad: 1.16e+01 lr: 1.0e-03 35.9%┣┫ 898/2.5k [16:30<29:28, 1s/it]\n",
      "Loss train: 8.99e-02 val: 1.31e-01 grad: 1.25e+01 lr: 1.0e-03 36.0%┣┫ 899/2.5k [16:31<29:27, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.22e-01 grad: 1.24e+01 lr: 1.0e-03 36.0%┣┫ 900/2.5k [16:32<29:25, 1s/it]\n",
      "Loss train: 8.95e-02 val: 1.31e-01 grad: 1.04e+01 lr: 1.0e-03 36.0%┣┫ 901/2.5k [16:33<29:24, 1s/it]\n",
      "Loss train: 8.84e-02 val: 1.25e-01 grad: 1.18e+01 lr: 1.0e-03 36.1%┣┫ 902/2.5k [16:34<29:23, 1s/it]\n",
      "Loss train: 8.84e-02 val: 1.31e-01 grad: 1.12e+01 lr: 1.0e-03 36.1%┣┫ 903/2.5k [16:35<29:22, 1s/it]\n",
      "Loss train: 8.75e-02 val: 1.24e-01 grad: 1.24e+01 lr: 1.0e-03 36.2%┣┫ 904/2.5k [16:36<29:20, 1s/it]\n",
      "Loss train: 9.00e-02 val: 1.33e-01 grad: 1.04e+01 lr: 1.0e-03 36.2%┣┫ 905/2.5k [16:37<29:19, 1s/it]\n",
      "Loss train: 9.01e-02 val: 1.23e-01 grad: 1.87e+01 lr: 1.0e-03 36.2%┣┫ 906/2.5k [16:38<29:19, 1s/it]\n",
      "Loss train: 1.39e-01 val: 1.48e-01 grad: 1.15e+01 lr: 1.0e-03 36.3%┣┫ 907/2.5k [16:39<29:17, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.28e-01 grad: 1.29e+01 lr: 1.0e-03 36.3%┣┫ 908/2.5k [16:40<29:16, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.24e-01 grad: 1.10e+01 lr: 1.0e-03 36.4%┣┫ 909/2.5k [16:41<29:15, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.28e-01 grad: 1.20e+01 lr: 1.0e-03 36.4%┣┫ 910/2.5k [16:43<29:14, 1s/it]\n",
      "Loss train: 9.91e-02 val: 1.41e-01 grad: 1.20e+01 lr: 1.0e-03 36.4%┣┫ 911/2.5k [16:44<29:12, 1s/it]\n",
      "Loss train: 9.23e-02 val: 1.33e-01 grad: 7.82e+00 lr: 1.0e-03 36.5%┣┫ 912/2.5k [16:45<29:11, 1s/it]\n",
      "Loss train: 8.96e-02 val: 1.25e-01 grad: 1.23e+01 lr: 1.0e-03 36.5%┣┫ 913/2.5k [16:46<29:10, 1s/it]\n",
      "Loss train: 8.79e-02 val: 1.29e-01 grad: 1.21e+01 lr: 1.0e-03 36.6%┣┫ 914/2.5k [16:47<29:08, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.30e-01 grad: 1.20e+01 lr: 1.0e-03 36.6%┣┫ 915/2.5k [16:48<29:07, 1s/it]\n",
      "Loss train: 8.83e-02 val: 1.20e-01 grad: 1.19e+01 lr: 1.0e-03 36.6%┣┫ 916/2.5k [16:49<29:06, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.15e-01 grad: 1.11e+01 lr: 1.0e-03 36.7%┣┫ 917/2.5k [16:50<29:05, 1s/it]\n",
      "Loss train: 9.07e-02 val: 1.29e-01 grad: 2.00e+01 lr: 1.0e-03 36.7%┣┫ 918/2.5k [16:51<29:04, 1s/it]\n",
      "Loss train: 1.55e-01 val: 1.66e-01 grad: 9.68e+00 lr: 1.0e-03 36.8%┣┫ 919/2.5k [16:52<29:03, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.20e-01 grad: 1.20e+01 lr: 1.0e-03 36.8%┣┫ 920/2.5k [16:53<29:02, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.17e-01 grad: 1.11e+01 lr: 1.0e-03 36.8%┣┫ 921/2.5k [16:54<29:01, 1s/it]\n",
      "Loss train: 9.61e-02 val: 1.20e-01 grad: 1.15e+01 lr: 1.0e-03 36.9%┣┫ 922/2.5k [16:55<29:00, 1s/it]\n",
      "Loss train: 9.17e-02 val: 1.31e-01 grad: 1.23e+01 lr: 1.0e-03 36.9%┣┫ 923/2.5k [16:56<28:58, 1s/it]\n",
      "Loss train: 9.45e-02 val: 1.36e-01 grad: 9.85e+00 lr: 1.0e-03 37.0%┣┫ 924/2.5k [16:58<28:58, 1s/it]\n",
      "Loss train: 8.92e-02 val: 1.24e-01 grad: 1.15e+01 lr: 1.0e-03 37.0%┣┫ 925/2.5k [16:59<28:56, 1s/it]\n",
      "Loss train: 8.81e-02 val: 1.23e-01 grad: 1.19e+01 lr: 1.0e-03 37.0%┣┫ 926/2.5k [17:00<28:55, 1s/it]\n",
      "Loss train: 8.76e-02 val: 1.24e-01 grad: 1.09e+01 lr: 1.0e-03 37.1%┣┫ 927/2.5k [17:01<28:54, 1s/it]\n",
      "Loss train: 9.19e-02 val: 1.34e-01 grad: 9.69e+00 lr: 1.0e-03 37.1%┣┫ 928/2.5k [17:02<28:53, 1s/it]\n",
      "Loss train: 9.01e-02 val: 1.10e-01 grad: 8.79e+00 lr: 1.0e-03 37.2%┣┫ 929/2.5k [17:03<28:51, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.33e-01 grad: 1.98e+01 lr: 1.0e-03 37.2%┣┫ 930/2.5k [17:04<28:50, 1s/it]\n",
      "Loss train: 1.38e-01 val: 1.17e-01 grad: 9.18e+00 lr: 1.0e-03 37.2%┣┫ 931/2.5k [17:05<28:49, 1s/it]\n",
      "Loss train: 1.11e-01 val: 1.20e-01 grad: 1.46e+01 lr: 1.0e-03 37.3%┣┫ 932/2.5k [17:06<28:48, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.27e-01 grad: 1.21e+01 lr: 1.0e-03 37.3%┣┫ 933/2.5k [17:07<28:47, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.29e-01 grad: 1.12e+01 lr: 1.0e-03 37.4%┣┫ 934/2.5k [17:08<28:46, 1s/it]\n",
      "Loss train: 9.95e-02 val: 1.36e-01 grad: 9.96e+00 lr: 1.0e-03 37.4%┣┫ 935/2.5k [17:09<28:44, 1s/it]\n",
      "Loss train: 9.08e-02 val: 1.27e-01 grad: 1.03e+01 lr: 1.0e-03 37.4%┣┫ 936/2.5k [17:10<28:43, 1s/it]\n",
      "Loss train: 9.00e-02 val: 1.28e-01 grad: 1.22e+01 lr: 1.0e-03 37.5%┣┫ 937/2.5k [17:11<28:42, 1s/it]\n",
      "Loss train: 8.84e-02 val: 1.39e-01 grad: 1.21e+01 lr: 1.0e-03 37.5%┣┫ 938/2.5k [17:12<28:41, 1s/it]\n",
      "Loss train: 8.79e-02 val: 1.31e-01 grad: 1.21e+01 lr: 1.0e-03 37.6%┣┫ 939/2.5k [17:13<28:40, 1s/it]\n",
      "Loss train: 8.73e-02 val: 1.25e-01 grad: 1.23e+01 lr: 1.0e-03 37.6%┣┫ 940/2.5k [17:15<28:39, 1s/it]\n",
      "Loss train: 1.06e-01 val: 1.44e-01 grad: 1.33e+01 lr: 1.0e-03 37.6%┣┫ 941/2.5k [17:16<28:38, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.24e-01 grad: 1.07e+01 lr: 1.0e-03 37.7%┣┫ 942/2.5k [17:17<28:36, 1s/it]\n",
      "Loss train: 9.64e-02 val: 1.26e-01 grad: 1.08e+01 lr: 1.0e-03 37.7%┣┫ 943/2.5k [17:18<28:35, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.34e-01 grad: 1.08e+01 lr: 1.0e-03 37.8%┣┫ 944/2.5k [17:19<28:34, 1s/it]\n",
      "Loss train: 8.93e-02 val: 1.33e-01 grad: 1.20e+01 lr: 1.0e-03 37.8%┣┫ 945/2.5k [17:20<28:33, 1s/it]\n",
      "Loss train: 9.11e-02 val: 1.38e-01 grad: 1.16e+01 lr: 1.0e-03 37.8%┣┫ 946/2.5k [17:21<28:31, 1s/it]\n",
      "Loss train: 8.85e-02 val: 1.34e-01 grad: 1.11e+01 lr: 1.0e-03 37.9%┣┫ 947/2.5k [17:22<28:30, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.36e-01 grad: 9.02e+00 lr: 1.0e-03 37.9%┣┫ 948/2.5k [17:23<28:29, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.21e-01 grad: 1.22e+01 lr: 1.0e-03 38.0%┣┫ 949/2.5k [17:24<28:28, 1s/it]\n",
      "Loss train: 8.82e-02 val: 1.33e-01 grad: 1.21e+01 lr: 1.0e-03 38.0%┣┫ 950/2.5k [17:25<28:26, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.17e-01 grad: 1.09e+01 lr: 1.0e-03 38.0%┣┫ 951/2.5k [17:26<28:26, 1s/it]\n",
      "Loss train: 8.74e-02 val: 1.26e-01 grad: 1.93e+01 lr: 1.0e-03 38.1%┣┫ 952/2.5k [17:27<28:24, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.34e-01 grad: 9.20e+00 lr: 1.0e-03 38.1%┣┫ 953/2.5k [17:28<28:23, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.16e-01 grad: 1.24e+01 lr: 1.0e-03 38.2%┣┫ 954/2.5k [17:29<28:22, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.23e-01 grad: 1.24e+01 lr: 1.0e-03 38.2%┣┫ 955/2.5k [17:30<28:21, 1s/it]\n",
      "Loss train: 9.71e-02 val: 1.24e-01 grad: 1.16e+01 lr: 1.0e-03 38.2%┣┫ 956/2.5k [17:31<28:20, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.46e-01 grad: 1.01e+01 lr: 1.0e-03 38.3%┣┫ 957/2.5k [17:33<28:19, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.39e-01 grad: 7.51e+00 lr: 1.0e-03 38.3%┣┫ 958/2.5k [17:33<28:17, 1s/it]\n",
      "Loss train: 9.26e-02 val: 1.23e-01 grad: 9.57e+00 lr: 1.0e-03 38.4%┣┫ 959/2.5k [17:35<28:16, 1s/it]\n",
      "Loss train: 9.15e-02 val: 1.23e-01 grad: 1.14e+01 lr: 1.0e-03 38.4%┣┫ 960/2.5k [17:35<28:15, 1s/it]\n",
      "Loss train: 8.87e-02 val: 1.32e-01 grad: 1.06e+01 lr: 1.0e-03 38.4%┣┫ 961/2.5k [17:36<28:14, 1s/it]\n",
      "Loss train: 9.45e-02 val: 1.43e-01 grad: 1.04e+01 lr: 1.0e-03 38.5%┣┫ 962/2.5k [17:37<28:12, 1s/it]\n",
      "Loss train: 8.78e-02 val: 1.32e-01 grad: 7.90e+00 lr: 1.0e-03 38.5%┣┫ 963/2.5k [17:39<28:11, 1s/it]\n",
      "Loss train: 8.67e-02 val: 1.29e-01 grad: 1.20e+01 lr: 1.0e-03 38.6%┣┫ 964/2.5k [17:40<28:10, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.17e-01 grad: 1.32e+01 lr: 1.0e-03 38.6%┣┫ 965/2.5k [17:41<28:09, 1s/it]\n",
      "Loss train: 9.24e-02 val: 1.22e-01 grad: 1.03e+01 lr: 1.0e-03 38.6%┣┫ 966/2.5k [17:42<28:08, 1s/it]\n",
      "Loss train: 9.19e-02 val: 1.34e-01 grad: 1.13e+01 lr: 1.0e-03 38.7%┣┫ 967/2.5k [17:43<28:07, 1s/it]\n",
      "Loss train: 8.75e-02 val: 1.24e-01 grad: 1.22e+01 lr: 1.0e-03 38.7%┣┫ 968/2.5k [17:44<28:06, 1s/it]\n",
      "Loss train: 8.85e-02 val: 1.28e-01 grad: 1.20e+01 lr: 1.0e-03 38.8%┣┫ 969/2.5k [17:45<28:04, 1s/it]\n",
      "Loss train: 8.93e-02 val: 1.25e-01 grad: 8.72e+00 lr: 1.0e-03 38.8%┣┫ 970/2.5k [17:46<28:03, 1s/it]\n",
      "Loss train: 9.26e-02 val: 1.27e-01 grad: 1.61e+01 lr: 1.0e-03 38.8%┣┫ 971/2.5k [17:47<28:02, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.31e-01 grad: 1.12e+01 lr: 1.0e-03 38.9%┣┫ 972/2.5k [17:48<28:01, 1s/it]\n",
      "Loss train: 9.57e-02 val: 1.25e-01 grad: 9.22e+00 lr: 1.0e-03 38.9%┣┫ 973/2.5k [17:49<28:00, 1s/it]\n",
      "Loss train: 9.64e-02 val: 1.19e-01 grad: 1.17e+01 lr: 1.0e-03 39.0%┣┫ 974/2.5k [17:50<27:59, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.26e-01 grad: 1.06e+01 lr: 1.0e-03 39.0%┣┫ 975/2.5k [17:51<27:58, 1s/it]\n",
      "Loss train: 9.24e-02 val: 1.32e-01 grad: 1.17e+01 lr: 1.0e-03 39.0%┣┫ 976/2.5k [17:52<27:56, 1s/it]\n",
      "Loss train: 9.07e-02 val: 1.31e-01 grad: 8.65e+00 lr: 1.0e-03 39.1%┣┫ 977/2.5k [17:53<27:55, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.15e-01 grad: 1.08e+01 lr: 1.0e-03 39.1%┣┫ 978/2.5k [17:54<27:54, 1s/it]\n",
      "Loss train: 8.76e-02 val: 1.25e-01 grad: 1.14e+01 lr: 1.0e-03 39.2%┣┫ 979/2.5k [17:55<27:53, 1s/it]\n",
      "Loss train: 8.81e-02 val: 1.33e-01 grad: 9.71e+00 lr: 1.0e-03 39.2%┣┫ 980/2.5k [17:57<27:51, 1s/it]\n",
      "Loss train: 8.69e-02 val: 1.28e-01 grad: 8.76e+00 lr: 1.0e-03 39.2%┣┫ 981/2.5k [17:58<27:51, 1s/it]\n",
      "Loss train: 8.67e-02 val: 1.25e-01 grad: 1.48e+01 lr: 1.0e-03 39.3%┣┫ 982/2.5k [17:59<27:49, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.36e-01 grad: 1.16e+01 lr: 1.0e-03 39.3%┣┫ 983/2.5k [18:00<27:48, 1s/it]\n",
      "Loss train: 9.40e-02 val: 1.27e-01 grad: 9.96e+00 lr: 1.0e-03 39.4%┣┫ 984/2.5k [18:01<27:47, 1s/it]\n",
      "Loss train: 9.68e-02 val: 1.14e-01 grad: 1.08e+01 lr: 1.0e-03 39.4%┣┫ 985/2.5k [18:02<27:46, 1s/it]\n",
      "Loss train: 8.99e-02 val: 1.34e-01 grad: 1.15e+01 lr: 1.0e-03 39.4%┣┫ 986/2.5k [18:03<27:45, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.32e-01 grad: 8.48e+00 lr: 1.0e-03 39.5%┣┫ 987/2.5k [18:04<27:44, 1s/it]\n",
      "Loss train: 8.90e-02 val: 1.24e-01 grad: 1.16e+01 lr: 1.0e-03 39.5%┣┫ 988/2.5k [18:05<27:42, 1s/it]\n",
      "Loss train: 8.70e-02 val: 1.24e-01 grad: 1.21e+01 lr: 1.0e-03 39.6%┣┫ 989/2.5k [18:06<27:41, 1s/it]\n",
      "Loss train: 8.68e-02 val: 1.27e-01 grad: 1.19e+01 lr: 1.0e-03 39.6%┣┫ 990/2.5k [18:07<27:40, 1s/it]\n",
      "Loss train: 8.65e-02 val: 1.31e-01 grad: 1.11e+01 lr: 1.0e-03 39.6%┣┫ 991/2.5k [18:08<27:39, 1s/it]\n",
      "Loss train: 8.61e-02 val: 1.28e-01 grad: 9.32e+00 lr: 1.0e-03 39.7%┣┫ 992/2.5k [18:09<27:38, 1s/it]\n",
      "Loss train: 8.80e-02 val: 1.22e-01 grad: 2.03e+01 lr: 1.0e-03 39.7%┣┫ 993/2.5k [18:10<27:37, 1s/it]\n",
      "Loss train: 1.58e-01 val: 1.66e-01 grad: 1.19e+01 lr: 1.0e-03 39.8%┣┫ 994/2.5k [18:12<27:35, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.22e-01 grad: 1.34e+01 lr: 1.0e-03 39.8%┣┫ 995/2.5k [18:13<27:34, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.15e-01 grad: 1.05e+01 lr: 1.0e-03 39.8%┣┫ 996/2.5k [18:14<27:33, 1s/it]\n",
      "Loss train: 1.03e-01 val: 1.13e-01 grad: 1.10e+01 lr: 1.0e-03 39.9%┣┫ 997/2.5k [18:15<27:32, 1s/it]\n",
      "Loss train: 9.01e-02 val: 1.28e-01 grad: 1.18e+01 lr: 1.0e-03 39.9%┣┫ 998/2.5k [18:16<27:31, 1s/it]\n",
      "Loss train: 9.91e-02 val: 1.37e-01 grad: 7.66e+00 lr: 1.0e-03 40.0%┣┫ 999/2.5k [18:17<27:30, 1s/it]\n",
      "Loss train: 8.81e-02 val: 1.20e-01 grad: 9.53e+00 lr: 5.0e-04 40.0%┣┫ 1.0k/2.5k [18:18<27:29, 1s/it]\n",
      "Loss train: 9.43e-02 val: 1.13e-01 grad: 1.13e+01 lr: 5.0e-04 40.0%┣┫ 1.0k/2.5k [18:19<27:28, 1s/it]\n",
      "Loss train: 9.33e-02 val: 1.12e-01 grad: 1.12e+01 lr: 5.0e-04 40.1%┣┫ 1.0k/2.5k [18:20<27:27, 1s/it]\n",
      "Loss train: 8.73e-02 val: 1.21e-01 grad: 1.17e+01 lr: 5.0e-04 40.1%┣┫ 1.0k/2.5k [18:21<27:25, 1s/it]\n",
      "Loss train: 8.65e-02 val: 1.24e-01 grad: 1.17e+01 lr: 5.0e-04 40.2%┣┫ 1.0k/2.5k [18:22<27:24, 1s/it]\n",
      "Loss train: 8.60e-02 val: 1.28e-01 grad: 1.13e+01 lr: 5.0e-04 40.2%┣┫ 1.0k/2.5k [18:23<27:23, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.26e-01 grad: 1.07e+01 lr: 5.0e-04 40.2%┣┫ 1.0k/2.5k [18:25<27:22, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.24e-01 grad: 1.10e+01 lr: 5.0e-04 40.3%┣┫ 1.0k/2.5k [18:26<27:21, 1s/it]\n",
      "Loss train: 8.62e-02 val: 1.19e-01 grad: 1.17e+01 lr: 5.0e-04 40.3%┣┫ 1.0k/2.5k [18:27<27:20, 1s/it]\n",
      "Loss train: 8.56e-02 val: 1.21e-01 grad: 1.18e+01 lr: 5.0e-04 40.4%┣┫ 1.0k/2.5k [18:28<27:18, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.18e-01 grad: 1.04e+01 lr: 5.0e-04 40.4%┣┫ 1.0k/2.5k [18:29<27:17, 1s/it]\n",
      "Loss train: 9.08e-02 val: 1.30e-01 grad: 1.57e+01 lr: 5.0e-04 40.4%┣┫ 1.0k/2.5k [18:30<27:16, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.28e-01 grad: 1.11e+01 lr: 5.0e-04 40.5%┣┫ 1.0k/2.5k [18:31<27:15, 1s/it]\n",
      "Loss train: 9.63e-02 val: 1.29e-01 grad: 1.04e+01 lr: 5.0e-04 40.5%┣┫ 1.0k/2.5k [18:32<27:14, 1s/it]\n",
      "Loss train: 9.11e-02 val: 1.23e-01 grad: 1.12e+01 lr: 5.0e-04 40.6%┣┫ 1.0k/2.5k [18:33<27:13, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.18e-01 grad: 1.15e+01 lr: 5.0e-04 40.6%┣┫ 1.0k/2.5k [18:34<27:12, 1s/it]\n",
      "Loss train: 8.83e-02 val: 1.17e-01 grad: 1.10e+01 lr: 5.0e-04 40.6%┣┫ 1.0k/2.5k [18:35<27:10, 1s/it]\n",
      "Loss train: 8.69e-02 val: 1.20e-01 grad: 1.13e+01 lr: 5.0e-04 40.7%┣┫ 1.0k/2.5k [18:36<27:09, 1s/it]\n",
      "Loss train: 8.85e-02 val: 1.24e-01 grad: 1.05e+01 lr: 5.0e-04 40.7%┣┫ 1.0k/2.5k [18:37<27:08, 1s/it]\n",
      "Loss train: 8.76e-02 val: 1.23e-01 grad: 9.01e+00 lr: 5.0e-04 40.8%┣┫ 1.0k/2.5k [18:38<27:07, 1s/it]\n",
      "Loss train: 8.64e-02 val: 1.20e-01 grad: 1.02e+01 lr: 5.0e-04 40.8%┣┫ 1.0k/2.5k [18:39<27:06, 1s/it]\n",
      "Loss train: 8.91e-02 val: 1.13e-01 grad: 1.20e+01 lr: 5.0e-04 40.8%┣┫ 1.0k/2.5k [18:40<27:04, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.18e-01 grad: 1.12e+01 lr: 5.0e-04 40.9%┣┫ 1.0k/2.5k [18:41<27:03, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.22e-01 grad: 1.18e+01 lr: 5.0e-04 40.9%┣┫ 1.0k/2.5k [18:42<27:02, 1s/it]\n",
      "Loss train: 8.54e-02 val: 1.21e-01 grad: 1.10e+01 lr: 5.0e-04 41.0%┣┫ 1.0k/2.5k [18:43<27:01, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.24e-01 grad: 9.65e+00 lr: 5.0e-04 41.0%┣┫ 1.0k/2.5k [18:44<27:00, 1s/it]\n",
      "Loss train: 8.53e-02 val: 1.19e-01 grad: 9.57e+00 lr: 5.0e-04 41.0%┣┫ 1.0k/2.5k [18:46<26:59, 1s/it]\n",
      "Loss train: 8.75e-02 val: 1.24e-01 grad: 1.49e+01 lr: 5.0e-04 41.1%┣┫ 1.0k/2.5k [18:47<26:58, 1s/it]\n",
      "Loss train: 8.97e-02 val: 1.25e-01 grad: 1.14e+01 lr: 5.0e-04 41.1%┣┫ 1.0k/2.5k [18:48<26:57, 1s/it]\n",
      "Loss train: 8.85e-02 val: 1.21e-01 grad: 1.15e+01 lr: 5.0e-04 41.2%┣┫ 1.0k/2.5k [18:49<26:55, 1s/it]\n",
      "Loss train: 8.75e-02 val: 1.19e-01 grad: 1.16e+01 lr: 5.0e-04 41.2%┣┫ 1.0k/2.5k [18:50<26:54, 1s/it]\n",
      "Loss train: 8.64e-02 val: 1.22e-01 grad: 1.18e+01 lr: 5.0e-04 41.2%┣┫ 1.0k/2.5k [18:51<26:53, 1s/it]\n",
      "Loss train: 8.60e-02 val: 1.18e-01 grad: 1.17e+01 lr: 5.0e-04 41.3%┣┫ 1.0k/2.5k [18:52<26:52, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.18e-01 grad: 1.18e+01 lr: 5.0e-04 41.3%┣┫ 1.0k/2.5k [18:53<26:51, 1s/it]\n",
      "Loss train: 8.53e-02 val: 1.17e-01 grad: 1.19e+01 lr: 5.0e-04 41.4%┣┫ 1.0k/2.5k [18:54<26:50, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.16e-01 grad: 1.18e+01 lr: 5.0e-04 41.4%┣┫ 1.0k/2.5k [18:55<26:48, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.22e-01 grad: 1.03e+01 lr: 5.0e-04 41.4%┣┫ 1.0k/2.5k [18:56<26:47, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.18e-01 grad: 9.72e+00 lr: 5.0e-04 41.5%┣┫ 1.0k/2.5k [18:58<26:46, 1s/it]\n",
      "Loss train: 8.54e-02 val: 1.17e-01 grad: 1.23e+01 lr: 5.0e-04 41.5%┣┫ 1.0k/2.5k [18:59<26:45, 1s/it]\n",
      "Loss train: 8.70e-02 val: 1.20e-01 grad: 1.16e+01 lr: 5.0e-04 41.6%┣┫ 1.0k/2.5k [19:00<26:44, 1s/it]\n",
      "Loss train: 8.66e-02 val: 1.19e-01 grad: 1.05e+01 lr: 5.0e-04 41.6%┣┫ 1.0k/2.5k [19:01<26:43, 1s/it]\n",
      "Loss train: 8.70e-02 val: 1.15e-01 grad: 1.13e+01 lr: 5.0e-04 41.6%┣┫ 1.0k/2.5k [19:02<26:42, 1s/it]\n",
      "Loss train: 8.64e-02 val: 1.14e-01 grad: 1.05e+01 lr: 5.0e-04 41.7%┣┫ 1.0k/2.5k [19:03<26:41, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.19e-01 grad: 1.18e+01 lr: 5.0e-04 41.7%┣┫ 1.0k/2.5k [19:04<26:40, 1s/it]\n",
      "Loss train: 8.99e-02 val: 1.25e-01 grad: 9.44e+00 lr: 5.0e-04 41.8%┣┫ 1.0k/2.5k [19:05<26:39, 1s/it]\n",
      "Loss train: 8.48e-02 val: 1.17e-01 grad: 1.03e+01 lr: 5.0e-04 41.8%┣┫ 1.0k/2.5k [19:06<26:38, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.16e-01 grad: 1.18e+01 lr: 5.0e-04 41.8%┣┫ 1.0k/2.5k [19:08<26:37, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.18e-01 grad: 1.17e+01 lr: 5.0e-04 41.9%┣┫ 1.0k/2.5k [19:09<26:36, 1s/it]\n",
      "Loss train: 8.52e-02 val: 1.20e-01 grad: 1.17e+01 lr: 5.0e-04 41.9%┣┫ 1.0k/2.5k [19:10<26:35, 1s/it]\n",
      "Loss train: 8.53e-02 val: 1.21e-01 grad: 9.58e+00 lr: 5.0e-04 42.0%┣┫ 1.0k/2.5k [19:11<26:34, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.11e-01 grad: 1.17e+01 lr: 5.0e-04 42.0%┣┫ 1.1k/2.5k [19:12<26:32, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.17e-01 grad: 1.80e+01 lr: 5.0e-04 42.0%┣┫ 1.1k/2.5k [19:13<26:31, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.29e-01 grad: 1.02e+01 lr: 5.0e-04 42.1%┣┫ 1.1k/2.5k [19:14<26:30, 1s/it]\n",
      "Loss train: 9.79e-02 val: 1.21e-01 grad: 9.51e+00 lr: 5.0e-04 42.1%┣┫ 1.1k/2.5k [19:15<26:29, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.14e-01 grad: 1.04e+01 lr: 5.0e-04 42.2%┣┫ 1.1k/2.5k [19:16<26:28, 1s/it]\n",
      "Loss train: 9.28e-02 val: 1.14e-01 grad: 1.06e+01 lr: 5.0e-04 42.2%┣┫ 1.1k/2.5k [19:18<26:27, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.16e-01 grad: 1.14e+01 lr: 5.0e-04 42.2%┣┫ 1.1k/2.5k [19:19<26:26, 1s/it]\n",
      "Loss train: 9.15e-02 val: 1.29e-01 grad: 1.14e+01 lr: 5.0e-04 42.3%┣┫ 1.1k/2.5k [19:20<26:25, 1s/it]\n",
      "Loss train: 8.73e-02 val: 1.24e-01 grad: 9.22e+00 lr: 5.0e-04 42.3%┣┫ 1.1k/2.5k [19:21<26:23, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.22e-01 grad: 9.78e+00 lr: 5.0e-04 42.4%┣┫ 1.1k/2.5k [19:22<26:22, 1s/it]\n",
      "Loss train: 8.66e-02 val: 1.18e-01 grad: 1.16e+01 lr: 5.0e-04 42.4%┣┫ 1.1k/2.5k [19:23<26:21, 1s/it]\n",
      "Loss train: 8.82e-02 val: 1.13e-01 grad: 1.17e+01 lr: 5.0e-04 42.4%┣┫ 1.1k/2.5k [19:24<26:20, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.18e-01 grad: 1.07e+01 lr: 5.0e-04 42.5%┣┫ 1.1k/2.5k [19:25<26:18, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.24e-01 grad: 1.14e+01 lr: 5.0e-04 42.5%┣┫ 1.1k/2.5k [19:26<26:17, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.24e-01 grad: 9.19e+00 lr: 5.0e-04 42.6%┣┫ 1.1k/2.5k [19:27<26:16, 1s/it]\n",
      "Loss train: 8.53e-02 val: 1.18e-01 grad: 9.81e+00 lr: 5.0e-04 42.6%┣┫ 1.1k/2.5k [19:28<26:15, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.19e-01 grad: 1.17e+01 lr: 5.0e-04 42.6%┣┫ 1.1k/2.5k [19:29<26:14, 1s/it]\n",
      "Loss train: 8.54e-02 val: 1.25e-01 grad: 1.16e+01 lr: 5.0e-04 42.7%┣┫ 1.1k/2.5k [19:30<26:13, 1s/it]\n",
      "Loss train: 8.45e-02 val: 1.15e-01 grad: 9.20e+00 lr: 5.0e-04 42.7%┣┫ 1.1k/2.5k [19:31<26:12, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.22e-01 grad: 1.36e+01 lr: 5.0e-04 42.8%┣┫ 1.1k/2.5k [19:32<26:11, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.18e-01 grad: 1.07e+01 lr: 5.0e-04 42.8%┣┫ 1.1k/2.5k [19:33<26:10, 1s/it]\n",
      "Loss train: 8.62e-02 val: 1.22e-01 grad: 1.16e+01 lr: 5.0e-04 42.8%┣┫ 1.1k/2.5k [19:34<26:08, 1s/it]\n",
      "Loss train: 8.65e-02 val: 1.25e-01 grad: 1.12e+01 lr: 5.0e-04 42.9%┣┫ 1.1k/2.5k [19:36<26:07, 1s/it]\n",
      "Loss train: 8.48e-02 val: 1.20e-01 grad: 9.16e+00 lr: 5.0e-04 42.9%┣┫ 1.1k/2.5k [19:37<26:06, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.14e-01 grad: 1.17e+01 lr: 5.0e-04 43.0%┣┫ 1.1k/2.5k [19:38<26:05, 1s/it]\n",
      "Loss train: 8.52e-02 val: 1.15e-01 grad: 1.15e+01 lr: 5.0e-04 43.0%┣┫ 1.1k/2.5k [19:39<26:04, 1s/it]\n",
      "Loss train: 8.42e-02 val: 1.19e-01 grad: 1.18e+01 lr: 5.0e-04 43.0%┣┫ 1.1k/2.5k [19:40<26:03, 1s/it]\n",
      "Loss train: 8.62e-02 val: 1.23e-01 grad: 1.35e+01 lr: 5.0e-04 43.1%┣┫ 1.1k/2.5k [19:41<26:02, 1s/it]\n",
      "Loss train: 8.73e-02 val: 1.23e-01 grad: 9.26e+00 lr: 5.0e-04 43.1%┣┫ 1.1k/2.5k [19:42<26:01, 1s/it]\n",
      "Loss train: 8.69e-02 val: 1.23e-01 grad: 9.22e+00 lr: 5.0e-04 43.2%┣┫ 1.1k/2.5k [19:43<25:59, 1s/it]\n",
      "Loss train: 8.75e-02 val: 1.15e-01 grad: 9.27e+00 lr: 5.0e-04 43.2%┣┫ 1.1k/2.5k [19:44<25:58, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.17e-01 grad: 1.09e+01 lr: 5.0e-04 43.2%┣┫ 1.1k/2.5k [19:45<25:57, 1s/it]\n",
      "Loss train: 8.64e-02 val: 1.14e-01 grad: 1.15e+01 lr: 5.0e-04 43.3%┣┫ 1.1k/2.5k [19:46<25:56, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.15e-01 grad: 1.04e+01 lr: 5.0e-04 43.3%┣┫ 1.1k/2.5k [19:47<25:55, 1s/it]\n",
      "Loss train: 8.48e-02 val: 1.24e-01 grad: 1.06e+01 lr: 5.0e-04 43.4%┣┫ 1.1k/2.5k [19:48<25:54, 1s/it]\n",
      "Loss train: 8.87e-02 val: 1.24e-01 grad: 8.14e+00 lr: 5.0e-04 43.4%┣┫ 1.1k/2.5k [19:49<25:53, 1s/it]\n",
      "Loss train: 9.83e-02 val: 9.67e-02 grad: 8.06e+00 lr: 5.0e-04 43.4%┣┫ 1.1k/2.5k [19:51<25:52, 1s/it]\n",
      "Loss train: 9.94e-02 val: 1.25e-01 grad: 2.01e+01 lr: 5.0e-04 43.5%┣┫ 1.1k/2.5k [19:52<25:51, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.23e-01 grad: 1.15e+01 lr: 5.0e-04 43.5%┣┫ 1.1k/2.5k [19:53<25:49, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.14e-01 grad: 1.29e+01 lr: 5.0e-04 43.6%┣┫ 1.1k/2.5k [19:54<25:48, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.14e-01 grad: 1.16e+01 lr: 5.0e-04 43.6%┣┫ 1.1k/2.5k [19:55<25:47, 1s/it]\n",
      "Loss train: 9.35e-02 val: 1.18e-01 grad: 1.13e+01 lr: 5.0e-04 43.6%┣┫ 1.1k/2.5k [19:56<25:46, 1s/it]\n",
      "Loss train: 8.74e-02 val: 1.30e-01 grad: 1.14e+01 lr: 5.0e-04 43.7%┣┫ 1.1k/2.5k [19:57<25:45, 1s/it]\n",
      "Loss train: 9.23e-02 val: 1.34e-01 grad: 9.58e+00 lr: 5.0e-04 43.7%┣┫ 1.1k/2.5k [19:58<25:44, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.28e-01 grad: 7.71e+00 lr: 5.0e-04 43.8%┣┫ 1.1k/2.5k [19:59<25:43, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.25e-01 grad: 8.81e+00 lr: 5.0e-04 43.8%┣┫ 1.1k/2.5k [20:00<25:41, 1s/it]\n",
      "Loss train: 8.61e-02 val: 1.21e-01 grad: 1.16e+01 lr: 5.0e-04 43.8%┣┫ 1.1k/2.5k [20:01<25:40, 1s/it]\n",
      "Loss train: 8.60e-02 val: 1.20e-01 grad: 1.17e+01 lr: 5.0e-04 43.9%┣┫ 1.1k/2.5k [20:02<25:39, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.23e-01 grad: 1.19e+01 lr: 5.0e-04 43.9%┣┫ 1.1k/2.5k [20:03<25:38, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.18e-01 grad: 1.17e+01 lr: 5.0e-04 44.0%┣┫ 1.1k/2.5k [20:04<25:36, 1s/it]\n",
      "Loss train: 8.47e-02 val: 1.19e-01 grad: 1.16e+01 lr: 5.0e-04 44.0%┣┫ 1.1k/2.5k [20:05<25:35, 1s/it]\n",
      "Loss train: 8.45e-02 val: 1.18e-01 grad: 1.17e+01 lr: 5.0e-04 44.0%┣┫ 1.1k/2.5k [20:06<25:34, 1s/it]\n",
      "Loss train: 8.37e-02 val: 1.22e-01 grad: 1.14e+01 lr: 5.0e-04 44.1%┣┫ 1.1k/2.5k [20:07<25:33, 1s/it]\n",
      "Loss train: 8.41e-02 val: 1.22e-01 grad: 1.47e+01 lr: 5.0e-04 44.1%┣┫ 1.1k/2.5k [20:08<25:32, 1s/it]\n",
      "Loss train: 9.25e-02 val: 1.26e-01 grad: 9.20e+00 lr: 5.0e-04 44.2%┣┫ 1.1k/2.5k [20:09<25:31, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.20e-01 grad: 1.11e+01 lr: 5.0e-04 44.2%┣┫ 1.1k/2.5k [20:11<25:30, 1s/it]\n",
      "Loss train: 9.03e-02 val: 1.18e-01 grad: 1.06e+01 lr: 5.0e-04 44.2%┣┫ 1.1k/2.5k [20:12<25:28, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.19e-01 grad: 1.08e+01 lr: 5.0e-04 44.3%┣┫ 1.1k/2.5k [20:13<25:27, 1s/it]\n",
      "Loss train: 8.62e-02 val: 1.24e-01 grad: 1.15e+01 lr: 5.0e-04 44.3%┣┫ 1.1k/2.5k [20:14<25:26, 1s/it]\n",
      "Loss train: 8.56e-02 val: 1.23e-01 grad: 9.12e+00 lr: 5.0e-04 44.4%┣┫ 1.1k/2.5k [20:15<25:25, 1s/it]\n",
      "Loss train: 8.52e-02 val: 1.21e-01 grad: 1.13e+01 lr: 5.0e-04 44.4%┣┫ 1.1k/2.5k [20:16<25:24, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.14e-01 grad: 1.18e+01 lr: 5.0e-04 44.4%┣┫ 1.1k/2.5k [20:17<25:22, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.14e-01 grad: 1.08e+01 lr: 5.0e-04 44.5%┣┫ 1.1k/2.5k [20:18<25:21, 1s/it]\n",
      "Loss train: 9.14e-02 val: 1.27e-01 grad: 1.15e+01 lr: 5.0e-04 44.5%┣┫ 1.1k/2.5k [20:19<25:20, 1s/it]\n",
      "Loss train: 8.78e-02 val: 1.24e-01 grad: 8.29e+00 lr: 5.0e-04 44.6%┣┫ 1.1k/2.5k [20:20<25:19, 1s/it]\n",
      "Loss train: 8.40e-02 val: 1.15e-01 grad: 1.11e+01 lr: 5.0e-04 44.6%┣┫ 1.1k/2.5k [20:21<25:18, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.12e-01 grad: 1.07e+01 lr: 5.0e-04 44.6%┣┫ 1.1k/2.5k [20:22<25:17, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.09e-01 grad: 1.25e+01 lr: 5.0e-04 44.7%┣┫ 1.1k/2.5k [20:23<25:16, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.18e-01 grad: 1.16e+01 lr: 5.0e-04 44.7%┣┫ 1.1k/2.5k [20:24<25:15, 1s/it]\n",
      "Loss train: 8.48e-02 val: 1.16e-01 grad: 1.14e+01 lr: 5.0e-04 44.8%┣┫ 1.1k/2.5k [20:25<25:14, 1s/it]\n",
      "Loss train: 8.45e-02 val: 1.18e-01 grad: 1.16e+01 lr: 5.0e-04 44.8%┣┫ 1.1k/2.5k [20:26<25:12, 1s/it]\n",
      "Loss train: 8.66e-02 val: 1.21e-01 grad: 9.60e+00 lr: 5.0e-04 44.8%┣┫ 1.1k/2.5k [20:27<25:11, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.13e-01 grad: 1.03e+01 lr: 5.0e-04 44.9%┣┫ 1.1k/2.5k [20:28<25:10, 1s/it]\n",
      "Loss train: 8.64e-02 val: 1.14e-01 grad: 1.45e+01 lr: 5.0e-04 44.9%┣┫ 1.1k/2.5k [20:30<25:09, 1s/it]\n",
      "Loss train: 9.06e-02 val: 1.23e-01 grad: 1.03e+01 lr: 5.0e-04 45.0%┣┫ 1.1k/2.5k [20:31<25:08, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.17e-01 grad: 1.17e+01 lr: 5.0e-04 45.0%┣┫ 1.1k/2.5k [20:32<25:07, 1s/it]\n",
      "Loss train: 8.76e-02 val: 1.21e-01 grad: 1.09e+01 lr: 5.0e-04 45.0%┣┫ 1.1k/2.5k [20:33<25:06, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.14e-01 grad: 9.71e+00 lr: 5.0e-04 45.1%┣┫ 1.1k/2.5k [20:34<25:04, 1s/it]\n",
      "Loss train: 8.44e-02 val: 1.15e-01 grad: 1.15e+01 lr: 5.0e-04 45.1%┣┫ 1.1k/2.5k [20:35<25:03, 1s/it]\n",
      "Loss train: 8.41e-02 val: 1.14e-01 grad: 1.01e+01 lr: 5.0e-04 45.2%┣┫ 1.1k/2.5k [20:36<25:02, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.14e-01 grad: 1.16e+01 lr: 5.0e-04 45.2%┣┫ 1.1k/2.5k [20:37<25:01, 1s/it]\n",
      "Loss train: 8.44e-02 val: 1.19e-01 grad: 9.56e+00 lr: 5.0e-04 45.2%┣┫ 1.1k/2.5k [20:38<25:00, 1s/it]\n",
      "Loss train: 8.35e-02 val: 1.18e-01 grad: 9.48e+00 lr: 5.0e-04 45.3%┣┫ 1.1k/2.5k [20:39<24:59, 1s/it]\n",
      "Loss train: 8.44e-02 val: 1.13e-01 grad: 1.15e+01 lr: 5.0e-04 45.3%┣┫ 1.1k/2.5k [20:40<24:57, 1s/it]\n",
      "Loss train: 8.41e-02 val: 1.11e-01 grad: 1.06e+01 lr: 5.0e-04 45.4%┣┫ 1.1k/2.5k [20:41<24:56, 1s/it]\n",
      "Loss train: 8.35e-02 val: 1.15e-01 grad: 1.51e+01 lr: 5.0e-04 45.4%┣┫ 1.1k/2.5k [20:42<24:55, 1s/it]\n",
      "Loss train: 9.42e-02 val: 1.26e-01 grad: 9.62e+00 lr: 5.0e-04 45.4%┣┫ 1.1k/2.5k [20:43<24:54, 1s/it]\n",
      "Loss train: 9.05e-02 val: 1.18e-01 grad: 1.06e+01 lr: 5.0e-04 45.5%┣┫ 1.1k/2.5k [20:44<24:53, 1s/it]\n",
      "Loss train: 8.79e-02 val: 1.18e-01 grad: 1.13e+01 lr: 5.0e-04 45.5%┣┫ 1.1k/2.5k [20:45<24:52, 1s/it]\n",
      "Loss train: 8.68e-02 val: 1.18e-01 grad: 1.16e+01 lr: 5.0e-04 45.6%┣┫ 1.1k/2.5k [20:46<24:50, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.12e-01 grad: 1.16e+01 lr: 5.0e-04 45.6%┣┫ 1.1k/2.5k [20:47<24:49, 1s/it]\n",
      "Loss train: 8.44e-02 val: 1.14e-01 grad: 9.95e+00 lr: 5.0e-04 45.6%┣┫ 1.1k/2.5k [20:48<24:48, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.09e-01 grad: 1.00e+01 lr: 5.0e-04 45.7%┣┫ 1.1k/2.5k [20:49<24:47, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.09e-01 grad: 1.05e+01 lr: 5.0e-04 45.7%┣┫ 1.1k/2.5k [20:50<24:46, 1s/it]\n",
      "Loss train: 8.35e-02 val: 1.17e-01 grad: 1.02e+01 lr: 5.0e-04 45.8%┣┫ 1.1k/2.5k [20:51<24:44, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.18e-01 grad: 1.14e+01 lr: 5.0e-04 45.8%┣┫ 1.1k/2.5k [20:52<24:43, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.19e-01 grad: 9.97e+00 lr: 5.0e-04 45.8%┣┫ 1.1k/2.5k [20:54<24:42, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.18e-01 grad: 9.29e+00 lr: 5.0e-04 45.9%┣┫ 1.1k/2.5k [20:55<24:41, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.16e-01 grad: 1.52e+01 lr: 5.0e-04 45.9%┣┫ 1.1k/2.5k [20:56<24:40, 1s/it]\n",
      "Loss train: 9.63e-02 val: 1.21e-01 grad: 1.13e+01 lr: 5.0e-04 46.0%┣┫ 1.1k/2.5k [20:57<24:39, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.14e-01 grad: 1.14e+01 lr: 5.0e-04 46.0%┣┫ 1.1k/2.5k [20:58<24:38, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.14e-01 grad: 1.06e+01 lr: 5.0e-04 46.0%┣┫ 1.2k/2.5k [20:59<24:37, 1s/it]\n",
      "Loss train: 8.88e-02 val: 1.13e-01 grad: 1.07e+01 lr: 5.0e-04 46.1%┣┫ 1.2k/2.5k [21:00<24:36, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.21e-01 grad: 9.58e+00 lr: 5.0e-04 46.1%┣┫ 1.2k/2.5k [21:01<24:34, 1s/it]\n",
      "Loss train: 8.52e-02 val: 1.19e-01 grad: 9.15e+00 lr: 5.0e-04 46.2%┣┫ 1.2k/2.5k [21:02<24:33, 1s/it]\n",
      "Loss train: 8.74e-02 val: 1.10e-01 grad: 1.17e+01 lr: 5.0e-04 46.2%┣┫ 1.2k/2.5k [21:03<24:32, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.18e-01 grad: 1.12e+01 lr: 5.0e-04 46.2%┣┫ 1.2k/2.5k [21:04<24:31, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.16e-01 grad: 9.65e+00 lr: 5.0e-04 46.3%┣┫ 1.2k/2.5k [21:05<24:30, 1s/it]\n",
      "Loss train: 8.42e-02 val: 1.14e-01 grad: 1.14e+01 lr: 5.0e-04 46.3%┣┫ 1.2k/2.5k [21:06<24:28, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.15e-01 grad: 1.15e+01 lr: 5.0e-04 46.4%┣┫ 1.2k/2.5k [21:07<24:27, 1s/it]\n",
      "Loss train: 8.72e-02 val: 1.23e-01 grad: 1.11e+01 lr: 5.0e-04 46.4%┣┫ 1.2k/2.5k [21:08<24:26, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.17e-01 grad: 8.79e+00 lr: 5.0e-04 46.4%┣┫ 1.2k/2.5k [21:09<24:25, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.08e-01 grad: 1.16e+01 lr: 5.0e-04 46.5%┣┫ 1.2k/2.5k [21:10<24:24, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.10e-01 grad: 1.15e+01 lr: 5.0e-04 46.5%┣┫ 1.2k/2.5k [21:11<24:23, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.10e-01 grad: 1.16e+01 lr: 5.0e-04 46.6%┣┫ 1.2k/2.5k [21:13<24:22, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.11e-01 grad: 1.05e+01 lr: 5.0e-04 46.6%┣┫ 1.2k/2.5k [21:14<24:21, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.14e-01 grad: 1.09e+01 lr: 5.0e-04 46.6%┣┫ 1.2k/2.5k [21:15<24:20, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.19e-01 grad: 9.02e+00 lr: 5.0e-04 46.7%┣┫ 1.2k/2.5k [21:16<24:18, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.16e-01 grad: 9.91e+00 lr: 5.0e-04 46.7%┣┫ 1.2k/2.5k [21:17<24:17, 1s/it]\n",
      "Loss train: 8.54e-02 val: 1.07e-01 grad: 1.13e+01 lr: 5.0e-04 46.8%┣┫ 1.2k/2.5k [21:18<24:16, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.17e-01 grad: 1.01e+01 lr: 5.0e-04 46.8%┣┫ 1.2k/2.5k [21:19<24:15, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.16e-01 grad: 1.01e+01 lr: 5.0e-04 46.8%┣┫ 1.2k/2.5k [21:20<24:14, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.12e-01 grad: 1.52e+01 lr: 5.0e-04 46.9%┣┫ 1.2k/2.5k [21:21<24:13, 1s/it]\n",
      "Loss train: 9.77e-02 val: 1.23e-01 grad: 1.11e+01 lr: 5.0e-04 46.9%┣┫ 1.2k/2.5k [21:22<24:12, 1s/it]\n",
      "Loss train: 9.32e-02 val: 1.15e-01 grad: 9.38e+00 lr: 5.0e-04 47.0%┣┫ 1.2k/2.5k [21:23<24:10, 1s/it]\n",
      "Loss train: 9.00e-02 val: 1.09e-01 grad: 1.13e+01 lr: 5.0e-04 47.0%┣┫ 1.2k/2.5k [21:24<24:09, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.13e-01 grad: 1.03e+01 lr: 5.0e-04 47.0%┣┫ 1.2k/2.5k [21:25<24:08, 1s/it]\n",
      "Loss train: 8.45e-02 val: 1.12e-01 grad: 1.06e+01 lr: 5.0e-04 47.1%┣┫ 1.2k/2.5k [21:26<24:07, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.16e-01 grad: 1.14e+01 lr: 5.0e-04 47.1%┣┫ 1.2k/2.5k [21:27<24:06, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.16e-01 grad: 9.39e+00 lr: 5.0e-04 47.2%┣┫ 1.2k/2.5k [21:28<24:05, 1s/it]\n",
      "Loss train: 8.36e-02 val: 1.16e-01 grad: 9.31e+00 lr: 5.0e-04 47.2%┣┫ 1.2k/2.5k [21:29<24:03, 1s/it]\n",
      "Loss train: 8.33e-02 val: 1.18e-01 grad: 9.64e+00 lr: 5.0e-04 47.2%┣┫ 1.2k/2.5k [21:30<24:02, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.16e-01 grad: 1.16e+01 lr: 5.0e-04 47.3%┣┫ 1.2k/2.5k [21:31<24:01, 1s/it]\n",
      "Loss train: 8.28e-02 val: 1.18e-01 grad: 1.15e+01 lr: 5.0e-04 47.3%┣┫ 1.2k/2.5k [21:32<24:00, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.14e-01 grad: 1.17e+01 lr: 5.0e-04 47.4%┣┫ 1.2k/2.5k [21:34<23:59, 1s/it]\n",
      "Loss train: 8.28e-02 val: 1.11e-01 grad: 1.09e+01 lr: 5.0e-04 47.4%┣┫ 1.2k/2.5k [21:35<23:58, 1s/it]\n",
      "Loss train: 8.23e-02 val: 1.17e-01 grad: 1.13e+01 lr: 5.0e-04 47.4%┣┫ 1.2k/2.5k [21:36<23:57, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.17e-01 grad: 8.99e+00 lr: 5.0e-04 47.5%┣┫ 1.2k/2.5k [21:37<23:55, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.18e-01 grad: 1.15e+01 lr: 5.0e-04 47.5%┣┫ 1.2k/2.5k [21:38<23:54, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.20e-01 grad: 1.10e+01 lr: 5.0e-04 47.6%┣┫ 1.2k/2.5k [21:39<23:53, 1s/it]\n",
      "Loss train: 8.36e-02 val: 1.16e-01 grad: 1.01e+01 lr: 5.0e-04 47.6%┣┫ 1.2k/2.5k [21:40<23:52, 1s/it]\n",
      "Loss train: 9.03e-02 val: 1.05e-01 grad: 1.16e+01 lr: 5.0e-04 47.6%┣┫ 1.2k/2.5k [21:41<23:51, 1s/it]\n",
      "Loss train: 8.65e-02 val: 1.08e-01 grad: 1.10e+01 lr: 5.0e-04 47.7%┣┫ 1.2k/2.5k [21:42<23:50, 1s/it]\n",
      "Loss train: 8.24e-02 val: 1.15e-01 grad: 1.14e+01 lr: 5.0e-04 47.7%┣┫ 1.2k/2.5k [21:43<23:49, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.20e-01 grad: 1.10e+01 lr: 5.0e-04 47.8%┣┫ 1.2k/2.5k [21:44<23:48, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.14e-01 grad: 8.89e+00 lr: 5.0e-04 47.8%┣┫ 1.2k/2.5k [21:45<23:47, 1s/it]\n",
      "Loss train: 8.74e-02 val: 1.18e-01 grad: 1.71e+01 lr: 5.0e-04 47.8%┣┫ 1.2k/2.5k [21:46<23:46, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.21e-01 grad: 9.58e+00 lr: 5.0e-04 47.9%┣┫ 1.2k/2.5k [21:47<23:44, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.14e-01 grad: 1.14e+01 lr: 5.0e-04 47.9%┣┫ 1.2k/2.5k [21:48<23:43, 1s/it]\n",
      "Loss train: 9.05e-02 val: 1.14e-01 grad: 1.05e+01 lr: 5.0e-04 48.0%┣┫ 1.2k/2.5k [21:49<23:42, 1s/it]\n",
      "Loss train: 8.75e-02 val: 1.18e-01 grad: 1.07e+01 lr: 5.0e-04 48.0%┣┫ 1.2k/2.5k [21:50<23:41, 1s/it]\n",
      "Loss train: 8.74e-02 val: 1.22e-01 grad: 9.53e+00 lr: 5.0e-04 48.0%┣┫ 1.2k/2.5k [21:51<23:40, 1s/it]\n",
      "Loss train: 8.46e-02 val: 1.14e-01 grad: 1.14e+01 lr: 5.0e-04 48.1%┣┫ 1.2k/2.5k [21:52<23:39, 1s/it]\n",
      "Loss train: 8.46e-02 val: 1.18e-01 grad: 9.60e+00 lr: 5.0e-04 48.1%┣┫ 1.2k/2.5k [21:54<23:38, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.14e-01 grad: 1.12e+01 lr: 5.0e-04 48.2%┣┫ 1.2k/2.5k [21:55<23:37, 1s/it]\n",
      "Loss train: 8.30e-02 val: 1.16e-01 grad: 1.12e+01 lr: 5.0e-04 48.2%┣┫ 1.2k/2.5k [21:56<23:35, 1s/it]\n",
      "Loss train: 8.26e-02 val: 1.17e-01 grad: 1.08e+01 lr: 5.0e-04 48.2%┣┫ 1.2k/2.5k [21:57<23:34, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.19e-01 grad: 9.72e+00 lr: 5.0e-04 48.3%┣┫ 1.2k/2.5k [21:58<23:33, 1s/it]\n",
      "Loss train: 8.23e-02 val: 1.18e-01 grad: 1.12e+01 lr: 5.0e-04 48.3%┣┫ 1.2k/2.5k [21:59<23:32, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.18e-01 grad: 1.13e+01 lr: 5.0e-04 48.4%┣┫ 1.2k/2.5k [22:00<23:31, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.09e-01 grad: 1.03e+01 lr: 5.0e-04 48.4%┣┫ 1.2k/2.5k [22:01<23:30, 1s/it]\n",
      "Loss train: 8.14e-02 val: 1.10e-01 grad: 1.13e+01 lr: 5.0e-04 48.4%┣┫ 1.2k/2.5k [22:03<23:29, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.16e-01 grad: 1.12e+01 lr: 5.0e-04 48.5%┣┫ 1.2k/2.5k [22:04<23:28, 1s/it]\n",
      "Loss train: 8.14e-02 val: 1.14e-01 grad: 9.58e+00 lr: 5.0e-04 48.5%┣┫ 1.2k/2.5k [22:05<23:27, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.12e-01 grad: 1.64e+01 lr: 5.0e-04 48.6%┣┫ 1.2k/2.5k [22:06<23:26, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.20e-01 grad: 9.99e+00 lr: 5.0e-04 48.6%┣┫ 1.2k/2.5k [22:07<23:24, 1s/it]\n",
      "Loss train: 9.56e-02 val: 1.07e-01 grad: 9.99e+00 lr: 5.0e-04 48.6%┣┫ 1.2k/2.5k [22:08<23:23, 1s/it]\n",
      "Loss train: 9.10e-02 val: 1.10e-01 grad: 1.13e+01 lr: 5.0e-04 48.7%┣┫ 1.2k/2.5k [22:09<23:22, 1s/it]\n",
      "Loss train: 8.65e-02 val: 1.13e-01 grad: 1.11e+01 lr: 5.0e-04 48.7%┣┫ 1.2k/2.5k [22:10<23:21, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.25e-01 grad: 1.14e+01 lr: 5.0e-04 48.8%┣┫ 1.2k/2.5k [22:11<23:20, 1s/it]\n",
      "Loss train: 8.43e-02 val: 1.22e-01 grad: 7.78e+00 lr: 5.0e-04 48.8%┣┫ 1.2k/2.5k [22:12<23:19, 1s/it]\n",
      "Loss train: 8.41e-02 val: 1.17e-01 grad: 9.67e+00 lr: 5.0e-04 48.8%┣┫ 1.2k/2.5k [22:13<23:18, 1s/it]\n",
      "Loss train: 8.42e-02 val: 1.14e-01 grad: 1.13e+01 lr: 5.0e-04 48.9%┣┫ 1.2k/2.5k [22:14<23:17, 1s/it]\n",
      "Loss train: 8.36e-02 val: 1.14e-01 grad: 1.12e+01 lr: 5.0e-04 48.9%┣┫ 1.2k/2.5k [22:15<23:15, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.15e-01 grad: 1.12e+01 lr: 5.0e-04 49.0%┣┫ 1.2k/2.5k [22:17<23:14, 1s/it]\n",
      "Loss train: 8.19e-02 val: 1.20e-01 grad: 1.12e+01 lr: 5.0e-04 49.0%┣┫ 1.2k/2.5k [22:18<23:13, 1s/it]\n",
      "Loss train: 8.16e-02 val: 1.18e-01 grad: 1.10e+01 lr: 5.0e-04 49.0%┣┫ 1.2k/2.5k [22:19<23:12, 1s/it]\n",
      "Loss train: 8.12e-02 val: 1.17e-01 grad: 9.90e+00 lr: 5.0e-04 49.1%┣┫ 1.2k/2.5k [22:20<23:11, 1s/it]\n",
      "Loss train: 8.26e-02 val: 1.08e-01 grad: 9.40e+00 lr: 5.0e-04 49.1%┣┫ 1.2k/2.5k [22:21<23:10, 1s/it]\n",
      "Loss train: 9.02e-02 val: 1.10e-01 grad: 1.89e+01 lr: 5.0e-04 49.2%┣┫ 1.2k/2.5k [22:22<23:09, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.12e-01 grad: 9.86e+00 lr: 5.0e-04 49.2%┣┫ 1.2k/2.5k [22:23<23:08, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.10e-01 grad: 1.01e+01 lr: 5.0e-04 49.2%┣┫ 1.2k/2.5k [22:24<23:07, 1s/it]\n",
      "Loss train: 9.12e-02 val: 1.19e-01 grad: 1.10e+01 lr: 5.0e-04 49.3%┣┫ 1.2k/2.5k [22:25<23:06, 1s/it]\n",
      "Loss train: 8.84e-02 val: 1.29e-01 grad: 1.08e+01 lr: 5.0e-04 49.3%┣┫ 1.2k/2.5k [22:26<23:04, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.24e-01 grad: 1.13e+01 lr: 5.0e-04 49.4%┣┫ 1.2k/2.5k [22:27<23:03, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.25e-01 grad: 1.07e+01 lr: 5.0e-04 49.4%┣┫ 1.2k/2.5k [22:28<23:02, 1s/it]\n",
      "Loss train: 8.42e-02 val: 1.18e-01 grad: 9.08e+00 lr: 5.0e-04 49.4%┣┫ 1.2k/2.5k [22:29<23:01, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.16e-01 grad: 1.12e+01 lr: 5.0e-04 49.5%┣┫ 1.2k/2.5k [22:31<23:00, 1s/it]\n",
      "Loss train: 8.35e-02 val: 1.15e-01 grad: 1.13e+01 lr: 5.0e-04 49.5%┣┫ 1.2k/2.5k [22:31<22:59, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.20e-01 grad: 1.12e+01 lr: 5.0e-04 49.6%┣┫ 1.2k/2.5k [22:33<22:58, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.21e-01 grad: 9.50e+00 lr: 5.0e-04 49.6%┣┫ 1.2k/2.5k [22:34<22:57, 1s/it]\n",
      "Loss train: 8.45e-02 val: 1.10e-01 grad: 9.72e+00 lr: 5.0e-04 49.6%┣┫ 1.2k/2.5k [22:35<22:56, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.11e-01 grad: 1.05e+01 lr: 5.0e-04 49.7%┣┫ 1.2k/2.5k [22:36<22:54, 1s/it]\n",
      "Loss train: 8.22e-02 val: 1.14e-01 grad: 1.01e+01 lr: 5.0e-04 49.7%┣┫ 1.2k/2.5k [22:37<22:53, 1s/it]\n",
      "Loss train: 8.16e-02 val: 1.21e-01 grad: 1.01e+01 lr: 5.0e-04 49.8%┣┫ 1.2k/2.5k [22:38<22:52, 1s/it]\n",
      "Loss train: 8.15e-02 val: 1.18e-01 grad: 9.50e+00 lr: 5.0e-04 49.8%┣┫ 1.2k/2.5k [22:39<22:51, 1s/it]\n",
      "Loss train: 8.21e-02 val: 1.23e-01 grad: 9.02e+00 lr: 5.0e-04 49.8%┣┫ 1.2k/2.5k [22:40<22:50, 1s/it]\n",
      "Loss train: 8.37e-02 val: 1.12e-01 grad: 1.05e+01 lr: 5.0e-04 49.9%┣┫ 1.2k/2.5k [22:41<22:49, 1s/it]\n",
      "Loss train: 8.13e-02 val: 1.18e-01 grad: 1.01e+01 lr: 5.0e-04 49.9%┣┫ 1.2k/2.5k [22:42<22:48, 1s/it]\n",
      "Loss train: 8.09e-02 val: 1.21e-01 grad: 1.12e+01 lr: 5.0e-04 50.0%┣┫ 1.2k/2.5k [22:44<22:47, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.26e-01 grad: 1.21e+01 lr: 5.0e-04 50.0%┣┫ 1.2k/2.5k [22:45<22:46, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.23e-01 grad: 9.60e+00 lr: 5.0e-04 50.0%┣┫ 1.3k/2.5k [22:46<22:45, 1s/it]\n",
      "Loss train: 8.33e-02 val: 1.17e-01 grad: 1.03e+01 lr: 5.0e-04 50.1%┣┫ 1.3k/2.5k [22:47<22:44, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.19e-01 grad: 1.07e+01 lr: 5.0e-04 50.1%┣┫ 1.3k/2.5k [22:48<22:42, 1s/it]\n",
      "Loss train: 8.20e-02 val: 1.12e-01 grad: 1.05e+01 lr: 5.0e-04 50.2%┣┫ 1.3k/2.5k [22:49<22:41, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.09e-01 grad: 1.12e+01 lr: 5.0e-04 50.2%┣┫ 1.3k/2.5k [22:50<22:40, 1s/it]\n",
      "Loss train: 8.10e-02 val: 1.14e-01 grad: 1.09e+01 lr: 5.0e-04 50.2%┣┫ 1.3k/2.5k [22:51<22:39, 1s/it]\n",
      "Loss train: 8.17e-02 val: 1.17e-01 grad: 1.13e+01 lr: 5.0e-04 50.3%┣┫ 1.3k/2.5k [22:52<22:38, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.14e-01 grad: 1.00e+01 lr: 5.0e-04 50.3%┣┫ 1.3k/2.5k [22:53<22:37, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.07e-01 grad: 1.46e+01 lr: 5.0e-04 50.4%┣┫ 1.3k/2.5k [22:54<22:36, 1s/it]\n",
      "Loss train: 9.47e-02 val: 1.20e-01 grad: 8.41e+00 lr: 5.0e-04 50.4%┣┫ 1.3k/2.5k [22:55<22:35, 1s/it]\n",
      "Loss train: 8.76e-02 val: 1.12e-01 grad: 9.05e+00 lr: 5.0e-04 50.4%┣┫ 1.3k/2.5k [22:56<22:34, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.15e-01 grad: 1.10e+01 lr: 5.0e-04 50.5%┣┫ 1.3k/2.5k [22:58<22:32, 1s/it]\n",
      "Loss train: 8.55e-02 val: 1.10e-01 grad: 1.03e+01 lr: 5.0e-04 50.5%┣┫ 1.3k/2.5k [22:59<22:31, 1s/it]\n",
      "Loss train: 8.33e-02 val: 1.10e-01 grad: 1.03e+01 lr: 5.0e-04 50.6%┣┫ 1.3k/2.5k [23:00<22:30, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.16e-01 grad: 1.08e+01 lr: 5.0e-04 50.6%┣┫ 1.3k/2.5k [23:01<22:29, 1s/it]\n",
      "Loss train: 8.60e-02 val: 1.20e-01 grad: 9.41e+00 lr: 5.0e-04 50.6%┣┫ 1.3k/2.5k [23:02<22:28, 1s/it]\n",
      "Loss train: 8.24e-02 val: 1.11e-01 grad: 8.95e+00 lr: 5.0e-04 50.7%┣┫ 1.3k/2.5k [23:03<22:27, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.10e-01 grad: 1.05e+01 lr: 5.0e-04 50.7%┣┫ 1.3k/2.5k [23:04<22:26, 1s/it]\n",
      "Loss train: 8.16e-02 val: 1.21e-01 grad: 1.02e+01 lr: 5.0e-04 50.8%┣┫ 1.3k/2.5k [23:05<22:25, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.18e-01 grad: 1.03e+01 lr: 5.0e-04 50.8%┣┫ 1.3k/2.5k [23:06<22:24, 1s/it]\n",
      "Loss train: 8.03e-02 val: 1.12e-01 grad: 1.13e+01 lr: 5.0e-04 50.8%┣┫ 1.3k/2.5k [23:07<22:22, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.33e-01 grad: 1.50e+01 lr: 5.0e-04 50.9%┣┫ 1.3k/2.5k [23:08<22:21, 1s/it]\n",
      "Loss train: 9.31e-02 val: 1.17e-01 grad: 8.34e+00 lr: 5.0e-04 50.9%┣┫ 1.3k/2.5k [23:10<22:20, 1s/it]\n",
      "Loss train: 8.99e-02 val: 1.05e-01 grad: 9.63e+00 lr: 5.0e-04 51.0%┣┫ 1.3k/2.5k [23:11<22:19, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.11e-01 grad: 1.03e+01 lr: 5.0e-04 51.0%┣┫ 1.3k/2.5k [23:12<22:18, 1s/it]\n",
      "Loss train: 8.36e-02 val: 1.09e-01 grad: 1.09e+01 lr: 5.0e-04 51.0%┣┫ 1.3k/2.5k [23:13<22:17, 1s/it]\n",
      "Loss train: 8.79e-02 val: 1.24e-01 grad: 8.28e+00 lr: 5.0e-04 51.1%┣┫ 1.3k/2.5k [23:14<22:16, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.21e-01 grad: 8.00e+00 lr: 5.0e-04 51.1%┣┫ 1.3k/2.5k [23:15<22:15, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.12e-01 grad: 1.10e+01 lr: 5.0e-04 51.2%┣┫ 1.3k/2.5k [23:16<22:14, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.14e-01 grad: 1.07e+01 lr: 5.0e-04 51.2%┣┫ 1.3k/2.5k [23:17<22:12, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.24e-01 grad: 1.10e+01 lr: 5.0e-04 51.2%┣┫ 1.3k/2.5k [23:18<22:11, 1s/it]\n",
      "Loss train: 8.15e-02 val: 1.20e-01 grad: 8.78e+00 lr: 5.0e-04 51.3%┣┫ 1.3k/2.5k [23:19<22:10, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.21e-01 grad: 1.11e+01 lr: 5.0e-04 51.3%┣┫ 1.3k/2.5k [23:20<22:09, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.19e-01 grad: 1.08e+01 lr: 5.0e-04 51.4%┣┫ 1.3k/2.5k [23:21<22:08, 1s/it]\n",
      "Loss train: 8.16e-02 val: 1.05e-01 grad: 1.09e+01 lr: 5.0e-04 51.4%┣┫ 1.3k/2.5k [23:22<22:07, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.18e-01 grad: 1.70e+01 lr: 5.0e-04 51.4%┣┫ 1.3k/2.5k [23:23<22:06, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.20e-01 grad: 1.44e+01 lr: 5.0e-04 51.5%┣┫ 1.3k/2.5k [23:25<22:05, 1s/it]\n",
      "Loss train: 9.70e-02 val: 1.11e-01 grad: 1.06e+01 lr: 5.0e-04 51.5%┣┫ 1.3k/2.5k [23:26<22:04, 1s/it]\n",
      "Loss train: 8.86e-02 val: 1.14e-01 grad: 1.11e+01 lr: 5.0e-04 51.6%┣┫ 1.3k/2.5k [23:27<22:03, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.18e-01 grad: 1.09e+01 lr: 5.0e-04 51.6%┣┫ 1.3k/2.5k [23:28<22:01, 1s/it]\n",
      "Loss train: 9.11e-02 val: 1.31e-01 grad: 9.30e+00 lr: 5.0e-04 51.6%┣┫ 1.3k/2.5k [23:29<22:00, 1s/it]\n",
      "Loss train: 8.36e-02 val: 1.26e-01 grad: 7.35e+00 lr: 5.0e-04 51.7%┣┫ 1.3k/2.5k [23:30<21:59, 1s/it]\n",
      "Loss train: 8.24e-02 val: 1.16e-01 grad: 1.05e+01 lr: 5.0e-04 51.7%┣┫ 1.3k/2.5k [23:31<21:58, 1s/it]\n",
      "Loss train: 8.22e-02 val: 1.13e-01 grad: 1.09e+01 lr: 5.0e-04 51.8%┣┫ 1.3k/2.5k [23:32<21:57, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.07e-01 grad: 1.02e+01 lr: 5.0e-04 51.8%┣┫ 1.3k/2.5k [23:33<21:56, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.13e-01 grad: 1.07e+01 lr: 5.0e-04 51.8%┣┫ 1.3k/2.5k [23:34<21:55, 1s/it]\n",
      "Loss train: 8.33e-02 val: 1.09e-01 grad: 1.79e+01 lr: 5.0e-04 51.9%┣┫ 1.3k/2.5k [23:35<21:54, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.19e-01 grad: 1.07e+01 lr: 5.0e-04 51.9%┣┫ 1.3k/2.5k [23:36<21:53, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.10e-01 grad: 1.23e+01 lr: 5.0e-04 52.0%┣┫ 1.3k/2.5k [23:38<21:52, 1s/it]\n",
      "Loss train: 1.06e-01 val: 1.19e-01 grad: 1.10e+01 lr: 5.0e-04 52.0%┣┫ 1.3k/2.5k [23:39<21:51, 1s/it]\n",
      "Loss train: 9.35e-02 val: 1.12e-01 grad: 9.98e+00 lr: 5.0e-04 52.0%┣┫ 1.3k/2.5k [23:40<21:49, 1s/it]\n",
      "Loss train: 8.46e-02 val: 1.21e-01 grad: 1.12e+01 lr: 5.0e-04 52.1%┣┫ 1.3k/2.5k [23:41<21:48, 1s/it]\n",
      "Loss train: 8.46e-02 val: 1.27e-01 grad: 1.08e+01 lr: 5.0e-04 52.1%┣┫ 1.3k/2.5k [23:42<21:47, 1s/it]\n",
      "Loss train: 8.26e-02 val: 1.23e-01 grad: 1.06e+01 lr: 5.0e-04 52.2%┣┫ 1.3k/2.5k [23:43<21:46, 1s/it]\n",
      "Loss train: 9.11e-02 val: 1.33e-01 grad: 1.03e+01 lr: 5.0e-04 52.2%┣┫ 1.3k/2.5k [23:44<21:45, 1s/it]\n",
      "Loss train: 8.17e-02 val: 1.17e-01 grad: 9.13e+00 lr: 5.0e-04 52.2%┣┫ 1.3k/2.5k [23:45<21:44, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.20e-01 grad: 1.11e+01 lr: 5.0e-04 52.3%┣┫ 1.3k/2.5k [23:46<21:42, 1s/it]\n",
      "Loss train: 8.05e-02 val: 1.17e-01 grad: 1.09e+01 lr: 5.0e-04 52.3%┣┫ 1.3k/2.5k [23:47<21:41, 1s/it]\n",
      "Loss train: 8.09e-02 val: 1.11e-01 grad: 1.10e+01 lr: 5.0e-04 52.4%┣┫ 1.3k/2.5k [23:48<21:40, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.24e-01 grad: 1.26e+01 lr: 5.0e-04 52.4%┣┫ 1.3k/2.5k [23:49<21:39, 1s/it]\n",
      "Loss train: 8.29e-02 val: 1.15e-01 grad: 7.86e+00 lr: 5.0e-04 52.4%┣┫ 1.3k/2.5k [23:50<21:38, 1s/it]\n",
      "Loss train: 8.37e-02 val: 1.10e-01 grad: 1.01e+01 lr: 5.0e-04 52.5%┣┫ 1.3k/2.5k [23:51<21:37, 1s/it]\n",
      "Loss train: 8.23e-02 val: 1.10e-01 grad: 1.01e+01 lr: 5.0e-04 52.5%┣┫ 1.3k/2.5k [23:52<21:36, 1s/it]\n",
      "Loss train: 8.05e-02 val: 1.15e-01 grad: 1.11e+01 lr: 5.0e-04 52.6%┣┫ 1.3k/2.5k [23:53<21:35, 1s/it]\n",
      "Loss train: 8.00e-02 val: 1.15e-01 grad: 1.10e+01 lr: 5.0e-04 52.6%┣┫ 1.3k/2.5k [23:55<21:34, 1s/it]\n",
      "Loss train: 8.17e-02 val: 1.13e-01 grad: 1.92e+01 lr: 5.0e-04 52.6%┣┫ 1.3k/2.5k [23:56<21:33, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.22e-01 grad: 1.17e+01 lr: 5.0e-04 52.7%┣┫ 1.3k/2.5k [23:57<21:32, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.11e-01 grad: 1.16e+01 lr: 5.0e-04 52.7%┣┫ 1.3k/2.5k [23:58<21:31, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.20e-01 grad: 1.15e+01 lr: 5.0e-04 52.8%┣┫ 1.3k/2.5k [23:59<21:29, 1s/it]\n",
      "Loss train: 9.36e-02 val: 1.13e-01 grad: 1.02e+01 lr: 5.0e-04 52.8%┣┫ 1.3k/2.5k [24:00<21:28, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.18e-01 grad: 1.03e+01 lr: 5.0e-04 52.8%┣┫ 1.3k/2.5k [24:01<21:27, 1s/it]\n",
      "Loss train: 8.74e-02 val: 1.27e-01 grad: 9.50e+00 lr: 5.0e-04 52.9%┣┫ 1.3k/2.5k [24:02<21:26, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.24e-01 grad: 7.58e+00 lr: 5.0e-04 52.9%┣┫ 1.3k/2.5k [24:03<21:25, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.16e-01 grad: 1.09e+01 lr: 5.0e-04 53.0%┣┫ 1.3k/2.5k [24:04<21:24, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.11e-01 grad: 1.01e+01 lr: 5.0e-04 53.0%┣┫ 1.3k/2.5k [24:05<21:23, 1s/it]\n",
      "Loss train: 8.17e-02 val: 1.19e-01 grad: 1.04e+01 lr: 5.0e-04 53.0%┣┫ 1.3k/2.5k [24:06<21:22, 1s/it]\n",
      "Loss train: 8.42e-02 val: 1.27e-01 grad: 9.15e+00 lr: 5.0e-04 53.1%┣┫ 1.3k/2.5k [24:07<21:20, 1s/it]\n",
      "Loss train: 8.28e-02 val: 1.09e-01 grad: 7.93e+00 lr: 5.0e-04 53.1%┣┫ 1.3k/2.5k [24:09<21:19, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.24e-01 grad: 2.11e+01 lr: 5.0e-04 53.2%┣┫ 1.3k/2.5k [24:10<21:18, 1s/it]\n",
      "Loss train: 1.25e-01 val: 1.07e-01 grad: 1.44e+01 lr: 5.0e-04 53.2%┣┫ 1.3k/2.5k [24:11<21:17, 1s/it]\n",
      "Loss train: 1.05e-01 val: 1.17e-01 grad: 1.14e+01 lr: 5.0e-04 53.2%┣┫ 1.3k/2.5k [24:12<21:16, 1s/it]\n",
      "Loss train: 9.29e-02 val: 1.19e-01 grad: 1.01e+01 lr: 5.0e-04 53.3%┣┫ 1.3k/2.5k [24:13<21:15, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.32e-01 grad: 1.08e+01 lr: 5.0e-04 53.3%┣┫ 1.3k/2.5k [24:14<21:14, 1s/it]\n",
      "Loss train: 9.07e-02 val: 1.32e-01 grad: 9.13e+00 lr: 5.0e-04 53.4%┣┫ 1.3k/2.5k [24:15<21:13, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.21e-01 grad: 7.51e+00 lr: 5.0e-04 53.4%┣┫ 1.3k/2.5k [24:16<21:12, 1s/it]\n",
      "Loss train: 9.10e-02 val: 1.10e-01 grad: 1.32e+01 lr: 5.0e-04 53.4%┣┫ 1.3k/2.5k [24:17<21:10, 1s/it]\n",
      "Loss train: 8.78e-02 val: 1.16e-01 grad: 1.07e+01 lr: 5.0e-04 53.5%┣┫ 1.3k/2.5k [24:18<21:09, 1s/it]\n",
      "Loss train: 8.82e-02 val: 1.22e-01 grad: 1.08e+01 lr: 5.0e-04 53.5%┣┫ 1.3k/2.5k [24:19<21:08, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.20e-01 grad: 1.05e+01 lr: 5.0e-04 53.6%┣┫ 1.3k/2.5k [24:20<21:07, 1s/it]\n",
      "Loss train: 8.37e-02 val: 1.20e-01 grad: 1.03e+01 lr: 5.0e-04 53.6%┣┫ 1.3k/2.5k [24:21<21:06, 1s/it]\n",
      "Loss train: 8.18e-02 val: 1.14e-01 grad: 1.09e+01 lr: 5.0e-04 53.6%┣┫ 1.3k/2.5k [24:22<21:05, 1s/it]\n",
      "Loss train: 8.23e-02 val: 1.15e-01 grad: 9.61e+00 lr: 5.0e-04 53.7%┣┫ 1.3k/2.5k [24:24<21:04, 1s/it]\n",
      "Loss train: 8.16e-02 val: 1.08e-01 grad: 1.07e+01 lr: 5.0e-04 53.7%┣┫ 1.3k/2.5k [24:25<21:03, 1s/it]\n",
      "Loss train: 8.28e-02 val: 1.15e-01 grad: 1.06e+01 lr: 5.0e-04 53.8%┣┫ 1.3k/2.5k [24:25<21:01, 1s/it]\n",
      "Loss train: 8.01e-02 val: 1.08e-01 grad: 1.05e+01 lr: 5.0e-04 53.8%┣┫ 1.3k/2.5k [24:27<21:00, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.11e-01 grad: 1.02e+01 lr: 5.0e-04 53.8%┣┫ 1.3k/2.5k [24:28<20:59, 1s/it]\n",
      "Loss train: 8.03e-02 val: 1.12e-01 grad: 9.87e+00 lr: 5.0e-04 53.9%┣┫ 1.3k/2.5k [24:29<20:58, 1s/it]\n",
      "Loss train: 7.93e-02 val: 1.04e-01 grad: 9.27e+00 lr: 5.0e-04 53.9%┣┫ 1.3k/2.5k [24:30<20:57, 1s/it]\n",
      "Loss train: 9.72e-02 val: 1.19e-01 grad: 1.71e+01 lr: 5.0e-04 54.0%┣┫ 1.3k/2.5k [24:31<20:56, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.13e-01 grad: 9.02e+00 lr: 5.0e-04 54.0%┣┫ 1.4k/2.5k [24:32<20:55, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.06e-01 grad: 1.02e+01 lr: 5.0e-04 54.0%┣┫ 1.4k/2.5k [24:33<20:54, 1s/it]\n",
      "Loss train: 9.06e-02 val: 1.13e-01 grad: 1.10e+01 lr: 5.0e-04 54.1%┣┫ 1.4k/2.5k [24:34<20:53, 1s/it]\n",
      "Loss train: 9.16e-02 val: 1.23e-01 grad: 9.71e+00 lr: 5.0e-04 54.1%┣┫ 1.4k/2.5k [24:36<20:52, 1s/it]\n",
      "Loss train: 8.90e-02 val: 1.23e-01 grad: 8.47e+00 lr: 5.0e-04 54.2%┣┫ 1.4k/2.5k [24:37<20:51, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.08e-01 grad: 1.04e+01 lr: 5.0e-04 54.2%┣┫ 1.4k/2.5k [24:39<20:51, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.06e-01 grad: 1.07e+01 lr: 5.0e-04 54.2%┣┫ 1.4k/2.5k [24:41<20:50, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.13e-01 grad: 1.01e+01 lr: 5.0e-04 54.3%┣┫ 1.4k/2.5k [24:42<20:50, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.19e-01 grad: 8.59e+00 lr: 5.0e-04 54.3%┣┫ 1.4k/2.5k [24:44<20:49, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.08e-01 grad: 9.79e+00 lr: 5.0e-04 54.4%┣┫ 1.4k/2.5k [24:45<20:48, 1s/it]\n",
      "Loss train: 8.22e-02 val: 1.11e-01 grad: 1.07e+01 lr: 5.0e-04 54.4%┣┫ 1.4k/2.5k [24:46<20:47, 1s/it]\n",
      "Loss train: 8.09e-02 val: 1.20e-01 grad: 1.13e+01 lr: 5.0e-04 54.4%┣┫ 1.4k/2.5k [24:47<20:46, 1s/it]\n",
      "Loss train: 8.03e-02 val: 1.19e-01 grad: 8.17e+00 lr: 5.0e-04 54.5%┣┫ 1.4k/2.5k [24:48<20:44, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.15e-01 grad: 9.56e+00 lr: 5.0e-04 54.5%┣┫ 1.4k/2.5k [24:49<20:43, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.21e-01 grad: 1.48e+01 lr: 5.0e-04 54.6%┣┫ 1.4k/2.5k [24:50<20:42, 1s/it]\n",
      "Loss train: 9.09e-02 val: 1.13e-01 grad: 8.91e+00 lr: 5.0e-04 54.6%┣┫ 1.4k/2.5k [24:52<20:41, 1s/it]\n",
      "Loss train: 9.32e-02 val: 1.07e-01 grad: 1.01e+01 lr: 5.0e-04 54.6%┣┫ 1.4k/2.5k [24:53<20:40, 1s/it]\n",
      "Loss train: 8.56e-02 val: 1.12e-01 grad: 1.08e+01 lr: 5.0e-04 54.7%┣┫ 1.4k/2.5k [24:54<20:39, 1s/it]\n",
      "Loss train: 9.30e-02 val: 1.27e-01 grad: 1.07e+01 lr: 5.0e-04 54.7%┣┫ 1.4k/2.5k [24:55<20:38, 1s/it]\n",
      "Loss train: 8.19e-02 val: 1.13e-01 grad: 9.45e+00 lr: 5.0e-04 54.8%┣┫ 1.4k/2.5k [24:56<20:37, 1s/it]\n",
      "Loss train: 8.15e-02 val: 1.16e-01 grad: 1.11e+01 lr: 5.0e-04 54.8%┣┫ 1.4k/2.5k [24:57<20:36, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.15e-01 grad: 1.05e+01 lr: 5.0e-04 54.8%┣┫ 1.4k/2.5k [24:58<20:35, 1s/it]\n",
      "Loss train: 8.02e-02 val: 1.14e-01 grad: 1.08e+01 lr: 5.0e-04 54.9%┣┫ 1.4k/2.5k [24:59<20:33, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.16e-01 grad: 1.08e+01 lr: 5.0e-04 54.9%┣┫ 1.4k/2.5k [25:00<20:32, 1s/it]\n",
      "Loss train: 8.18e-02 val: 1.07e-01 grad: 9.47e+00 lr: 5.0e-04 55.0%┣┫ 1.4k/2.5k [25:01<20:31, 1s/it]\n",
      "Loss train: 8.30e-02 val: 1.04e-01 grad: 1.09e+01 lr: 5.0e-04 55.0%┣┫ 1.4k/2.5k [25:02<20:30, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.14e-01 grad: 1.05e+01 lr: 5.0e-04 55.0%┣┫ 1.4k/2.5k [25:04<20:29, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.14e-01 grad: 8.79e+00 lr: 5.0e-04 55.1%┣┫ 1.4k/2.5k [25:05<20:28, 1s/it]\n",
      "Loss train: 7.94e-02 val: 1.11e-01 grad: 1.13e+01 lr: 5.0e-04 55.1%┣┫ 1.4k/2.5k [25:06<20:27, 1s/it]\n",
      "Loss train: 8.06e-02 val: 1.07e-01 grad: 9.81e+00 lr: 5.0e-04 55.2%┣┫ 1.4k/2.5k [25:07<20:26, 1s/it]\n",
      "Loss train: 7.87e-02 val: 1.11e-01 grad: 1.12e+01 lr: 5.0e-04 55.2%┣┫ 1.4k/2.5k [25:08<20:25, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.08e-01 grad: 9.18e+00 lr: 5.0e-04 55.2%┣┫ 1.4k/2.5k [25:09<20:24, 1s/it]\n",
      "Loss train: 8.15e-02 val: 1.14e-01 grad: 2.08e+01 lr: 5.0e-04 55.3%┣┫ 1.4k/2.5k [25:10<20:23, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.07e-01 grad: 1.14e+01 lr: 5.0e-04 55.3%┣┫ 1.4k/2.5k [25:11<20:22, 1s/it]\n",
      "Loss train: 1.14e-01 val: 1.14e-01 grad: 1.28e+01 lr: 5.0e-04 55.4%┣┫ 1.4k/2.5k [25:13<20:21, 1s/it]\n",
      "Loss train: 1.07e-01 val: 1.20e-01 grad: 1.11e+01 lr: 5.0e-04 55.4%┣┫ 1.4k/2.5k [25:14<20:19, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.20e-01 grad: 9.84e+00 lr: 5.0e-04 55.4%┣┫ 1.4k/2.5k [25:15<20:18, 1s/it]\n",
      "Loss train: 8.89e-02 val: 1.36e-01 grad: 9.57e+00 lr: 5.0e-04 55.5%┣┫ 1.4k/2.5k [25:16<20:17, 1s/it]\n",
      "Loss train: 8.83e-02 val: 1.34e-01 grad: 6.56e+00 lr: 5.0e-04 55.5%┣┫ 1.4k/2.5k [25:17<20:16, 1s/it]\n",
      "Loss train: 8.42e-02 val: 1.26e-01 grad: 9.85e+00 lr: 5.0e-04 55.6%┣┫ 1.4k/2.5k [25:18<20:15, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.19e-01 grad: 1.11e+01 lr: 5.0e-04 55.6%┣┫ 1.4k/2.5k [25:19<20:14, 1s/it]\n",
      "Loss train: 8.19e-02 val: 1.22e-01 grad: 1.06e+01 lr: 5.0e-04 55.6%┣┫ 1.4k/2.5k [25:20<20:13, 1s/it]\n",
      "Loss train: 8.11e-02 val: 1.19e-01 grad: 1.00e+01 lr: 5.0e-04 55.7%┣┫ 1.4k/2.5k [25:21<20:12, 1s/it]\n",
      "Loss train: 8.05e-02 val: 1.19e-01 grad: 8.58e+00 lr: 5.0e-04 55.7%┣┫ 1.4k/2.5k [25:22<20:11, 1s/it]\n",
      "Loss train: 8.40e-02 val: 1.07e-01 grad: 1.05e+01 lr: 5.0e-04 55.8%┣┫ 1.4k/2.5k [25:23<20:10, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.11e-01 grad: 9.87e+00 lr: 5.0e-04 55.8%┣┫ 1.4k/2.5k [25:25<20:09, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.22e-01 grad: 9.01e+00 lr: 5.0e-04 55.8%┣┫ 1.4k/2.5k [25:26<20:07, 1s/it]\n",
      "Loss train: 8.11e-02 val: 1.18e-01 grad: 6.99e+00 lr: 5.0e-04 55.9%┣┫ 1.4k/2.5k [25:27<20:06, 1s/it]\n",
      "Loss train: 8.72e-02 val: 1.16e-01 grad: 1.55e+01 lr: 5.0e-04 55.9%┣┫ 1.4k/2.5k [25:28<20:05, 1s/it]\n",
      "Loss train: 9.93e-02 val: 1.23e-01 grad: 1.05e+01 lr: 5.0e-04 56.0%┣┫ 1.4k/2.5k [25:29<20:04, 1s/it]\n",
      "Loss train: 9.29e-02 val: 1.15e-01 grad: 9.10e+00 lr: 5.0e-04 56.0%┣┫ 1.4k/2.5k [25:30<20:03, 1s/it]\n",
      "Loss train: 9.04e-02 val: 1.08e-01 grad: 1.02e+01 lr: 5.0e-04 56.0%┣┫ 1.4k/2.5k [25:31<20:02, 1s/it]\n",
      "Loss train: 8.48e-02 val: 1.16e-01 grad: 9.99e+00 lr: 5.0e-04 56.1%┣┫ 1.4k/2.5k [25:32<20:01, 1s/it]\n",
      "Loss train: 8.66e-02 val: 1.27e-01 grad: 9.56e+00 lr: 5.0e-04 56.1%┣┫ 1.4k/2.5k [25:33<20:00, 1s/it]\n",
      "Loss train: 8.23e-02 val: 1.20e-01 grad: 7.39e+00 lr: 5.0e-04 56.2%┣┫ 1.4k/2.5k [25:34<19:58, 1s/it]\n",
      "Loss train: 8.72e-02 val: 1.06e-01 grad: 1.12e+01 lr: 5.0e-04 56.2%┣┫ 1.4k/2.5k [25:35<19:57, 1s/it]\n",
      "Loss train: 8.08e-02 val: 1.14e-01 grad: 1.06e+01 lr: 5.0e-04 56.2%┣┫ 1.4k/2.5k [25:36<19:56, 1s/it]\n",
      "Loss train: 8.32e-02 val: 1.22e-01 grad: 9.99e+00 lr: 5.0e-04 56.3%┣┫ 1.4k/2.5k [25:37<19:55, 1s/it]\n",
      "Loss train: 7.99e-02 val: 1.13e-01 grad: 1.03e+01 lr: 5.0e-04 56.3%┣┫ 1.4k/2.5k [25:38<19:54, 1s/it]\n",
      "Loss train: 7.94e-02 val: 1.15e-01 grad: 1.05e+01 lr: 5.0e-04 56.4%┣┫ 1.4k/2.5k [25:39<19:53, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.19e-01 grad: 1.00e+01 lr: 5.0e-04 56.4%┣┫ 1.4k/2.5k [25:40<19:52, 1s/it]\n",
      "Loss train: 7.94e-02 val: 1.06e-01 grad: 8.69e+00 lr: 5.0e-04 56.4%┣┫ 1.4k/2.5k [25:42<19:51, 1s/it]\n",
      "Loss train: 9.34e-02 val: 1.14e-01 grad: 1.65e+01 lr: 5.0e-04 56.5%┣┫ 1.4k/2.5k [25:43<19:50, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.12e-01 grad: 1.07e+01 lr: 5.0e-04 56.5%┣┫ 1.4k/2.5k [25:44<19:49, 1s/it]\n",
      "Loss train: 9.68e-02 val: 1.09e-01 grad: 9.90e+00 lr: 5.0e-04 56.6%┣┫ 1.4k/2.5k [25:45<19:47, 1s/it]\n",
      "Loss train: 9.20e-02 val: 1.11e-01 grad: 1.07e+01 lr: 5.0e-04 56.6%┣┫ 1.4k/2.5k [25:46<19:46, 1s/it]\n",
      "Loss train: 8.41e-02 val: 1.23e-01 grad: 1.02e+01 lr: 5.0e-04 56.6%┣┫ 1.4k/2.5k [25:47<19:45, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.30e-01 grad: 8.52e+00 lr: 5.0e-04 56.7%┣┫ 1.4k/2.5k [25:48<19:44, 1s/it]\n",
      "Loss train: 8.24e-02 val: 1.23e-01 grad: 7.50e+00 lr: 5.0e-04 56.7%┣┫ 1.4k/2.5k [25:49<19:43, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.12e-01 grad: 1.08e+01 lr: 5.0e-04 56.8%┣┫ 1.4k/2.5k [25:50<19:42, 1s/it]\n",
      "Loss train: 8.39e-02 val: 1.09e-01 grad: 9.96e+00 lr: 5.0e-04 56.8%┣┫ 1.4k/2.5k [25:51<19:41, 1s/it]\n",
      "Loss train: 8.22e-02 val: 1.23e-01 grad: 9.36e+00 lr: 5.0e-04 56.8%┣┫ 1.4k/2.5k [25:52<19:40, 1s/it]\n",
      "Loss train: 7.96e-02 val: 1.18e-01 grad: 9.68e+00 lr: 5.0e-04 56.9%┣┫ 1.4k/2.5k [25:53<19:38, 1s/it]\n",
      "Loss train: 8.03e-02 val: 1.19e-01 grad: 9.58e+00 lr: 5.0e-04 56.9%┣┫ 1.4k/2.5k [25:55<19:37, 1s/it]\n",
      "Loss train: 7.97e-02 val: 1.06e-01 grad: 1.10e+01 lr: 5.0e-04 57.0%┣┫ 1.4k/2.5k [25:56<19:36, 1s/it]\n",
      "Loss train: 7.87e-02 val: 1.10e-01 grad: 1.11e+01 lr: 5.0e-04 57.0%┣┫ 1.4k/2.5k [25:57<19:35, 1s/it]\n",
      "Loss train: 8.21e-02 val: 1.19e-01 grad: 8.99e+00 lr: 5.0e-04 57.0%┣┫ 1.4k/2.5k [25:58<19:34, 1s/it]\n",
      "Loss train: 8.01e-02 val: 1.12e-01 grad: 9.21e+00 lr: 5.0e-04 57.1%┣┫ 1.4k/2.5k [25:59<19:33, 1s/it]\n",
      "Loss train: 7.97e-02 val: 1.13e-01 grad: 1.01e+01 lr: 5.0e-04 57.1%┣┫ 1.4k/2.5k [26:00<19:32, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.13e-01 grad: 1.07e+01 lr: 5.0e-04 57.2%┣┫ 1.4k/2.5k [26:01<19:31, 1s/it]\n",
      "Loss train: 7.91e-02 val: 1.07e-01 grad: 1.04e+01 lr: 5.0e-04 57.2%┣┫ 1.4k/2.5k [26:02<19:30, 1s/it]\n",
      "Loss train: 1.17e-01 val: 1.41e-01 grad: 1.59e+01 lr: 5.0e-04 57.2%┣┫ 1.4k/2.5k [26:03<19:29, 1s/it]\n",
      "Loss train: 9.27e-02 val: 1.11e-01 grad: 1.06e+01 lr: 5.0e-04 57.3%┣┫ 1.4k/2.5k [26:05<19:28, 1s/it]\n",
      "Loss train: 9.65e-02 val: 1.11e-01 grad: 1.09e+01 lr: 5.0e-04 57.3%┣┫ 1.4k/2.5k [26:06<19:27, 1s/it]\n",
      "Loss train: 9.26e-02 val: 1.13e-01 grad: 1.07e+01 lr: 5.0e-04 57.4%┣┫ 1.4k/2.5k [26:07<19:25, 1s/it]\n",
      "Loss train: 8.38e-02 val: 1.19e-01 grad: 9.81e+00 lr: 5.0e-04 57.4%┣┫ 1.4k/2.5k [26:08<19:24, 1s/it]\n",
      "Loss train: 8.78e-02 val: 1.26e-01 grad: 9.43e+00 lr: 5.0e-04 57.4%┣┫ 1.4k/2.5k [26:09<19:23, 1s/it]\n",
      "Loss train: 8.26e-02 val: 1.17e-01 grad: 7.75e+00 lr: 5.0e-04 57.5%┣┫ 1.4k/2.5k [26:10<19:22, 1s/it]\n",
      "Loss train: 8.61e-02 val: 1.07e-01 grad: 1.10e+01 lr: 5.0e-04 57.5%┣┫ 1.4k/2.5k [26:11<19:21, 1s/it]\n",
      "Loss train: 8.28e-02 val: 1.09e-01 grad: 1.07e+01 lr: 5.0e-04 57.6%┣┫ 1.4k/2.5k [26:12<19:20, 1s/it]\n",
      "Loss train: 8.02e-02 val: 1.17e-01 grad: 9.94e+00 lr: 5.0e-04 57.6%┣┫ 1.4k/2.5k [26:13<19:19, 1s/it]\n",
      "Loss train: 8.67e-02 val: 1.26e-01 grad: 9.12e+00 lr: 5.0e-04 57.6%┣┫ 1.4k/2.5k [26:14<19:18, 1s/it]\n",
      "Loss train: 7.95e-02 val: 1.14e-01 grad: 9.32e+00 lr: 5.0e-04 57.7%┣┫ 1.4k/2.5k [26:16<19:17, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.06e-01 grad: 1.00e+01 lr: 5.0e-04 57.7%┣┫ 1.4k/2.5k [26:17<19:16, 1s/it]\n",
      "Loss train: 7.88e-02 val: 1.12e-01 grad: 1.00e+01 lr: 5.0e-04 57.8%┣┫ 1.4k/2.5k [26:18<19:15, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.17e-01 grad: 9.57e+00 lr: 5.0e-04 57.8%┣┫ 1.4k/2.5k [26:19<19:13, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.15e-01 grad: 1.05e+01 lr: 5.0e-04 57.8%┣┫ 1.4k/2.5k [26:20<19:12, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.07e-01 grad: 1.01e+01 lr: 5.0e-04 57.9%┣┫ 1.4k/2.5k [26:21<19:11, 1s/it]\n",
      "Loss train: 7.91e-02 val: 1.14e-01 grad: 1.04e+01 lr: 5.0e-04 57.9%┣┫ 1.4k/2.5k [26:22<19:10, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.13e-01 grad: 8.98e+00 lr: 5.0e-04 58.0%┣┫ 1.4k/2.5k [26:23<19:09, 1s/it]\n",
      "Loss train: 7.95e-02 val: 1.08e-01 grad: 1.73e+01 lr: 5.0e-04 58.0%┣┫ 1.4k/2.5k [26:24<19:08, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.09e-01 grad: 8.91e+00 lr: 5.0e-04 58.0%┣┫ 1.5k/2.5k [26:25<19:07, 1s/it]\n",
      "Loss train: 9.96e-02 val: 1.05e-01 grad: 1.17e+01 lr: 5.0e-04 58.1%┣┫ 1.5k/2.5k [26:27<19:06, 1s/it]\n",
      "Loss train: 8.98e-02 val: 1.11e-01 grad: 8.65e+00 lr: 5.0e-04 58.1%┣┫ 1.5k/2.5k [26:28<19:05, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.15e-01 grad: 1.08e+01 lr: 5.0e-04 58.2%┣┫ 1.5k/2.5k [26:29<19:04, 1s/it]\n",
      "Loss train: 8.40e-02 val: 1.21e-01 grad: 1.07e+01 lr: 5.0e-04 58.2%┣┫ 1.5k/2.5k [26:30<19:03, 1s/it]\n",
      "Loss train: 8.64e-02 val: 1.31e-01 grad: 9.58e+00 lr: 5.0e-04 58.2%┣┫ 1.5k/2.5k [26:31<19:02, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.28e-01 grad: 8.70e+00 lr: 5.0e-04 58.3%┣┫ 1.5k/2.5k [26:32<19:00, 1s/it]\n",
      "Loss train: 7.99e-02 val: 1.20e-01 grad: 9.90e+00 lr: 5.0e-04 58.3%┣┫ 1.5k/2.5k [26:33<18:59, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.10e-01 grad: 1.05e+01 lr: 5.0e-04 58.4%┣┫ 1.5k/2.5k [26:34<18:58, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.15e-01 grad: 1.06e+01 lr: 5.0e-04 58.4%┣┫ 1.5k/2.5k [26:35<18:57, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.15e-01 grad: 1.02e+01 lr: 5.0e-04 58.4%┣┫ 1.5k/2.5k [26:36<18:56, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.07e-01 grad: 9.79e+00 lr: 5.0e-04 58.5%┣┫ 1.5k/2.5k [26:38<18:55, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.00e-01 grad: 9.94e+00 lr: 5.0e-04 58.5%┣┫ 1.5k/2.5k [26:39<18:54, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.28e-01 grad: 2.11e+01 lr: 5.0e-04 58.6%┣┫ 1.5k/2.5k [26:40<18:53, 1s/it]\n",
      "Loss train: 9.46e-02 val: 1.12e-01 grad: 9.49e+00 lr: 5.0e-04 58.6%┣┫ 1.5k/2.5k [26:41<18:52, 1s/it]\n",
      "Loss train: 9.69e-02 val: 1.06e-01 grad: 9.20e+00 lr: 5.0e-04 58.6%┣┫ 1.5k/2.5k [26:42<18:51, 1s/it]\n",
      "Loss train: 8.77e-02 val: 1.08e-01 grad: 1.06e+01 lr: 5.0e-04 58.7%┣┫ 1.5k/2.5k [26:43<18:50, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.17e-01 grad: 1.03e+01 lr: 5.0e-04 58.7%┣┫ 1.5k/2.5k [26:44<18:48, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.13e-01 grad: 9.54e+00 lr: 5.0e-04 58.8%┣┫ 1.5k/2.5k [26:45<18:47, 1s/it]\n",
      "Loss train: 8.18e-02 val: 1.18e-01 grad: 1.06e+01 lr: 5.0e-04 58.8%┣┫ 1.5k/2.5k [26:46<18:46, 1s/it]\n",
      "Loss train: 8.08e-02 val: 1.16e-01 grad: 9.22e+00 lr: 5.0e-04 58.8%┣┫ 1.5k/2.5k [26:47<18:45, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.10e-01 grad: 1.03e+01 lr: 5.0e-04 58.9%┣┫ 1.5k/2.5k [26:49<18:44, 1s/it]\n",
      "Loss train: 7.93e-02 val: 1.11e-01 grad: 9.92e+00 lr: 5.0e-04 58.9%┣┫ 1.5k/2.5k [26:50<18:43, 1s/it]\n",
      "Loss train: 8.11e-02 val: 1.14e-01 grad: 8.46e+00 lr: 5.0e-04 59.0%┣┫ 1.5k/2.5k [26:51<18:42, 1s/it]\n",
      "Loss train: 7.92e-02 val: 1.05e-01 grad: 7.20e+00 lr: 5.0e-04 59.0%┣┫ 1.5k/2.5k [26:52<18:41, 1s/it]\n",
      "Loss train: 8.17e-02 val: 1.02e-01 grad: 9.95e+00 lr: 5.0e-04 59.0%┣┫ 1.5k/2.5k [26:53<18:40, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.09e-01 grad: 1.01e+01 lr: 5.0e-04 59.1%┣┫ 1.5k/2.5k [26:54<18:39, 1s/it]\n",
      "Loss train: 8.20e-02 val: 1.17e-01 grad: 9.61e+00 lr: 5.0e-04 59.1%┣┫ 1.5k/2.5k [26:55<18:38, 1s/it]\n",
      "Loss train: 8.27e-02 val: 9.17e-02 grad: 7.29e+00 lr: 5.0e-04 59.2%┣┫ 1.5k/2.5k [26:56<18:36, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.06e-01 grad: 1.69e+01 lr: 5.0e-04 59.2%┣┫ 1.5k/2.5k [26:57<18:35, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.05e-01 grad: 1.17e+01 lr: 5.0e-04 59.2%┣┫ 1.5k/2.5k [26:59<18:34, 1s/it]\n",
      "Loss train: 9.38e-02 val: 1.11e-01 grad: 1.04e+01 lr: 5.0e-04 59.3%┣┫ 1.5k/2.5k [27:00<18:33, 1s/it]\n",
      "Loss train: 8.80e-02 val: 1.14e-01 grad: 1.06e+01 lr: 5.0e-04 59.3%┣┫ 1.5k/2.5k [27:01<18:32, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.23e-01 grad: 9.64e+00 lr: 5.0e-04 59.4%┣┫ 1.5k/2.5k [27:02<18:31, 1s/it]\n",
      "Loss train: 8.94e-02 val: 1.28e-01 grad: 8.30e+00 lr: 5.0e-04 59.4%┣┫ 1.5k/2.5k [27:03<18:30, 1s/it]\n",
      "Loss train: 8.13e-02 val: 1.14e-01 grad: 8.40e+00 lr: 5.0e-04 59.4%┣┫ 1.5k/2.5k [27:04<18:29, 1s/it]\n",
      "Loss train: 8.02e-02 val: 1.14e-01 grad: 1.03e+01 lr: 5.0e-04 59.5%┣┫ 1.5k/2.5k [27:05<18:28, 1s/it]\n",
      "Loss train: 8.11e-02 val: 1.09e-01 grad: 1.02e+01 lr: 5.0e-04 59.5%┣┫ 1.5k/2.5k [27:06<18:27, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.10e-01 grad: 1.00e+01 lr: 5.0e-04 59.6%┣┫ 1.5k/2.5k [27:08<18:26, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.16e-01 grad: 9.47e+00 lr: 5.0e-04 59.6%┣┫ 1.5k/2.5k [27:09<18:25, 1s/it]\n",
      "Loss train: 8.01e-02 val: 1.20e-01 grad: 7.65e+00 lr: 5.0e-04 59.6%┣┫ 1.5k/2.5k [27:10<18:24, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.12e-01 grad: 7.54e+00 lr: 5.0e-04 59.7%┣┫ 1.5k/2.5k [27:11<18:22, 1s/it]\n",
      "Loss train: 8.32e-02 val: 8.96e-02 grad: 1.01e+01 lr: 5.0e-04 59.7%┣┫ 1.5k/2.5k [27:12<18:21, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.21e-01 grad: 1.92e+01 lr: 5.0e-04 59.8%┣┫ 1.5k/2.5k [27:13<18:20, 1s/it]\n",
      "Loss train: 1.24e-01 val: 1.30e-01 grad: 7.42e+00 lr: 5.0e-04 59.8%┣┫ 1.5k/2.5k [27:14<18:19, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.05e-01 grad: 1.09e+01 lr: 5.0e-04 59.8%┣┫ 1.5k/2.5k [27:15<18:18, 1s/it]\n",
      "Loss train: 1.23e-01 val: 1.33e-01 grad: 1.10e+01 lr: 5.0e-04 59.9%┣┫ 1.5k/2.5k [27:16<18:17, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.20e-01 grad: 1.10e+01 lr: 5.0e-04 59.9%┣┫ 1.5k/2.5k [27:18<18:16, 1s/it]\n",
      "Loss train: 8.37e-02 val: 1.21e-01 grad: 9.07e+00 lr: 5.0e-04 60.0%┣┫ 1.5k/2.5k [27:19<18:15, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.33e-01 grad: 8.79e+00 lr: 2.5e-04 60.0%┣┫ 1.5k/2.5k [27:20<18:14, 1s/it]\n",
      "Loss train: 8.31e-02 val: 1.20e-01 grad: 6.98e+00 lr: 2.5e-04 60.0%┣┫ 1.5k/2.5k [27:21<18:13, 1s/it]\n",
      "Loss train: 8.08e-02 val: 1.09e-01 grad: 9.53e+00 lr: 2.5e-04 60.1%┣┫ 1.5k/2.5k [27:22<18:12, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.08e-01 grad: 9.81e+00 lr: 2.5e-04 60.1%┣┫ 1.5k/2.5k [27:23<18:10, 1s/it]\n",
      "Loss train: 7.96e-02 val: 1.09e-01 grad: 9.70e+00 lr: 2.5e-04 60.2%┣┫ 1.5k/2.5k [27:24<18:09, 1s/it]\n",
      "Loss train: 7.96e-02 val: 1.15e-01 grad: 1.05e+01 lr: 2.5e-04 60.2%┣┫ 1.5k/2.5k [27:25<18:08, 1s/it]\n",
      "Loss train: 7.84e-02 val: 1.13e-01 grad: 8.71e+00 lr: 2.5e-04 60.2%┣┫ 1.5k/2.5k [27:26<18:07, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.08e-01 grad: 1.05e+01 lr: 2.5e-04 60.3%┣┫ 1.5k/2.5k [27:27<18:06, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.09e-01 grad: 1.03e+01 lr: 2.5e-04 60.3%┣┫ 1.5k/2.5k [27:28<18:05, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.08e-01 grad: 1.04e+01 lr: 2.5e-04 60.4%┣┫ 1.5k/2.5k [27:29<18:04, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.10e-01 grad: 1.02e+01 lr: 2.5e-04 60.4%┣┫ 1.5k/2.5k [27:30<18:03, 1s/it]\n",
      "Loss train: 7.71e-02 val: 1.07e-01 grad: 9.13e+00 lr: 2.5e-04 60.4%┣┫ 1.5k/2.5k [27:31<18:02, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.03e-01 grad: 1.02e+01 lr: 2.5e-04 60.5%┣┫ 1.5k/2.5k [27:33<18:01, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.02e-01 grad: 1.01e+01 lr: 2.5e-04 60.5%┣┫ 1.5k/2.5k [27:34<18:00, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.07e-01 grad: 1.40e+01 lr: 2.5e-04 60.6%┣┫ 1.5k/2.5k [27:35<17:59, 1s/it]\n",
      "Loss train: 8.06e-02 val: 1.08e-01 grad: 9.28e+00 lr: 2.5e-04 60.6%┣┫ 1.5k/2.5k [27:36<17:57, 1s/it]\n",
      "Loss train: 8.23e-02 val: 1.10e-01 grad: 8.82e+00 lr: 2.5e-04 60.6%┣┫ 1.5k/2.5k [27:37<17:56, 1s/it]\n",
      "Loss train: 8.11e-02 val: 1.04e-01 grad: 1.05e+01 lr: 2.5e-04 60.7%┣┫ 1.5k/2.5k [27:38<17:55, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.04e-01 grad: 9.59e+00 lr: 2.5e-04 60.7%┣┫ 1.5k/2.5k [27:39<17:54, 1s/it]\n",
      "Loss train: 7.84e-02 val: 1.05e-01 grad: 9.64e+00 lr: 2.5e-04 60.8%┣┫ 1.5k/2.5k [27:40<17:53, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.09e-01 grad: 8.77e+00 lr: 2.5e-04 60.8%┣┫ 1.5k/2.5k [27:41<17:52, 1s/it]\n",
      "Loss train: 7.78e-02 val: 1.06e-01 grad: 8.43e+00 lr: 2.5e-04 60.8%┣┫ 1.5k/2.5k [27:43<17:51, 1s/it]\n",
      "Loss train: 7.88e-02 val: 1.03e-01 grad: 9.58e+00 lr: 2.5e-04 60.9%┣┫ 1.5k/2.5k [27:44<17:50, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.06e-01 grad: 1.00e+01 lr: 2.5e-04 60.9%┣┫ 1.5k/2.5k [27:45<17:49, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.08e-01 grad: 1.02e+01 lr: 2.5e-04 61.0%┣┫ 1.5k/2.5k [27:46<17:47, 1s/it]\n",
      "Loss train: 7.71e-02 val: 1.04e-01 grad: 1.02e+01 lr: 2.5e-04 61.0%┣┫ 1.5k/2.5k [27:47<17:46, 1s/it]\n",
      "Loss train: 7.69e-02 val: 1.04e-01 grad: 1.00e+01 lr: 2.5e-04 61.0%┣┫ 1.5k/2.5k [27:48<17:45, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.08e-01 grad: 9.43e+00 lr: 2.5e-04 61.1%┣┫ 1.5k/2.5k [27:49<17:44, 1s/it]\n",
      "Loss train: 7.70e-02 val: 1.04e-01 grad: 8.31e+00 lr: 2.5e-04 61.1%┣┫ 1.5k/2.5k [27:50<17:43, 1s/it]\n",
      "Loss train: 7.95e-02 val: 9.95e-02 grad: 1.51e+01 lr: 2.5e-04 61.2%┣┫ 1.5k/2.5k [27:51<17:42, 1s/it]\n",
      "Loss train: 8.26e-02 val: 1.07e-01 grad: 9.56e+00 lr: 2.5e-04 61.2%┣┫ 1.5k/2.5k [27:53<17:41, 1s/it]\n",
      "Loss train: 8.48e-02 val: 1.08e-01 grad: 8.25e+00 lr: 2.5e-04 61.2%┣┫ 1.5k/2.5k [27:54<17:40, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.05e-01 grad: 9.35e+00 lr: 2.5e-04 61.3%┣┫ 1.5k/2.5k [27:55<17:39, 1s/it]\n",
      "Loss train: 8.18e-02 val: 1.09e-01 grad: 9.41e+00 lr: 2.5e-04 61.3%┣┫ 1.5k/2.5k [27:56<17:38, 1s/it]\n",
      "Loss train: 7.94e-02 val: 1.07e-01 grad: 9.31e+00 lr: 2.5e-04 61.4%┣┫ 1.5k/2.5k [27:57<17:37, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.08e-01 grad: 9.22e+00 lr: 2.5e-04 61.4%┣┫ 1.5k/2.5k [27:58<17:36, 1s/it]\n",
      "Loss train: 8.10e-02 val: 1.01e-01 grad: 1.04e+01 lr: 2.5e-04 61.4%┣┫ 1.5k/2.5k [27:59<17:35, 1s/it]\n",
      "Loss train: 8.02e-02 val: 1.01e-01 grad: 1.04e+01 lr: 2.5e-04 61.5%┣┫ 1.5k/2.5k [28:00<17:34, 1s/it]\n",
      "Loss train: 7.97e-02 val: 1.09e-01 grad: 1.05e+01 lr: 2.5e-04 61.5%┣┫ 1.5k/2.5k [28:01<17:32, 1s/it]\n",
      "Loss train: 7.91e-02 val: 1.09e-01 grad: 8.34e+00 lr: 2.5e-04 61.6%┣┫ 1.5k/2.5k [28:02<17:31, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.06e-01 grad: 8.30e+00 lr: 2.5e-04 61.6%┣┫ 1.5k/2.5k [28:04<17:30, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.05e-01 grad: 1.00e+01 lr: 2.5e-04 61.6%┣┫ 1.5k/2.5k [28:05<17:29, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.07e-01 grad: 9.82e+00 lr: 2.5e-04 61.7%┣┫ 1.5k/2.5k [28:06<17:28, 1s/it]\n",
      "Loss train: 7.70e-02 val: 1.07e-01 grad: 9.60e+00 lr: 2.5e-04 61.7%┣┫ 1.5k/2.5k [28:07<17:27, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.03e-01 grad: 9.40e+00 lr: 2.5e-04 61.8%┣┫ 1.5k/2.5k [28:08<17:26, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.07e-01 grad: 1.02e+01 lr: 2.5e-04 61.8%┣┫ 1.5k/2.5k [28:09<17:25, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.06e-01 grad: 1.02e+01 lr: 2.5e-04 61.8%┣┫ 1.5k/2.5k [28:10<17:24, 1s/it]\n",
      "Loss train: 7.70e-02 val: 1.07e-01 grad: 9.43e+00 lr: 2.5e-04 61.9%┣┫ 1.5k/2.5k [28:11<17:23, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.06e-01 grad: 8.59e+00 lr: 2.5e-04 61.9%┣┫ 1.5k/2.5k [28:13<17:22, 1s/it]\n",
      "Loss train: 7.79e-02 val: 9.46e-02 grad: 9.29e+00 lr: 2.5e-04 62.0%┣┫ 1.5k/2.5k [28:14<17:20, 1s/it]\n",
      "Loss train: 8.20e-02 val: 1.10e-01 grad: 1.53e+01 lr: 2.5e-04 62.0%┣┫ 1.6k/2.5k [28:15<17:19, 1s/it]\n",
      "Loss train: 8.53e-02 val: 1.06e-01 grad: 8.58e+00 lr: 2.5e-04 62.0%┣┫ 1.6k/2.5k [28:16<17:18, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.03e-01 grad: 9.50e+00 lr: 2.5e-04 62.1%┣┫ 1.6k/2.5k [28:17<17:17, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.01e-01 grad: 1.04e+01 lr: 2.5e-04 62.1%┣┫ 1.6k/2.5k [28:18<17:16, 1s/it]\n",
      "Loss train: 7.99e-02 val: 1.07e-01 grad: 9.86e+00 lr: 2.5e-04 62.2%┣┫ 1.6k/2.5k [28:19<17:15, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.11e-01 grad: 9.64e+00 lr: 2.5e-04 62.2%┣┫ 1.6k/2.5k [28:20<17:14, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.07e-01 grad: 1.01e+01 lr: 2.5e-04 62.2%┣┫ 1.6k/2.5k [28:21<17:13, 1s/it]\n",
      "Loss train: 7.80e-02 val: 1.06e-01 grad: 1.02e+01 lr: 2.5e-04 62.3%┣┫ 1.6k/2.5k [28:23<17:12, 1s/it]\n",
      "Loss train: 7.78e-02 val: 1.06e-01 grad: 1.01e+01 lr: 2.5e-04 62.3%┣┫ 1.6k/2.5k [28:24<17:11, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.05e-01 grad: 9.22e+00 lr: 2.5e-04 62.4%┣┫ 1.6k/2.5k [28:25<17:10, 1s/it]\n",
      "Loss train: 7.78e-02 val: 1.07e-01 grad: 9.65e+00 lr: 2.5e-04 62.4%┣┫ 1.6k/2.5k [28:26<17:09, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.06e-01 grad: 8.45e+00 lr: 2.5e-04 62.4%┣┫ 1.6k/2.5k [28:27<17:07, 1s/it]\n",
      "Loss train: 7.71e-02 val: 1.04e-01 grad: 1.03e+01 lr: 2.5e-04 62.5%┣┫ 1.6k/2.5k [28:28<17:06, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.02e-01 grad: 9.57e+00 lr: 2.5e-04 62.5%┣┫ 1.6k/2.5k [28:29<17:05, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.01e-01 grad: 9.84e+00 lr: 2.5e-04 62.6%┣┫ 1.6k/2.5k [28:30<17:04, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.08e-01 grad: 1.03e+01 lr: 2.5e-04 62.6%┣┫ 1.6k/2.5k [28:31<17:03, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.04e-01 grad: 8.64e+00 lr: 2.5e-04 62.6%┣┫ 1.6k/2.5k [28:32<17:02, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.06e-01 grad: 8.86e+00 lr: 2.5e-04 62.7%┣┫ 1.6k/2.5k [28:33<17:01, 1s/it]\n",
      "Loss train: 7.86e-02 val: 9.85e-02 grad: 9.10e+00 lr: 2.5e-04 62.7%┣┫ 1.6k/2.5k [28:35<17:00, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.01e-01 grad: 9.55e+00 lr: 2.5e-04 62.8%┣┫ 1.6k/2.5k [28:36<16:59, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.03e-01 grad: 9.48e+00 lr: 2.5e-04 62.8%┣┫ 1.6k/2.5k [28:37<16:58, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.07e-01 grad: 9.98e+00 lr: 2.5e-04 62.8%┣┫ 1.6k/2.5k [28:38<16:57, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.08e-01 grad: 8.89e+00 lr: 2.5e-04 62.9%┣┫ 1.6k/2.5k [28:39<16:55, 1s/it]\n",
      "Loss train: 7.70e-02 val: 9.82e-02 grad: 8.00e+00 lr: 2.5e-04 62.9%┣┫ 1.6k/2.5k [28:40<16:54, 1s/it]\n",
      "Loss train: 7.86e-02 val: 1.04e-01 grad: 1.16e+01 lr: 2.5e-04 63.0%┣┫ 1.6k/2.5k [28:41<16:53, 1s/it]\n",
      "Loss train: 8.01e-02 val: 1.09e-01 grad: 1.00e+01 lr: 2.5e-04 63.0%┣┫ 1.6k/2.5k [28:42<16:52, 1s/it]\n",
      "Loss train: 7.84e-02 val: 1.08e-01 grad: 9.14e+00 lr: 2.5e-04 63.0%┣┫ 1.6k/2.5k [28:43<16:51, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.09e-01 grad: 9.37e+00 lr: 2.5e-04 63.1%┣┫ 1.6k/2.5k [28:44<16:50, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.05e-01 grad: 8.39e+00 lr: 2.5e-04 63.1%┣┫ 1.6k/2.5k [28:45<16:49, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.02e-01 grad: 9.87e+00 lr: 2.5e-04 63.2%┣┫ 1.6k/2.5k [28:47<16:48, 1s/it]\n",
      "Loss train: 7.80e-02 val: 1.01e-01 grad: 9.55e+00 lr: 2.5e-04 63.2%┣┫ 1.6k/2.5k [28:48<16:47, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.06e-01 grad: 9.65e+00 lr: 2.5e-04 63.2%┣┫ 1.6k/2.5k [28:49<16:46, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.04e-01 grad: 1.03e+01 lr: 2.5e-04 63.3%┣┫ 1.6k/2.5k [28:50<16:44, 1s/it]\n",
      "Loss train: 7.60e-02 val: 1.05e-01 grad: 9.12e+00 lr: 2.5e-04 63.3%┣┫ 1.6k/2.5k [28:51<16:43, 1s/it]\n",
      "Loss train: 7.62e-02 val: 9.96e-02 grad: 1.04e+01 lr: 2.5e-04 63.4%┣┫ 1.6k/2.5k [28:52<16:42, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.04e-01 grad: 1.19e+01 lr: 2.5e-04 63.4%┣┫ 1.6k/2.5k [28:53<16:41, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.03e-01 grad: 8.73e+00 lr: 2.5e-04 63.4%┣┫ 1.6k/2.5k [28:55<16:40, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.03e-01 grad: 9.87e+00 lr: 2.5e-04 63.5%┣┫ 1.6k/2.5k [28:56<16:39, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.02e-01 grad: 9.58e+00 lr: 2.5e-04 63.5%┣┫ 1.6k/2.5k [28:57<16:38, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.04e-01 grad: 1.04e+01 lr: 2.5e-04 63.6%┣┫ 1.6k/2.5k [28:58<16:37, 1s/it]\n",
      "Loss train: 7.64e-02 val: 1.01e-01 grad: 9.77e+00 lr: 2.5e-04 63.6%┣┫ 1.6k/2.5k [28:59<16:36, 1s/it]\n",
      "Loss train: 7.60e-02 val: 1.01e-01 grad: 9.42e+00 lr: 2.5e-04 63.6%┣┫ 1.6k/2.5k [29:00<16:35, 1s/it]\n",
      "Loss train: 8.12e-02 val: 1.08e-01 grad: 8.25e+00 lr: 2.5e-04 63.7%┣┫ 1.6k/2.5k [29:01<16:34, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.05e-01 grad: 8.23e+00 lr: 2.5e-04 63.7%┣┫ 1.6k/2.5k [29:02<16:33, 1s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.3   0.0   0.0   0.0   0.0  0.0   162.2   0.06  15.4   -0.3    0.02  -0.0    0.27  0.01  0.0\n",
      "  0.11  0.0   0.0   0.35  0.0  0.01  139.5   0.2   18.89  -0.11   0.26   0.01  -0.35  0.18  0.0\n",
      "  0.69  0.17  0.03  0.0   0.0  0.11  145.31  0.41  24.28  -0.69  -0.17  -0.03   0.61  0.28  0.0\n",
      " -0.0   0.44  0.25  0.0   0.0  0.07  124.81  0.2   18.77   0.0   -0.44  -0.25   0.47  0.22  0.0\n",
      " -0.0   0.0   0.0   0.36  0.0  0.15  182.26  0.02  15.33   0.0   -0.0    0.28  -0.36  0.08  0.0\n",
      "  0.05  0.06  0.03  0.0   0.0  0.17  174.52  0.05  13.31  -0.05  -0.06  -0.03   0.14  0.0   0.0\n",
      " -0.0   0.0   0.0   0.76  0.0  0.32  102.36  0.27  20.23   0.0    0.16   0.27  -0.76  0.33  0.0\n",
      "  0.08  0.0   0.0   0.68  0.0  0.0   116.1   0.3   19.98  -0.08   0.23   0.09  -0.68  0.43  0.0\n",
      "\n",
      "Min Loss train: 7.60e-02 val: 8.69e-02\n",
      " update plot 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 7.99e-02 val: 8.69e-02 grad: 1.03e+01 lr: 2.5e-04 63.8%┣┫ 1.6k/2.5k [29:03<16:32, 1s/it]\n",
      "Loss train: 8.25e-02 val: 1.01e-01 grad: 1.54e+01 lr: 2.5e-04 63.8%┣┫ 1.6k/2.5k [29:05<16:31, 1s/it]\n",
      "Loss train: 8.92e-02 val: 1.08e-01 grad: 8.50e+00 lr: 2.5e-04 63.8%┣┫ 1.6k/2.5k [29:06<16:29, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.05e-01 grad: 8.55e+00 lr: 2.5e-04 63.9%┣┫ 1.6k/2.5k [29:07<16:28, 1s/it]\n",
      "Loss train: 8.35e-02 val: 1.06e-01 grad: 9.24e+00 lr: 2.5e-04 63.9%┣┫ 1.6k/2.5k [29:08<16:27, 1s/it]\n",
      "Loss train: 8.22e-02 val: 1.03e-01 grad: 1.01e+01 lr: 2.5e-04 64.0%┣┫ 1.6k/2.5k [29:09<16:26, 1s/it]\n",
      "Loss train: 7.94e-02 val: 1.06e-01 grad: 9.80e+00 lr: 2.5e-04 64.0%┣┫ 1.6k/2.5k [29:10<16:25, 1s/it]\n",
      "Loss train: 8.10e-02 val: 1.10e-01 grad: 1.01e+01 lr: 2.5e-04 64.0%┣┫ 1.6k/2.5k [29:11<16:24, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.05e-01 grad: 8.31e+00 lr: 2.5e-04 64.1%┣┫ 1.6k/2.5k [29:12<16:23, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.03e-01 grad: 9.66e+00 lr: 2.5e-04 64.1%┣┫ 1.6k/2.5k [29:13<16:22, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.05e-01 grad: 9.54e+00 lr: 2.5e-04 64.2%┣┫ 1.6k/2.5k [29:14<16:21, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.04e-01 grad: 9.57e+00 lr: 2.5e-04 64.2%┣┫ 1.6k/2.5k [29:15<16:19, 1s/it]\n",
      "Loss train: 7.80e-02 val: 1.09e-01 grad: 7.63e+00 lr: 2.5e-04 64.2%┣┫ 1.6k/2.5k [29:16<16:18, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.08e-01 grad: 8.24e+00 lr: 2.5e-04 64.3%┣┫ 1.6k/2.5k [29:17<16:17, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.06e-01 grad: 1.02e+01 lr: 2.5e-04 64.3%┣┫ 1.6k/2.5k [29:19<16:16, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.07e-01 grad: 1.02e+01 lr: 2.5e-04 64.4%┣┫ 1.6k/2.5k [29:20<16:15, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.03e-01 grad: 9.47e+00 lr: 2.5e-04 64.4%┣┫ 1.6k/2.5k [29:21<16:14, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.10e-01 grad: 1.02e+01 lr: 2.5e-04 64.4%┣┫ 1.6k/2.5k [29:22<16:13, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.02e-01 grad: 1.03e+01 lr: 2.5e-04 64.5%┣┫ 1.6k/2.5k [29:23<16:12, 1s/it]\n",
      "Loss train: 7.86e-02 val: 1.07e-01 grad: 1.05e+01 lr: 2.5e-04 64.5%┣┫ 1.6k/2.5k [29:24<16:11, 1s/it]\n",
      "Loss train: 7.56e-02 val: 1.03e-01 grad: 7.97e+00 lr: 2.5e-04 64.6%┣┫ 1.6k/2.5k [29:25<16:10, 1s/it]\n",
      "Loss train: 7.69e-02 val: 9.12e-02 grad: 9.49e+00 lr: 2.5e-04 64.6%┣┫ 1.6k/2.5k [29:26<16:09, 1s/it]\n",
      "Loss train: 7.73e-02 val: 9.85e-02 grad: 1.60e+01 lr: 2.5e-04 64.6%┣┫ 1.6k/2.5k [29:27<16:07, 1s/it]\n",
      "Loss train: 9.00e-02 val: 1.02e-01 grad: 9.92e+00 lr: 2.5e-04 64.7%┣┫ 1.6k/2.5k [29:28<16:06, 1s/it]\n",
      "Loss train: 1.00e-01 val: 1.04e-01 grad: 1.00e+01 lr: 2.5e-04 64.7%┣┫ 1.6k/2.5k [29:30<16:05, 1s/it]\n",
      "Loss train: 8.69e-02 val: 1.02e-01 grad: 7.41e+00 lr: 2.5e-04 64.8%┣┫ 1.6k/2.5k [29:31<16:04, 1s/it]\n",
      "Loss train: 8.63e-02 val: 1.02e-01 grad: 1.02e+01 lr: 2.5e-04 64.8%┣┫ 1.6k/2.5k [29:32<16:03, 1s/it]\n",
      "Loss train: 7.96e-02 val: 1.09e-01 grad: 9.81e+00 lr: 2.5e-04 64.8%┣┫ 1.6k/2.5k [29:33<16:02, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.14e-01 grad: 1.00e+01 lr: 2.5e-04 64.9%┣┫ 1.6k/2.5k [29:34<16:01, 1s/it]\n",
      "Loss train: 7.86e-02 val: 1.07e-01 grad: 1.01e+01 lr: 2.5e-04 64.9%┣┫ 1.6k/2.5k [29:35<16:00, 1s/it]\n",
      "Loss train: 7.78e-02 val: 1.07e-01 grad: 9.50e+00 lr: 2.5e-04 65.0%┣┫ 1.6k/2.5k [29:36<15:59, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.06e-01 grad: 9.89e+00 lr: 2.5e-04 65.0%┣┫ 1.6k/2.5k [29:37<15:58, 1s/it]\n",
      "Loss train: 8.02e-02 val: 1.12e-01 grad: 1.02e+01 lr: 2.5e-04 65.0%┣┫ 1.6k/2.5k [29:38<15:56, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.06e-01 grad: 8.60e+00 lr: 2.5e-04 65.1%┣┫ 1.6k/2.5k [29:39<15:55, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.06e-01 grad: 1.01e+01 lr: 2.5e-04 65.1%┣┫ 1.6k/2.5k [29:41<15:54, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.04e-01 grad: 9.33e+00 lr: 2.5e-04 65.2%┣┫ 1.6k/2.5k [29:42<15:53, 1s/it]\n",
      "Loss train: 7.92e-02 val: 1.11e-01 grad: 1.05e+01 lr: 2.5e-04 65.2%┣┫ 1.6k/2.5k [29:43<15:52, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.04e-01 grad: 8.46e+00 lr: 2.5e-04 65.2%┣┫ 1.6k/2.5k [29:44<15:51, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.01e-01 grad: 9.65e+00 lr: 2.5e-04 65.3%┣┫ 1.6k/2.5k [29:45<15:50, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.01e-01 grad: 1.03e+01 lr: 2.5e-04 65.3%┣┫ 1.6k/2.5k [29:46<15:49, 1s/it]\n",
      "Loss train: 7.58e-02 val: 1.09e-01 grad: 9.82e+00 lr: 2.5e-04 65.4%┣┫ 1.6k/2.5k [29:47<15:48, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.12e-01 grad: 7.92e+00 lr: 2.5e-04 65.4%┣┫ 1.6k/2.5k [29:48<15:47, 1s/it]\n",
      "Loss train: 7.54e-02 val: 1.05e-01 grad: 9.93e+00 lr: 2.5e-04 65.4%┣┫ 1.6k/2.5k [29:49<15:46, 1s/it]\n",
      "Loss train: 7.56e-02 val: 1.06e-01 grad: 1.38e+01 lr: 2.5e-04 65.5%┣┫ 1.6k/2.5k [29:50<15:44, 1s/it]\n",
      "Loss train: 8.43e-02 val: 1.11e-01 grad: 8.45e+00 lr: 2.5e-04 65.5%┣┫ 1.6k/2.5k [29:52<15:43, 1s/it]\n",
      "Loss train: 8.26e-02 val: 1.03e-01 grad: 7.48e+00 lr: 2.5e-04 65.6%┣┫ 1.6k/2.5k [29:53<15:42, 1s/it]\n",
      "Loss train: 8.13e-02 val: 1.02e-01 grad: 9.34e+00 lr: 2.5e-04 65.6%┣┫ 1.6k/2.5k [29:54<15:41, 1s/it]\n",
      "Loss train: 7.93e-02 val: 1.05e-01 grad: 9.90e+00 lr: 2.5e-04 65.6%┣┫ 1.6k/2.5k [29:55<15:40, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.08e-01 grad: 9.39e+00 lr: 2.5e-04 65.7%┣┫ 1.6k/2.5k [29:56<15:39, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.12e-01 grad: 9.52e+00 lr: 2.5e-04 65.7%┣┫ 1.6k/2.5k [29:57<15:38, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.09e-01 grad: 7.91e+00 lr: 2.5e-04 65.8%┣┫ 1.6k/2.5k [29:58<15:37, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.06e-01 grad: 9.96e+00 lr: 2.5e-04 65.8%┣┫ 1.6k/2.5k [29:59<15:36, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.07e-01 grad: 1.01e+01 lr: 2.5e-04 65.8%┣┫ 1.6k/2.5k [30:00<15:35, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.00e-01 grad: 1.02e+01 lr: 2.5e-04 65.9%┣┫ 1.6k/2.5k [30:02<15:34, 1s/it]\n",
      "Loss train: 7.75e-02 val: 9.97e-02 grad: 1.04e+01 lr: 2.5e-04 65.9%┣┫ 1.6k/2.5k [30:03<15:33, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.07e-01 grad: 1.02e+01 lr: 2.5e-04 66.0%┣┫ 1.6k/2.5k [30:04<15:31, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.07e-01 grad: 8.00e+00 lr: 2.5e-04 66.0%┣┫ 1.6k/2.5k [30:05<15:30, 1s/it]\n",
      "Loss train: 7.82e-02 val: 9.91e-02 grad: 1.07e+01 lr: 2.5e-04 66.0%┣┫ 1.7k/2.5k [30:06<15:29, 1s/it]\n",
      "Loss train: 7.53e-02 val: 1.04e-01 grad: 9.87e+00 lr: 2.5e-04 66.1%┣┫ 1.7k/2.5k [30:07<15:28, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.08e-01 grad: 9.45e+00 lr: 2.5e-04 66.1%┣┫ 1.7k/2.5k [30:08<15:27, 1s/it]\n",
      "Loss train: 7.75e-02 val: 9.85e-02 grad: 8.06e+00 lr: 2.5e-04 66.2%┣┫ 1.7k/2.5k [30:09<15:26, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.06e-01 grad: 1.81e+01 lr: 2.5e-04 66.2%┣┫ 1.7k/2.5k [30:10<15:25, 1s/it]\n",
      "Loss train: 9.25e-02 val: 1.01e-01 grad: 9.18e+00 lr: 2.5e-04 66.2%┣┫ 1.7k/2.5k [30:11<15:24, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.04e-01 grad: 9.82e+00 lr: 2.5e-04 66.3%┣┫ 1.7k/2.5k [30:12<15:23, 1s/it]\n",
      "Loss train: 8.87e-02 val: 1.00e-01 grad: 8.61e+00 lr: 2.5e-04 66.3%┣┫ 1.7k/2.5k [30:13<15:21, 1s/it]\n",
      "Loss train: 8.03e-02 val: 1.07e-01 grad: 9.49e+00 lr: 2.5e-04 66.4%┣┫ 1.7k/2.5k [30:15<15:20, 1s/it]\n",
      "Loss train: 7.86e-02 val: 1.14e-01 grad: 1.00e+01 lr: 2.5e-04 66.4%┣┫ 1.7k/2.5k [30:16<15:19, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.08e-01 grad: 9.60e+00 lr: 2.5e-04 66.4%┣┫ 1.7k/2.5k [30:17<15:18, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.12e-01 grad: 1.03e+01 lr: 2.5e-04 66.5%┣┫ 1.7k/2.5k [30:18<15:17, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.10e-01 grad: 9.05e+00 lr: 2.5e-04 66.5%┣┫ 1.7k/2.5k [30:19<15:16, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.05e-01 grad: 1.01e+01 lr: 2.5e-04 66.6%┣┫ 1.7k/2.5k [30:20<15:15, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.06e-01 grad: 9.83e+00 lr: 2.5e-04 66.6%┣┫ 1.7k/2.5k [30:21<15:14, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.08e-01 grad: 1.01e+01 lr: 2.5e-04 66.6%┣┫ 1.7k/2.5k [30:22<15:13, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.03e-01 grad: 8.70e+00 lr: 2.5e-04 66.7%┣┫ 1.7k/2.5k [30:23<15:12, 1s/it]\n",
      "Loss train: 8.00e-02 val: 9.81e-02 grad: 9.82e+00 lr: 2.5e-04 66.7%┣┫ 1.7k/2.5k [30:24<15:10, 1s/it]\n",
      "Loss train: 7.81e-02 val: 1.09e-01 grad: 9.98e+00 lr: 2.5e-04 66.8%┣┫ 1.7k/2.5k [30:25<15:09, 1s/it]\n",
      "Loss train: 7.60e-02 val: 1.09e-01 grad: 7.89e+00 lr: 2.5e-04 66.8%┣┫ 1.7k/2.5k [30:26<15:08, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.00e-01 grad: 8.21e+00 lr: 2.5e-04 66.8%┣┫ 1.7k/2.5k [30:27<15:07, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.05e-01 grad: 1.25e+01 lr: 2.5e-04 66.9%┣┫ 1.7k/2.5k [30:29<15:06, 1s/it]\n",
      "Loss train: 8.27e-02 val: 1.04e-01 grad: 9.03e+00 lr: 2.5e-04 66.9%┣┫ 1.7k/2.5k [30:30<15:05, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.09e-01 grad: 8.84e+00 lr: 2.5e-04 67.0%┣┫ 1.7k/2.5k [30:31<15:04, 1s/it]\n",
      "Loss train: 7.87e-02 val: 1.06e-01 grad: 8.60e+00 lr: 2.5e-04 67.0%┣┫ 1.7k/2.5k [30:32<15:03, 1s/it]\n",
      "Loss train: 7.70e-02 val: 1.06e-01 grad: 9.77e+00 lr: 2.5e-04 67.0%┣┫ 1.7k/2.5k [30:33<15:02, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.04e-01 grad: 9.86e+00 lr: 2.5e-04 67.1%┣┫ 1.7k/2.5k [30:34<15:01, 1s/it]\n",
      "Loss train: 7.60e-02 val: 1.06e-01 grad: 9.36e+00 lr: 2.5e-04 67.1%┣┫ 1.7k/2.5k [30:35<14:59, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.07e-01 grad: 9.34e+00 lr: 2.5e-04 67.2%┣┫ 1.7k/2.5k [30:36<14:58, 1s/it]\n",
      "Loss train: 7.58e-02 val: 1.04e-01 grad: 8.59e+00 lr: 2.5e-04 67.2%┣┫ 1.7k/2.5k [30:37<14:57, 1s/it]\n",
      "Loss train: 7.58e-02 val: 1.07e-01 grad: 9.90e+00 lr: 2.5e-04 67.2%┣┫ 1.7k/2.5k [30:38<14:56, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.00e-01 grad: 1.05e+01 lr: 2.5e-04 67.3%┣┫ 1.7k/2.5k [30:39<14:55, 1s/it]\n",
      "Loss train: 7.52e-02 val: 1.05e-01 grad: 1.05e+01 lr: 2.5e-04 67.3%┣┫ 1.7k/2.5k [30:41<14:54, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.06e-01 grad: 1.02e+01 lr: 2.5e-04 67.4%┣┫ 1.7k/2.5k [30:42<14:53, 1s/it]\n",
      "Loss train: 7.46e-02 val: 1.02e-01 grad: 1.03e+01 lr: 2.5e-04 67.4%┣┫ 1.7k/2.5k [30:43<14:52, 1s/it]\n",
      "Loss train: 7.51e-02 val: 1.02e-01 grad: 9.73e+00 lr: 2.5e-04 67.4%┣┫ 1.7k/2.5k [30:44<14:51, 1s/it]\n",
      "Loss train: 7.43e-02 val: 9.95e-02 grad: 9.88e+00 lr: 2.5e-04 67.5%┣┫ 1.7k/2.5k [30:45<14:50, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.04e-01 grad: 8.02e+00 lr: 2.5e-04 67.5%┣┫ 1.7k/2.5k [30:46<14:49, 1s/it]\n",
      "Loss train: 7.66e-02 val: 9.62e-02 grad: 1.05e+01 lr: 2.5e-04 67.6%┣┫ 1.7k/2.5k [30:47<14:47, 1s/it]\n",
      "Loss train: 7.52e-02 val: 1.01e-01 grad: 9.55e+00 lr: 2.5e-04 67.6%┣┫ 1.7k/2.5k [30:48<14:46, 1s/it]\n",
      "Loss train: 7.45e-02 val: 1.00e-01 grad: 8.26e+00 lr: 2.5e-04 67.6%┣┫ 1.7k/2.5k [30:49<14:45, 1s/it]\n",
      "Loss train: 7.46e-02 val: 9.97e-02 grad: 8.16e+00 lr: 2.5e-04 67.7%┣┫ 1.7k/2.5k [30:50<14:44, 1s/it]\n",
      "Loss train: 7.52e-02 val: 9.68e-02 grad: 9.38e+00 lr: 2.5e-04 67.7%┣┫ 1.7k/2.5k [30:51<14:43, 1s/it]\n",
      "Loss train: 8.35e-02 val: 1.09e-01 grad: 1.29e+01 lr: 2.5e-04 67.8%┣┫ 1.7k/2.5k [30:53<14:42, 1s/it]\n",
      "Loss train: 7.80e-02 val: 1.00e-01 grad: 8.28e+00 lr: 2.5e-04 67.8%┣┫ 1.7k/2.5k [30:54<14:41, 1s/it]\n",
      "Loss train: 7.78e-02 val: 9.84e-02 grad: 9.33e+00 lr: 2.5e-04 67.8%┣┫ 1.7k/2.5k [30:55<14:40, 1s/it]\n",
      "Loss train: 7.73e-02 val: 9.87e-02 grad: 9.64e+00 lr: 2.5e-04 67.9%┣┫ 1.7k/2.5k [30:56<14:39, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.05e-01 grad: 1.01e+01 lr: 2.5e-04 67.9%┣┫ 1.7k/2.5k [30:57<14:38, 1s/it]\n",
      "Loss train: 7.58e-02 val: 1.06e-01 grad: 8.37e+00 lr: 2.5e-04 68.0%┣┫ 1.7k/2.5k [30:58<14:37, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.01e-01 grad: 8.09e+00 lr: 2.5e-04 68.0%┣┫ 1.7k/2.5k [30:59<14:35, 1s/it]\n",
      "Loss train: 7.56e-02 val: 1.02e-01 grad: 9.33e+00 lr: 2.5e-04 68.0%┣┫ 1.7k/2.5k [31:00<14:34, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.11e-01 grad: 9.98e+00 lr: 2.5e-04 68.1%┣┫ 1.7k/2.5k [31:01<14:33, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.02e-01 grad: 8.59e+00 lr: 2.5e-04 68.1%┣┫ 1.7k/2.5k [31:03<14:32, 1s/it]\n",
      "Loss train: 7.81e-02 val: 9.91e-02 grad: 1.04e+01 lr: 2.5e-04 68.2%┣┫ 1.7k/2.5k [31:04<14:31, 1s/it]\n",
      "Loss train: 7.46e-02 val: 1.07e-01 grad: 1.03e+01 lr: 2.5e-04 68.2%┣┫ 1.7k/2.5k [31:05<14:30, 1s/it]\n",
      "Loss train: 7.53e-02 val: 1.09e-01 grad: 8.07e+00 lr: 2.5e-04 68.2%┣┫ 1.7k/2.5k [31:06<14:29, 1s/it]\n",
      "Loss train: 7.64e-02 val: 9.32e-02 grad: 9.61e+00 lr: 2.5e-04 68.3%┣┫ 1.7k/2.5k [31:07<14:28, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.02e-01 grad: 1.29e+01 lr: 2.5e-04 68.3%┣┫ 1.7k/2.5k [31:08<14:27, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.02e-01 grad: 9.25e+00 lr: 2.5e-04 68.4%┣┫ 1.7k/2.5k [31:09<14:26, 1s/it]\n",
      "Loss train: 7.92e-02 val: 1.05e-01 grad: 9.02e+00 lr: 2.5e-04 68.4%┣┫ 1.7k/2.5k [31:10<14:25, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.05e-01 grad: 8.88e+00 lr: 2.5e-04 68.4%┣┫ 1.7k/2.5k [31:11<14:23, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.04e-01 grad: 1.01e+01 lr: 2.5e-04 68.5%┣┫ 1.7k/2.5k [31:12<14:22, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.09e-01 grad: 9.09e+00 lr: 2.5e-04 68.5%┣┫ 1.7k/2.5k [31:14<14:21, 1s/it]\n",
      "Loss train: 7.57e-02 val: 1.02e-01 grad: 7.30e+00 lr: 2.5e-04 68.6%┣┫ 1.7k/2.5k [31:15<14:20, 1s/it]\n",
      "Loss train: 8.23e-02 val: 9.85e-02 grad: 1.02e+01 lr: 2.5e-04 68.6%┣┫ 1.7k/2.5k [31:16<14:19, 1s/it]\n",
      "Loss train: 7.50e-02 val: 1.04e-01 grad: 1.04e+01 lr: 2.5e-04 68.6%┣┫ 1.7k/2.5k [31:17<14:18, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.05e-01 grad: 9.92e+00 lr: 2.5e-04 68.7%┣┫ 1.7k/2.5k [31:18<14:17, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.09e-01 grad: 8.05e+00 lr: 2.5e-04 68.7%┣┫ 1.7k/2.5k [31:19<14:16, 1s/it]\n",
      "Loss train: 7.53e-02 val: 1.03e-01 grad: 9.89e+00 lr: 2.5e-04 68.8%┣┫ 1.7k/2.5k [31:20<14:15, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.03e-01 grad: 9.25e+00 lr: 2.5e-04 68.8%┣┫ 1.7k/2.5k [31:21<14:14, 1s/it]\n",
      "Loss train: 7.41e-02 val: 1.04e-01 grad: 1.05e+01 lr: 2.5e-04 68.8%┣┫ 1.7k/2.5k [31:22<14:12, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.03e-01 grad: 1.01e+01 lr: 2.5e-04 68.9%┣┫ 1.7k/2.5k [31:23<14:11, 1s/it]\n",
      "Loss train: 7.37e-02 val: 1.00e-01 grad: 9.00e+00 lr: 2.5e-04 68.9%┣┫ 1.7k/2.5k [31:24<14:10, 1s/it]\n",
      "Loss train: 7.41e-02 val: 1.04e-01 grad: 1.02e+01 lr: 2.5e-04 69.0%┣┫ 1.7k/2.5k [31:26<14:09, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.03e-01 grad: 8.35e+00 lr: 2.5e-04 69.0%┣┫ 1.7k/2.5k [31:27<14:08, 1s/it]\n",
      "Loss train: 7.39e-02 val: 1.03e-01 grad: 1.02e+01 lr: 2.5e-04 69.0%┣┫ 1.7k/2.5k [31:28<14:07, 1s/it]\n",
      "Loss train: 7.69e-02 val: 9.72e-02 grad: 1.02e+01 lr: 2.5e-04 69.1%┣┫ 1.7k/2.5k [31:29<14:06, 1s/it]\n",
      "Loss train: 7.47e-02 val: 9.74e-02 grad: 9.92e+00 lr: 2.5e-04 69.1%┣┫ 1.7k/2.5k [31:30<14:05, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.05e-01 grad: 1.15e+01 lr: 2.5e-04 69.2%┣┫ 1.7k/2.5k [31:31<14:04, 1s/it]\n",
      "Loss train: 7.92e-02 val: 1.04e-01 grad: 8.19e+00 lr: 2.5e-04 69.2%┣┫ 1.7k/2.5k [31:32<14:03, 1s/it]\n",
      "Loss train: 7.78e-02 val: 9.92e-02 grad: 9.26e+00 lr: 2.5e-04 69.2%┣┫ 1.7k/2.5k [31:33<14:02, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.07e-01 grad: 9.58e+00 lr: 2.5e-04 69.3%┣┫ 1.7k/2.5k [31:34<14:00, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.10e-01 grad: 9.83e+00 lr: 2.5e-04 69.3%┣┫ 1.7k/2.5k [31:35<13:59, 1s/it]\n",
      "Loss train: 7.51e-02 val: 1.02e-01 grad: 8.12e+00 lr: 2.5e-04 69.4%┣┫ 1.7k/2.5k [31:37<13:58, 1s/it]\n",
      "Loss train: 7.51e-02 val: 1.01e-01 grad: 9.70e+00 lr: 2.5e-04 69.4%┣┫ 1.7k/2.5k [31:38<13:57, 1s/it]\n",
      "Loss train: 7.54e-02 val: 1.07e-01 grad: 9.14e+00 lr: 2.5e-04 69.4%┣┫ 1.7k/2.5k [31:39<13:56, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.04e-01 grad: 8.76e+00 lr: 2.5e-04 69.5%┣┫ 1.7k/2.5k [31:40<13:55, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.02e-01 grad: 9.70e+00 lr: 2.5e-04 69.5%┣┫ 1.7k/2.5k [31:41<13:54, 1s/it]\n",
      "Loss train: 7.41e-02 val: 1.06e-01 grad: 9.13e+00 lr: 2.5e-04 69.6%┣┫ 1.7k/2.5k [31:42<13:53, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.07e-01 grad: 8.71e+00 lr: 2.5e-04 69.6%┣┫ 1.7k/2.5k [31:43<13:52, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.01e-01 grad: 7.55e+00 lr: 2.5e-04 69.6%┣┫ 1.7k/2.5k [31:44<13:51, 1s/it]\n",
      "Loss train: 7.50e-02 val: 1.01e-01 grad: 9.26e+00 lr: 2.5e-04 69.7%┣┫ 1.7k/2.5k [31:45<13:50, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.01e-01 grad: 9.32e+00 lr: 2.5e-04 69.7%┣┫ 1.7k/2.5k [31:46<13:48, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.07e-01 grad: 9.96e+00 lr: 2.5e-04 69.8%┣┫ 1.7k/2.5k [31:47<13:47, 1s/it]\n",
      "Loss train: 7.37e-02 val: 1.01e-01 grad: 7.79e+00 lr: 2.5e-04 69.8%┣┫ 1.7k/2.5k [31:48<13:46, 1s/it]\n",
      "Loss train: 8.68e-02 val: 1.04e-01 grad: 1.60e+01 lr: 2.5e-04 69.8%┣┫ 1.7k/2.5k [31:50<13:45, 1s/it]\n",
      "Loss train: 1.03e-01 val: 9.81e-02 grad: 1.00e+01 lr: 2.5e-04 69.9%┣┫ 1.7k/2.5k [31:51<13:44, 1s/it]\n",
      "Loss train: 8.82e-02 val: 1.00e-01 grad: 8.16e+00 lr: 2.5e-04 69.9%┣┫ 1.7k/2.5k [31:52<13:43, 1s/it]\n",
      "Loss train: 8.41e-02 val: 1.04e-01 grad: 9.18e+00 lr: 2.5e-04 70.0%┣┫ 1.7k/2.5k [31:53<13:42, 1s/it]\n",
      "Loss train: 7.91e-02 val: 1.17e-01 grad: 1.00e+01 lr: 2.5e-04 70.0%┣┫ 1.8k/2.5k [31:54<13:41, 1s/it]\n",
      "Loss train: 7.81e-02 val: 1.17e-01 grad: 9.31e+00 lr: 2.5e-04 70.0%┣┫ 1.8k/2.5k [31:55<13:40, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.12e-01 grad: 9.60e+00 lr: 2.5e-04 70.1%┣┫ 1.8k/2.5k [31:56<13:39, 1s/it]\n",
      "Loss train: 7.69e-02 val: 1.07e-01 grad: 9.35e+00 lr: 2.5e-04 70.1%┣┫ 1.8k/2.5k [31:57<13:37, 1s/it]\n",
      "Loss train: 7.70e-02 val: 1.13e-01 grad: 9.80e+00 lr: 2.5e-04 70.2%┣┫ 1.8k/2.5k [31:58<13:36, 1s/it]\n",
      "Loss train: 7.57e-02 val: 1.09e-01 grad: 8.23e+00 lr: 2.5e-04 70.2%┣┫ 1.8k/2.5k [31:59<13:35, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.04e-01 grad: 9.89e+00 lr: 2.5e-04 70.2%┣┫ 1.8k/2.5k [32:00<13:34, 1s/it]\n",
      "Loss train: 7.50e-02 val: 1.07e-01 grad: 9.43e+00 lr: 2.5e-04 70.3%┣┫ 1.8k/2.5k [32:02<13:33, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.05e-01 grad: 1.01e+01 lr: 2.5e-04 70.3%┣┫ 1.8k/2.5k [32:02<13:32, 1s/it]\n",
      "Loss train: 7.42e-02 val: 1.06e-01 grad: 9.29e+00 lr: 2.5e-04 70.4%┣┫ 1.8k/2.5k [32:04<13:31, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.12e-01 grad: 9.49e+00 lr: 2.5e-04 70.4%┣┫ 1.8k/2.5k [32:05<13:30, 1s/it]\n",
      "Loss train: 7.43e-02 val: 1.06e-01 grad: 7.55e+00 lr: 2.5e-04 70.4%┣┫ 1.8k/2.5k [32:06<13:29, 1s/it]\n",
      "Loss train: 7.36e-02 val: 9.93e-02 grad: 1.01e+01 lr: 2.5e-04 70.5%┣┫ 1.8k/2.5k [32:07<13:28, 1s/it]\n",
      "Loss train: 7.67e-02 val: 9.77e-02 grad: 9.94e+00 lr: 2.5e-04 70.5%┣┫ 1.8k/2.5k [32:08<13:26, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.03e-01 grad: 1.01e+01 lr: 2.5e-04 70.6%┣┫ 1.8k/2.5k [32:09<13:25, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.03e-01 grad: 8.19e+00 lr: 2.5e-04 70.6%┣┫ 1.8k/2.5k [32:10<13:24, 1s/it]\n",
      "Loss train: 7.59e-02 val: 9.77e-02 grad: 8.78e+00 lr: 2.5e-04 70.6%┣┫ 1.8k/2.5k [32:11<13:23, 1s/it]\n",
      "Loss train: 8.20e-02 val: 9.93e-02 grad: 1.03e+01 lr: 2.5e-04 70.7%┣┫ 1.8k/2.5k [32:12<13:22, 1s/it]\n",
      "Loss train: 7.36e-02 val: 1.05e-01 grad: 1.02e+01 lr: 2.5e-04 70.7%┣┫ 1.8k/2.5k [32:14<13:21, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.03e-01 grad: 9.91e+00 lr: 2.5e-04 70.8%┣┫ 1.8k/2.5k [32:15<13:20, 1s/it]\n",
      "Loss train: 7.46e-02 val: 1.04e-01 grad: 7.98e+00 lr: 2.5e-04 70.8%┣┫ 1.8k/2.5k [32:16<13:19, 1s/it]\n",
      "Loss train: 7.43e-02 val: 1.00e-01 grad: 1.82e+01 lr: 2.5e-04 70.8%┣┫ 1.8k/2.5k [32:17<13:18, 1s/it]\n",
      "Loss train: 9.94e-02 val: 1.06e-01 grad: 9.61e+00 lr: 2.5e-04 70.9%┣┫ 1.8k/2.5k [32:18<13:17, 1s/it]\n",
      "Loss train: 8.95e-02 val: 9.70e-02 grad: 9.47e+00 lr: 2.5e-04 70.9%┣┫ 1.8k/2.5k [32:19<13:16, 1s/it]\n",
      "Loss train: 8.31e-02 val: 9.99e-02 grad: 9.64e+00 lr: 2.5e-04 71.0%┣┫ 1.8k/2.5k [32:20<13:14, 1s/it]\n",
      "Loss train: 7.92e-02 val: 1.05e-01 grad: 9.98e+00 lr: 2.5e-04 71.0%┣┫ 1.8k/2.5k [32:21<13:13, 1s/it]\n",
      "Loss train: 7.69e-02 val: 1.14e-01 grad: 1.04e+01 lr: 2.5e-04 71.0%┣┫ 1.8k/2.5k [32:22<13:12, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.11e-01 grad: 9.41e+00 lr: 2.5e-04 71.1%┣┫ 1.8k/2.5k [32:23<13:11, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.11e-01 grad: 9.52e+00 lr: 2.5e-04 71.1%┣┫ 1.8k/2.5k [32:24<13:10, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.06e-01 grad: 7.69e+00 lr: 2.5e-04 71.2%┣┫ 1.8k/2.5k [32:25<13:09, 1s/it]\n",
      "Loss train: 7.50e-02 val: 1.03e-01 grad: 9.05e+00 lr: 2.5e-04 71.2%┣┫ 1.8k/2.5k [32:27<13:08, 1s/it]\n",
      "Loss train: 7.55e-02 val: 1.02e-01 grad: 9.74e+00 lr: 2.5e-04 71.2%┣┫ 1.8k/2.5k [32:28<13:07, 1s/it]\n",
      "Loss train: 7.54e-02 val: 1.10e-01 grad: 9.64e+00 lr: 2.5e-04 71.3%┣┫ 1.8k/2.5k [32:29<13:06, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.07e-01 grad: 7.62e+00 lr: 2.5e-04 71.3%┣┫ 1.8k/2.5k [32:30<13:05, 1s/it]\n",
      "Loss train: 7.39e-02 val: 1.02e-01 grad: 9.62e+00 lr: 2.5e-04 71.4%┣┫ 1.8k/2.5k [32:31<13:03, 1s/it]\n",
      "Loss train: 7.36e-02 val: 1.07e-01 grad: 1.00e+01 lr: 2.5e-04 71.4%┣┫ 1.8k/2.5k [32:32<13:02, 1s/it]\n",
      "Loss train: 7.30e-02 val: 1.04e-01 grad: 1.05e+01 lr: 2.5e-04 71.4%┣┫ 1.8k/2.5k [32:33<13:01, 1s/it]\n",
      "Loss train: 7.96e-02 val: 1.08e-01 grad: 8.83e+00 lr: 2.5e-04 71.5%┣┫ 1.8k/2.5k [32:34<13:00, 1s/it]\n",
      "Loss train: 7.38e-02 val: 9.91e-02 grad: 8.51e+00 lr: 2.5e-04 71.5%┣┫ 1.8k/2.5k [32:35<12:59, 1s/it]\n",
      "Loss train: 7.74e-02 val: 9.50e-02 grad: 9.51e+00 lr: 2.5e-04 71.6%┣┫ 1.8k/2.5k [32:37<12:58, 1s/it]\n",
      "Loss train: 7.32e-02 val: 9.88e-02 grad: 9.44e+00 lr: 2.5e-04 71.6%┣┫ 1.8k/2.5k [32:38<12:57, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.06e-01 grad: 9.29e+00 lr: 2.5e-04 71.6%┣┫ 1.8k/2.5k [32:39<12:56, 1s/it]\n",
      "Loss train: 7.37e-02 val: 9.27e-02 grad: 7.81e+00 lr: 2.5e-04 71.7%┣┫ 1.8k/2.5k [32:40<12:55, 1s/it]\n",
      "Loss train: 8.31e-02 val: 9.87e-02 grad: 1.41e+01 lr: 2.5e-04 71.7%┣┫ 1.8k/2.5k [32:41<12:54, 1s/it]\n",
      "Loss train: 8.55e-02 val: 9.55e-02 grad: 7.38e+00 lr: 2.5e-04 71.8%┣┫ 1.8k/2.5k [32:42<12:53, 1s/it]\n",
      "Loss train: 8.10e-02 val: 9.87e-02 grad: 9.79e+00 lr: 2.5e-04 71.8%┣┫ 1.8k/2.5k [32:43<12:52, 1s/it]\n",
      "Loss train: 7.89e-02 val: 1.02e-01 grad: 9.46e+00 lr: 2.5e-04 71.8%┣┫ 1.8k/2.5k [32:44<12:50, 1s/it]\n",
      "Loss train: 8.15e-02 val: 1.11e-01 grad: 9.61e+00 lr: 2.5e-04 71.9%┣┫ 1.8k/2.5k [32:45<12:49, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.05e-01 grad: 7.61e+00 lr: 2.5e-04 71.9%┣┫ 1.8k/2.5k [32:46<12:48, 1s/it]\n",
      "Loss train: 8.49e-02 val: 1.02e-01 grad: 1.04e+01 lr: 2.5e-04 72.0%┣┫ 1.8k/2.5k [32:47<12:47, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.03e-01 grad: 1.02e+01 lr: 2.5e-04 72.0%┣┫ 1.8k/2.5k [32:48<12:46, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.08e-01 grad: 8.38e+00 lr: 2.5e-04 72.0%┣┫ 1.8k/2.5k [32:49<12:45, 1s/it]\n",
      "Loss train: 7.59e-02 val: 1.09e-01 grad: 7.05e+00 lr: 2.5e-04 72.1%┣┫ 1.8k/2.5k [32:50<12:44, 1s/it]\n",
      "Loss train: 7.55e-02 val: 1.01e-01 grad: 9.57e+00 lr: 2.5e-04 72.1%┣┫ 1.8k/2.5k [32:52<12:43, 1s/it]\n",
      "Loss train: 7.97e-02 val: 9.75e-02 grad: 1.04e+01 lr: 2.5e-04 72.2%┣┫ 1.8k/2.5k [32:53<12:41, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.10e-01 grad: 9.71e+00 lr: 2.5e-04 72.2%┣┫ 1.8k/2.5k [32:54<12:40, 1s/it]\n",
      "Loss train: 7.35e-02 val: 1.06e-01 grad: 8.11e+00 lr: 2.5e-04 72.2%┣┫ 1.8k/2.5k [32:55<12:39, 1s/it]\n",
      "Loss train: 7.34e-02 val: 1.04e-01 grad: 9.53e+00 lr: 2.5e-04 72.3%┣┫ 1.8k/2.5k [32:56<12:38, 1s/it]\n",
      "Loss train: 7.57e-02 val: 9.93e-02 grad: 9.26e+00 lr: 2.5e-04 72.3%┣┫ 1.8k/2.5k [32:57<12:37, 1s/it]\n",
      "Loss train: 7.35e-02 val: 1.00e-01 grad: 9.63e+00 lr: 2.5e-04 72.4%┣┫ 1.8k/2.5k [32:58<12:36, 1s/it]\n",
      "Loss train: 7.29e-02 val: 1.02e-01 grad: 1.01e+01 lr: 2.5e-04 72.4%┣┫ 1.8k/2.5k [33:00<12:35, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.05e-01 grad: 1.03e+01 lr: 2.5e-04 72.4%┣┫ 1.8k/2.5k [33:01<12:34, 1s/it]\n",
      "Loss train: 7.40e-02 val: 9.98e-02 grad: 1.02e+01 lr: 2.5e-04 72.5%┣┫ 1.8k/2.5k [33:02<12:33, 1s/it]\n",
      "Loss train: 7.31e-02 val: 1.01e-01 grad: 9.91e+00 lr: 2.5e-04 72.5%┣┫ 1.8k/2.5k [33:03<12:32, 1s/it]\n",
      "Loss train: 7.25e-02 val: 1.00e-01 grad: 9.36e+00 lr: 2.5e-04 72.6%┣┫ 1.8k/2.5k [33:04<12:31, 1s/it]\n",
      "Loss train: 7.25e-02 val: 1.00e-01 grad: 9.84e+00 lr: 2.5e-04 72.6%┣┫ 1.8k/2.5k [33:05<12:30, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.01e-01 grad: 1.45e+01 lr: 2.5e-04 72.6%┣┫ 1.8k/2.5k [33:06<12:28, 1s/it]\n",
      "Loss train: 8.87e-02 val: 9.75e-02 grad: 8.85e+00 lr: 2.5e-04 72.7%┣┫ 1.8k/2.5k [33:07<12:27, 1s/it]\n",
      "Loss train: 8.70e-02 val: 9.38e-02 grad: 9.24e+00 lr: 2.5e-04 72.7%┣┫ 1.8k/2.5k [33:08<12:26, 1s/it]\n",
      "Loss train: 8.17e-02 val: 9.48e-02 grad: 1.01e+01 lr: 2.5e-04 72.8%┣┫ 1.8k/2.5k [33:09<12:25, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.04e-01 grad: 7.04e+00 lr: 2.5e-04 72.8%┣┫ 1.8k/2.5k [33:10<12:24, 1s/it]\n",
      "Loss train: 7.57e-02 val: 1.05e-01 grad: 9.29e+00 lr: 2.5e-04 72.8%┣┫ 1.8k/2.5k [33:11<12:23, 1s/it]\n",
      "Loss train: 7.54e-02 val: 1.04e-01 grad: 9.10e+00 lr: 2.5e-04 72.9%┣┫ 1.8k/2.5k [33:12<12:22, 1s/it]\n",
      "Loss train: 7.54e-02 val: 1.06e-01 grad: 9.04e+00 lr: 2.5e-04 72.9%┣┫ 1.8k/2.5k [33:13<12:21, 1s/it]\n",
      "Loss train: 7.53e-02 val: 1.04e-01 grad: 9.51e+00 lr: 2.5e-04 73.0%┣┫ 1.8k/2.5k [33:15<12:20, 1s/it]\n",
      "Loss train: 7.45e-02 val: 1.01e-01 grad: 9.92e+00 lr: 2.5e-04 73.0%┣┫ 1.8k/2.5k [33:16<12:19, 1s/it]\n",
      "Loss train: 7.35e-02 val: 1.00e-01 grad: 8.29e+00 lr: 2.5e-04 73.0%┣┫ 1.8k/2.5k [33:17<12:17, 1s/it]\n",
      "Loss train: 7.70e-02 val: 9.54e-02 grad: 1.02e+01 lr: 2.5e-04 73.1%┣┫ 1.8k/2.5k [33:18<12:16, 1s/it]\n",
      "Loss train: 7.39e-02 val: 1.03e-01 grad: 9.96e+00 lr: 2.5e-04 73.1%┣┫ 1.8k/2.5k [33:19<12:15, 1s/it]\n",
      "Loss train: 7.46e-02 val: 1.05e-01 grad: 7.87e+00 lr: 2.5e-04 73.2%┣┫ 1.8k/2.5k [33:20<12:14, 1s/it]\n",
      "Loss train: 7.35e-02 val: 9.95e-02 grad: 8.83e+00 lr: 2.5e-04 73.2%┣┫ 1.8k/2.5k [33:21<12:13, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.06e-01 grad: 9.39e+00 lr: 2.5e-04 73.2%┣┫ 1.8k/2.5k [33:22<12:12, 1s/it]\n",
      "Loss train: 7.26e-02 val: 9.85e-02 grad: 8.75e+00 lr: 2.5e-04 73.3%┣┫ 1.8k/2.5k [33:23<12:11, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.07e-01 grad: 9.76e+00 lr: 2.5e-04 73.3%┣┫ 1.8k/2.5k [33:24<12:10, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.01e-01 grad: 1.29e+01 lr: 2.5e-04 73.4%┣┫ 1.8k/2.5k [33:26<12:09, 1s/it]\n",
      "Loss train: 8.81e-02 val: 9.66e-02 grad: 9.90e+00 lr: 2.5e-04 73.4%┣┫ 1.8k/2.5k [33:27<12:08, 1s/it]\n",
      "Loss train: 8.55e-02 val: 9.54e-02 grad: 9.37e+00 lr: 2.5e-04 73.4%┣┫ 1.8k/2.5k [33:28<12:07, 1s/it]\n",
      "Loss train: 7.80e-02 val: 1.07e-01 grad: 1.04e+01 lr: 2.5e-04 73.5%┣┫ 1.8k/2.5k [33:29<12:05, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.09e-01 grad: 8.98e+00 lr: 2.5e-04 73.5%┣┫ 1.8k/2.5k [33:30<12:04, 1s/it]\n",
      "Loss train: 8.09e-02 val: 1.06e-01 grad: 9.28e+00 lr: 2.5e-04 73.6%┣┫ 1.8k/2.5k [33:31<12:03, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.11e-01 grad: 1.04e+01 lr: 2.5e-04 73.6%┣┫ 1.8k/2.5k [33:32<12:02, 1s/it]\n",
      "Loss train: 7.50e-02 val: 1.08e-01 grad: 9.48e+00 lr: 2.5e-04 73.6%┣┫ 1.8k/2.5k [33:33<12:01, 1s/it]\n",
      "Loss train: 7.43e-02 val: 1.05e-01 grad: 9.77e+00 lr: 2.5e-04 73.7%┣┫ 1.8k/2.5k [33:34<12:00, 1s/it]\n",
      "Loss train: 7.37e-02 val: 1.06e-01 grad: 9.00e+00 lr: 2.5e-04 73.7%┣┫ 1.8k/2.5k [33:35<11:59, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.09e-01 grad: 9.64e+00 lr: 2.5e-04 73.8%┣┫ 1.8k/2.5k [33:36<11:58, 1s/it]\n",
      "Loss train: 7.42e-02 val: 9.89e-02 grad: 9.49e+00 lr: 2.5e-04 73.8%┣┫ 1.8k/2.5k [33:37<11:57, 1s/it]\n",
      "Loss train: 7.26e-02 val: 1.01e-01 grad: 9.46e+00 lr: 2.5e-04 73.8%┣┫ 1.8k/2.5k [33:38<11:55, 1s/it]\n",
      "Loss train: 7.69e-02 val: 1.05e-01 grad: 8.93e+00 lr: 2.5e-04 73.9%┣┫ 1.8k/2.5k [33:39<11:54, 1s/it]\n",
      "Loss train: 7.36e-02 val: 9.53e-02 grad: 1.01e+01 lr: 2.5e-04 73.9%┣┫ 1.8k/2.5k [33:40<11:53, 1s/it]\n",
      "Loss train: 7.57e-02 val: 9.61e-02 grad: 1.18e+01 lr: 2.5e-04 74.0%┣┫ 1.8k/2.5k [33:42<11:52, 1s/it]\n",
      "Loss train: 8.09e-02 val: 9.82e-02 grad: 8.24e+00 lr: 2.5e-04 74.0%┣┫ 1.9k/2.5k [33:43<11:51, 1s/it]\n",
      "Loss train: 7.61e-02 val: 9.59e-02 grad: 8.02e+00 lr: 2.5e-04 74.0%┣┫ 1.9k/2.5k [33:44<11:50, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.00e-01 grad: 8.10e+00 lr: 2.5e-04 74.1%┣┫ 1.9k/2.5k [33:45<11:49, 1s/it]\n",
      "Loss train: 7.43e-02 val: 9.98e-02 grad: 9.15e+00 lr: 2.5e-04 74.1%┣┫ 1.9k/2.5k [33:46<11:48, 1s/it]\n",
      "Loss train: 7.37e-02 val: 1.01e-01 grad: 8.96e+00 lr: 2.5e-04 74.2%┣┫ 1.9k/2.5k [33:47<11:47, 1s/it]\n",
      "Loss train: 7.34e-02 val: 9.86e-02 grad: 9.77e+00 lr: 2.5e-04 74.2%┣┫ 1.9k/2.5k [33:48<11:46, 1s/it]\n",
      "Loss train: 7.33e-02 val: 9.92e-02 grad: 9.38e+00 lr: 2.5e-04 74.2%┣┫ 1.9k/2.5k [33:49<11:44, 1s/it]\n",
      "Loss train: 7.42e-02 val: 1.06e-01 grad: 8.14e+00 lr: 2.5e-04 74.3%┣┫ 1.9k/2.5k [33:50<11:43, 1s/it]\n",
      "Loss train: 7.50e-02 val: 9.99e-02 grad: 7.65e+00 lr: 2.5e-04 74.3%┣┫ 1.9k/2.5k [33:51<11:42, 1s/it]\n",
      "Loss train: 7.92e-02 val: 9.93e-02 grad: 1.41e+01 lr: 2.5e-04 74.4%┣┫ 1.9k/2.5k [33:52<11:41, 1s/it]\n",
      "Loss train: 9.43e-02 val: 1.04e-01 grad: 9.27e+00 lr: 2.5e-04 74.4%┣┫ 1.9k/2.5k [33:53<11:40, 1s/it]\n",
      "Loss train: 8.18e-02 val: 1.02e-01 grad: 9.73e+00 lr: 2.5e-04 74.4%┣┫ 1.9k/2.5k [33:54<11:39, 1s/it]\n",
      "Loss train: 8.01e-02 val: 1.07e-01 grad: 9.33e+00 lr: 2.5e-04 74.5%┣┫ 1.9k/2.5k [33:56<11:38, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.10e-01 grad: 9.82e+00 lr: 2.5e-04 74.5%┣┫ 1.9k/2.5k [33:57<11:37, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.14e-01 grad: 8.92e+00 lr: 2.5e-04 74.6%┣┫ 1.9k/2.5k [33:58<11:36, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.14e-01 grad: 8.84e+00 lr: 2.5e-04 74.6%┣┫ 1.9k/2.5k [33:59<11:35, 1s/it]\n",
      "Loss train: 7.41e-02 val: 1.05e-01 grad: 9.85e+00 lr: 2.5e-04 74.6%┣┫ 1.9k/2.5k [34:00<11:33, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.01e-01 grad: 9.22e+00 lr: 2.5e-04 74.7%┣┫ 1.9k/2.5k [34:01<11:32, 1s/it]\n",
      "Loss train: 7.32e-02 val: 1.05e-01 grad: 9.07e+00 lr: 2.5e-04 74.7%┣┫ 1.9k/2.5k [34:02<11:31, 1s/it]\n",
      "Loss train: 7.27e-02 val: 1.03e-01 grad: 9.26e+00 lr: 2.5e-04 74.8%┣┫ 1.9k/2.5k [34:03<11:30, 1s/it]\n",
      "Loss train: 7.37e-02 val: 9.87e-02 grad: 9.98e+00 lr: 2.5e-04 74.8%┣┫ 1.9k/2.5k [34:04<11:29, 1s/it]\n",
      "Loss train: 7.23e-02 val: 1.05e-01 grad: 9.43e+00 lr: 2.5e-04 74.8%┣┫ 1.9k/2.5k [34:05<11:28, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.03e-01 grad: 8.46e+00 lr: 2.5e-04 74.9%┣┫ 1.9k/2.5k [34:07<11:27, 1s/it]\n",
      "Loss train: 1.04e-01 val: 1.11e-01 grad: 2.16e+01 lr: 2.5e-04 74.9%┣┫ 1.9k/2.5k [34:08<11:26, 1s/it]\n",
      "Loss train: 9.90e-02 val: 1.01e-01 grad: 1.14e+01 lr: 2.5e-04 75.0%┣┫ 1.9k/2.5k [34:09<11:25, 1s/it]\n",
      "Loss train: 1.02e-01 val: 1.18e-01 grad: 1.06e+01 lr: 2.5e-04 75.0%┣┫ 1.9k/2.5k [34:10<11:24, 1s/it]\n",
      "Loss train: 8.86e-02 val: 1.12e-01 grad: 8.91e+00 lr: 2.5e-04 75.0%┣┫ 1.9k/2.5k [34:11<11:23, 1s/it]\n",
      "Loss train: 7.91e-02 val: 1.17e-01 grad: 9.23e+00 lr: 2.5e-04 75.1%┣┫ 1.9k/2.5k [34:12<11:21, 1s/it]\n",
      "Loss train: 8.17e-02 val: 1.22e-01 grad: 8.13e+00 lr: 2.5e-04 75.1%┣┫ 1.9k/2.5k [34:13<11:20, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.17e-01 grad: 6.37e+00 lr: 2.5e-04 75.2%┣┫ 1.9k/2.5k [34:14<11:19, 1s/it]\n",
      "Loss train: 7.75e-02 val: 1.07e-01 grad: 7.90e+00 lr: 2.5e-04 75.2%┣┫ 1.9k/2.5k [34:16<11:18, 1s/it]\n",
      "Loss train: 7.81e-02 val: 1.03e-01 grad: 9.67e+00 lr: 2.5e-04 75.2%┣┫ 1.9k/2.5k [34:17<11:17, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.11e-01 grad: 8.77e+00 lr: 2.5e-04 75.3%┣┫ 1.9k/2.5k [34:18<11:16, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.10e-01 grad: 8.53e+00 lr: 2.5e-04 75.3%┣┫ 1.9k/2.5k [34:19<11:15, 1s/it]\n",
      "Loss train: 7.42e-02 val: 1.06e-01 grad: 8.67e+00 lr: 2.5e-04 75.4%┣┫ 1.9k/2.5k [34:20<11:14, 1s/it]\n",
      "Loss train: 7.56e-02 val: 9.98e-02 grad: 8.90e+00 lr: 2.5e-04 75.4%┣┫ 1.9k/2.5k [34:21<11:13, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.02e-01 grad: 9.47e+00 lr: 2.5e-04 75.4%┣┫ 1.9k/2.5k [34:22<11:12, 1s/it]\n",
      "Loss train: 8.52e-02 val: 1.14e-01 grad: 8.70e+00 lr: 2.5e-04 75.5%┣┫ 1.9k/2.5k [34:23<11:11, 1s/it]\n",
      "Loss train: 7.25e-02 val: 1.02e-01 grad: 8.39e+00 lr: 2.5e-04 75.5%┣┫ 1.9k/2.5k [34:24<11:09, 1s/it]\n",
      "Loss train: 7.35e-02 val: 9.64e-02 grad: 9.54e+00 lr: 2.5e-04 75.6%┣┫ 1.9k/2.5k [34:25<11:08, 1s/it]\n",
      "Loss train: 7.24e-02 val: 9.65e-02 grad: 9.36e+00 lr: 2.5e-04 75.6%┣┫ 1.9k/2.5k [34:26<11:07, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.03e-01 grad: 6.82e+00 lr: 2.5e-04 75.6%┣┫ 1.9k/2.5k [34:27<11:06, 1s/it]\n",
      "Loss train: 7.27e-02 val: 9.53e-02 grad: 7.92e+00 lr: 2.5e-04 75.7%┣┫ 1.9k/2.5k [34:28<11:05, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.10e-01 grad: 1.85e+01 lr: 2.5e-04 75.7%┣┫ 1.9k/2.5k [34:30<11:04, 1s/it]\n",
      "Loss train: 1.08e-01 val: 8.99e-02 grad: 1.10e+01 lr: 2.5e-04 75.8%┣┫ 1.9k/2.5k [34:31<11:03, 1s/it]\n",
      "Loss train: 9.68e-02 val: 1.15e-01 grad: 1.09e+01 lr: 2.5e-04 75.8%┣┫ 1.9k/2.5k [34:32<11:02, 1s/it]\n",
      "Loss train: 8.54e-02 val: 1.09e-01 grad: 1.03e+01 lr: 2.5e-04 75.8%┣┫ 1.9k/2.5k [34:33<11:01, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.06e-01 grad: 1.02e+01 lr: 2.5e-04 75.9%┣┫ 1.9k/2.5k [34:34<11:00, 1s/it]\n",
      "Loss train: 7.90e-02 val: 1.19e-01 grad: 9.07e+00 lr: 2.5e-04 75.9%┣┫ 1.9k/2.5k [34:35<10:58, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.19e-01 grad: 7.40e+00 lr: 2.5e-04 76.0%┣┫ 1.9k/2.5k [34:36<10:57, 1s/it]\n",
      "Loss train: 7.71e-02 val: 1.06e-01 grad: 7.36e+00 lr: 2.5e-04 76.0%┣┫ 1.9k/2.5k [34:37<10:56, 1s/it]\n",
      "Loss train: 7.62e-02 val: 1.05e-01 grad: 9.91e+00 lr: 2.5e-04 76.0%┣┫ 1.9k/2.5k [34:38<10:55, 1s/it]\n",
      "Loss train: 7.92e-02 val: 1.04e-01 grad: 1.02e+01 lr: 2.5e-04 76.1%┣┫ 1.9k/2.5k [34:39<10:54, 1s/it]\n",
      "Loss train: 7.41e-02 val: 1.13e-01 grad: 1.04e+01 lr: 2.5e-04 76.1%┣┫ 1.9k/2.5k [34:40<10:53, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.12e-01 grad: 8.54e+00 lr: 2.5e-04 76.2%┣┫ 1.9k/2.5k [34:41<10:52, 1s/it]\n",
      "Loss train: 7.34e-02 val: 9.74e-02 grad: 9.30e+00 lr: 2.5e-04 76.2%┣┫ 1.9k/2.5k [34:42<10:51, 1s/it]\n",
      "Loss train: 8.16e-02 val: 1.10e-01 grad: 1.29e+01 lr: 2.5e-04 76.2%┣┫ 1.9k/2.5k [34:44<10:50, 1s/it]\n",
      "Loss train: 8.19e-02 val: 1.04e-01 grad: 8.43e+00 lr: 2.5e-04 76.3%┣┫ 1.9k/2.5k [34:45<10:49, 1s/it]\n",
      "Loss train: 7.98e-02 val: 1.00e-01 grad: 8.70e+00 lr: 2.5e-04 76.3%┣┫ 1.9k/2.5k [34:46<10:47, 1s/it]\n",
      "Loss train: 7.65e-02 val: 1.08e-01 grad: 9.28e+00 lr: 2.5e-04 76.4%┣┫ 1.9k/2.5k [34:47<10:46, 1s/it]\n",
      "Loss train: 7.69e-02 val: 1.11e-01 grad: 8.84e+00 lr: 2.5e-04 76.4%┣┫ 1.9k/2.5k [34:48<10:45, 1s/it]\n",
      "Loss train: 7.47e-02 val: 1.06e-01 grad: 6.61e+00 lr: 2.5e-04 76.4%┣┫ 1.9k/2.5k [34:49<10:44, 1s/it]\n",
      "Loss train: 7.84e-02 val: 9.90e-02 grad: 9.37e+00 lr: 2.5e-04 76.5%┣┫ 1.9k/2.5k [34:50<10:43, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.04e-01 grad: 9.20e+00 lr: 2.5e-04 76.5%┣┫ 1.9k/2.5k [34:51<10:42, 1s/it]\n",
      "Loss train: 7.36e-02 val: 1.07e-01 grad: 8.54e+00 lr: 2.5e-04 76.6%┣┫ 1.9k/2.5k [34:52<10:41, 1s/it]\n",
      "Loss train: 7.31e-02 val: 1.06e-01 grad: 9.01e+00 lr: 2.5e-04 76.6%┣┫ 1.9k/2.5k [34:53<10:40, 1s/it]\n",
      "Loss train: 7.29e-02 val: 1.00e-01 grad: 9.86e+00 lr: 2.5e-04 76.6%┣┫ 1.9k/2.5k [34:54<10:39, 1s/it]\n",
      "Loss train: 7.70e-02 val: 9.79e-02 grad: 9.13e+00 lr: 2.5e-04 76.7%┣┫ 1.9k/2.5k [34:55<10:38, 1s/it]\n",
      "Loss train: 7.17e-02 val: 1.02e-01 grad: 9.81e+00 lr: 2.5e-04 76.7%┣┫ 1.9k/2.5k [34:56<10:36, 1s/it]\n",
      "Loss train: 7.72e-02 val: 1.07e-01 grad: 8.56e+00 lr: 2.5e-04 76.8%┣┫ 1.9k/2.5k [34:57<10:35, 1s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.3   0.0   0.0   0.0   0.0  0.0   170.64  0.06  16.19  -0.3    0.02  -0.0    0.27  0.01  0.0\n",
      "  0.11  0.0   0.0   0.36  0.0  0.01  146.58  0.2   19.82  -0.11   0.29  -0.0   -0.36  0.18  0.0\n",
      "  0.74  0.2   0.02  0.0   0.0  0.14  150.82  0.42  25.95  -0.74  -0.2   -0.02   0.67  0.28  0.0\n",
      " -0.0   0.45  0.23  0.0   0.0  0.08  127.76  0.19  19.6    0.0   -0.45  -0.23   0.48  0.2   0.0\n",
      " -0.0   0.0   0.0   0.36  0.0  0.15  191.72  0.02  16.12   0.0   -0.0    0.28  -0.36  0.08  0.0\n",
      "  0.04  0.06  0.03  0.0   0.0  0.17  183.56  0.05  14.0   -0.04  -0.06  -0.03   0.14  0.0   0.0\n",
      " -0.0   0.0   0.0   0.81  0.0  0.34  104.48  0.28  21.49   0.0    0.19   0.25  -0.81  0.36  0.0\n",
      "  0.07  0.0   0.0   0.7   0.0  0.0   120.13  0.3   21.18  -0.07   0.25   0.08  -0.7   0.44  0.0\n",
      "\n",
      "Min Loss train: 7.17e-02 val: 7.81e-02\n",
      " update plot 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 8.11e-02 val: 7.81e-02 grad: 8.14e+00 lr: 2.5e-04 76.8%┣┫ 1.9k/2.5k [34:59<10:34, 1s/it]\n",
      "Loss train: 8.28e-02 val: 1.00e-01 grad: 1.35e+01 lr: 2.5e-04 76.8%┣┫ 1.9k/2.5k [35:00<10:33, 1s/it]\n",
      "Loss train: 8.59e-02 val: 9.95e-02 grad: 9.89e+00 lr: 2.5e-04 76.9%┣┫ 1.9k/2.5k [35:01<10:32, 1s/it]\n",
      "Loss train: 8.50e-02 val: 1.06e-01 grad: 8.92e+00 lr: 2.5e-04 76.9%┣┫ 1.9k/2.5k [35:02<10:31, 1s/it]\n",
      "Loss train: 7.73e-02 val: 1.03e-01 grad: 7.17e+00 lr: 2.5e-04 77.0%┣┫ 1.9k/2.5k [35:03<10:30, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.03e-01 grad: 9.50e+00 lr: 2.5e-04 77.0%┣┫ 1.9k/2.5k [35:04<10:29, 1s/it]\n",
      "Loss train: 7.64e-02 val: 1.02e-01 grad: 9.07e+00 lr: 2.5e-04 77.0%┣┫ 1.9k/2.5k [35:05<10:28, 1s/it]\n",
      "Loss train: 7.51e-02 val: 1.08e-01 grad: 1.00e+01 lr: 2.5e-04 77.1%┣┫ 1.9k/2.5k [35:06<10:27, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.07e-01 grad: 7.26e+00 lr: 2.5e-04 77.1%┣┫ 1.9k/2.5k [35:07<10:25, 1s/it]\n",
      "Loss train: 7.37e-02 val: 9.96e-02 grad: 9.00e+00 lr: 2.5e-04 77.2%┣┫ 1.9k/2.5k [35:08<10:24, 1s/it]\n",
      "Loss train: 7.47e-02 val: 9.64e-02 grad: 8.89e+00 lr: 2.5e-04 77.2%┣┫ 1.9k/2.5k [35:09<10:23, 1s/it]\n",
      "Loss train: 7.53e-02 val: 1.05e-01 grad: 9.32e+00 lr: 2.5e-04 77.2%┣┫ 1.9k/2.5k [35:10<10:22, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.01e-01 grad: 8.44e+00 lr: 2.5e-04 77.3%┣┫ 1.9k/2.5k [35:11<10:21, 1s/it]\n",
      "Loss train: 7.19e-02 val: 9.85e-02 grad: 9.32e+00 lr: 2.5e-04 77.3%┣┫ 1.9k/2.5k [35:12<10:20, 1s/it]\n",
      "Loss train: 7.23e-02 val: 9.57e-02 grad: 9.69e+00 lr: 2.5e-04 77.4%┣┫ 1.9k/2.5k [35:14<10:19, 1s/it]\n",
      "Loss train: 7.27e-02 val: 1.02e-01 grad: 8.44e+00 lr: 2.5e-04 77.4%┣┫ 1.9k/2.5k [35:15<10:18, 1s/it]\n",
      "Loss train: 7.40e-02 val: 9.62e-02 grad: 1.62e+01 lr: 2.5e-04 77.4%┣┫ 1.9k/2.5k [35:16<10:17, 1s/it]\n",
      "Loss train: 1.21e-01 val: 1.21e-01 grad: 9.84e+00 lr: 2.5e-04 77.5%┣┫ 1.9k/2.5k [35:17<10:16, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.00e-01 grad: 1.15e+01 lr: 2.5e-04 77.5%┣┫ 1.9k/2.5k [35:18<10:14, 1s/it]\n",
      "Loss train: 8.71e-02 val: 1.11e-01 grad: 9.92e+00 lr: 2.5e-04 77.6%┣┫ 1.9k/2.5k [35:19<10:13, 1s/it]\n",
      "Loss train: 8.07e-02 val: 1.09e-01 grad: 9.94e+00 lr: 2.5e-04 77.6%┣┫ 1.9k/2.5k [35:20<10:12, 1s/it]\n",
      "Loss train: 8.04e-02 val: 1.20e-01 grad: 9.04e+00 lr: 2.5e-04 77.6%┣┫ 1.9k/2.5k [35:21<10:11, 1s/it]\n",
      "Loss train: 7.93e-02 val: 1.18e-01 grad: 7.96e+00 lr: 2.5e-04 77.7%┣┫ 1.9k/2.5k [35:22<10:10, 1s/it]\n",
      "Loss train: 7.77e-02 val: 1.04e-01 grad: 9.66e+00 lr: 2.5e-04 77.7%┣┫ 1.9k/2.5k [35:23<10:09, 1s/it]\n",
      "Loss train: 7.51e-02 val: 1.01e-01 grad: 9.59e+00 lr: 2.5e-04 77.8%┣┫ 1.9k/2.5k [35:24<10:08, 1s/it]\n",
      "Loss train: 7.31e-02 val: 1.04e-01 grad: 9.07e+00 lr: 2.5e-04 77.8%┣┫ 1.9k/2.5k [35:25<10:07, 1s/it]\n",
      "Loss train: 7.58e-02 val: 1.08e-01 grad: 8.48e+00 lr: 2.5e-04 77.8%┣┫ 1.9k/2.5k [35:26<10:06, 1s/it]\n",
      "Loss train: 7.29e-02 val: 9.92e-02 grad: 9.54e+00 lr: 2.5e-04 77.9%┣┫ 1.9k/2.5k [35:27<10:05, 1s/it]\n",
      "Loss train: 7.22e-02 val: 9.85e-02 grad: 9.27e+00 lr: 2.5e-04 77.9%┣┫ 1.9k/2.5k [35:29<10:03, 1s/it]\n",
      "Loss train: 7.51e-02 val: 1.05e-01 grad: 8.76e+00 lr: 2.5e-04 78.0%┣┫ 1.9k/2.5k [35:30<10:02, 1s/it]\n",
      "Loss train: 7.17e-02 val: 9.85e-02 grad: 8.37e+00 lr: 2.5e-04 78.0%┣┫ 1.9k/2.5k [35:31<10:01, 1s/it]\n",
      "Loss train: 7.23e-02 val: 9.72e-02 grad: 9.05e+00 lr: 2.5e-04 78.0%┣┫ 2.0k/2.5k [35:32<10:00, 1s/it]\n",
      "Loss train: 7.17e-02 val: 9.75e-02 grad: 8.86e+00 lr: 2.5e-04 78.1%┣┫ 2.0k/2.5k [35:33<09:59, 1s/it]\n",
      "Loss train: 7.17e-02 val: 9.95e-02 grad: 9.49e+00 lr: 2.5e-04 78.1%┣┫ 2.0k/2.5k [35:34<09:58, 1s/it]\n",
      "Loss train: 7.12e-02 val: 9.72e-02 grad: 8.78e+00 lr: 2.5e-04 78.2%┣┫ 2.0k/2.5k [35:35<09:57, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.86e-02 grad: 1.37e+01 lr: 2.5e-04 78.2%┣┫ 2.0k/2.5k [35:36<09:56, 1s/it]\n",
      "Loss train: 8.44e-02 val: 9.56e-02 grad: 7.80e+00 lr: 2.5e-04 78.2%┣┫ 2.0k/2.5k [35:37<09:55, 1s/it]\n",
      "Loss train: 8.27e-02 val: 9.41e-02 grad: 8.48e+00 lr: 2.5e-04 78.3%┣┫ 2.0k/2.5k [35:38<09:54, 1s/it]\n",
      "Loss train: 7.90e-02 val: 9.22e-02 grad: 8.32e+00 lr: 2.5e-04 78.3%┣┫ 2.0k/2.5k [35:39<09:53, 1s/it]\n",
      "Loss train: 7.72e-02 val: 9.91e-02 grad: 8.70e+00 lr: 2.5e-04 78.4%┣┫ 2.0k/2.5k [35:40<09:51, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.01e-01 grad: 8.63e+00 lr: 2.5e-04 78.4%┣┫ 2.0k/2.5k [35:41<09:50, 1s/it]\n",
      "Loss train: 7.84e-02 val: 1.04e-01 grad: 8.35e+00 lr: 2.5e-04 78.4%┣┫ 2.0k/2.5k [35:42<09:49, 1s/it]\n",
      "Loss train: 7.45e-02 val: 9.60e-02 grad: 7.09e+00 lr: 2.5e-04 78.5%┣┫ 2.0k/2.5k [35:43<09:48, 1s/it]\n",
      "Loss train: 7.71e-02 val: 9.56e-02 grad: 9.54e+00 lr: 2.5e-04 78.5%┣┫ 2.0k/2.5k [35:44<09:47, 1s/it]\n",
      "Loss train: 7.26e-02 val: 9.78e-02 grad: 9.00e+00 lr: 2.5e-04 78.6%┣┫ 2.0k/2.5k [35:45<09:46, 1s/it]\n",
      "Loss train: 7.23e-02 val: 9.95e-02 grad: 8.51e+00 lr: 2.5e-04 78.6%┣┫ 2.0k/2.5k [35:46<09:45, 1s/it]\n",
      "Loss train: 7.19e-02 val: 9.82e-02 grad: 7.87e+00 lr: 2.5e-04 78.6%┣┫ 2.0k/2.5k [35:47<09:44, 1s/it]\n",
      "Loss train: 7.28e-02 val: 1.03e-01 grad: 7.21e+00 lr: 2.5e-04 78.7%┣┫ 2.0k/2.5k [35:48<09:42, 1s/it]\n",
      "Loss train: 7.64e-02 val: 9.79e-02 grad: 7.14e+00 lr: 2.5e-04 78.7%┣┫ 2.0k/2.5k [35:50<09:41, 1s/it]\n",
      "Loss train: 7.61e-02 val: 9.85e-02 grad: 1.06e+01 lr: 2.5e-04 78.8%┣┫ 2.0k/2.5k [35:51<09:40, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.10e-01 grad: 9.00e+00 lr: 2.5e-04 78.8%┣┫ 2.0k/2.5k [35:52<09:39, 1s/it]\n",
      "Loss train: 7.23e-02 val: 1.04e-01 grad: 9.04e+00 lr: 2.5e-04 78.8%┣┫ 2.0k/2.5k [35:53<09:38, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.05e-01 grad: 8.56e+00 lr: 2.5e-04 78.9%┣┫ 2.0k/2.5k [35:54<09:37, 1s/it]\n",
      "Loss train: 7.12e-02 val: 9.87e-02 grad: 9.22e+00 lr: 2.5e-04 78.9%┣┫ 2.0k/2.5k [35:55<09:36, 1s/it]\n",
      "Loss train: 7.18e-02 val: 9.49e-02 grad: 9.03e+00 lr: 2.5e-04 79.0%┣┫ 2.0k/2.5k [35:56<09:35, 1s/it]\n",
      "Loss train: 7.11e-02 val: 9.43e-02 grad: 8.99e+00 lr: 2.5e-04 79.0%┣┫ 2.0k/2.5k [35:57<09:34, 1s/it]\n",
      "Loss train: 7.05e-02 val: 9.64e-02 grad: 9.08e+00 lr: 2.5e-04 79.0%┣┫ 2.0k/2.5k [35:58<09:33, 1s/it]\n",
      "Loss train: 7.42e-02 val: 9.72e-02 grad: 1.35e+01 lr: 2.5e-04 79.1%┣┫ 2.0k/2.5k [35:59<09:32, 1s/it]\n",
      "Loss train: 8.43e-02 val: 9.61e-02 grad: 8.61e+00 lr: 2.5e-04 79.1%┣┫ 2.0k/2.5k [36:00<09:30, 1s/it]\n",
      "Loss train: 8.51e-02 val: 1.00e-01 grad: 8.82e+00 lr: 2.5e-04 79.2%┣┫ 2.0k/2.5k [36:01<09:29, 1s/it]\n",
      "Loss train: 8.15e-02 val: 1.05e-01 grad: 1.00e+01 lr: 2.5e-04 79.2%┣┫ 2.0k/2.5k [36:02<09:28, 1s/it]\n",
      "Loss train: 7.60e-02 val: 1.14e-01 grad: 9.20e+00 lr: 2.5e-04 79.2%┣┫ 2.0k/2.5k [36:04<09:27, 1s/it]\n",
      "Loss train: 7.84e-02 val: 1.17e-01 grad: 7.62e+00 lr: 2.5e-04 79.3%┣┫ 2.0k/2.5k [36:05<09:26, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.07e-01 grad: 8.58e+00 lr: 2.5e-04 79.3%┣┫ 2.0k/2.5k [36:06<09:25, 1s/it]\n",
      "Loss train: 7.43e-02 val: 1.00e-01 grad: 9.26e+00 lr: 2.5e-04 79.4%┣┫ 2.0k/2.5k [36:07<09:24, 1s/it]\n",
      "Loss train: 7.24e-02 val: 1.00e-01 grad: 8.89e+00 lr: 2.5e-04 79.4%┣┫ 2.0k/2.5k [36:08<09:23, 1s/it]\n",
      "Loss train: 7.17e-02 val: 9.84e-02 grad: 9.15e+00 lr: 2.5e-04 79.4%┣┫ 2.0k/2.5k [36:09<09:22, 1s/it]\n",
      "Loss train: 7.14e-02 val: 9.88e-02 grad: 9.22e+00 lr: 2.5e-04 79.5%┣┫ 2.0k/2.5k [36:10<09:21, 1s/it]\n",
      "Loss train: 7.11e-02 val: 9.57e-02 grad: 8.74e+00 lr: 2.5e-04 79.5%┣┫ 2.0k/2.5k [36:11<09:19, 1s/it]\n",
      "Loss train: 7.21e-02 val: 9.28e-02 grad: 8.65e+00 lr: 2.5e-04 79.6%┣┫ 2.0k/2.5k [36:12<09:18, 1s/it]\n",
      "Loss train: 7.29e-02 val: 9.22e-02 grad: 9.83e+00 lr: 2.5e-04 79.6%┣┫ 2.0k/2.5k [36:13<09:17, 1s/it]\n",
      "Loss train: 7.88e-02 val: 1.06e-01 grad: 6.83e+00 lr: 2.5e-04 79.6%┣┫ 2.0k/2.5k [36:14<09:16, 1s/it]\n",
      "Loss train: 7.22e-02 val: 8.71e-02 grad: 8.69e+00 lr: 2.5e-04 79.7%┣┫ 2.0k/2.5k [36:16<09:15, 1s/it]\n",
      "Loss train: 7.15e-02 val: 9.72e-02 grad: 1.81e+01 lr: 2.5e-04 79.7%┣┫ 2.0k/2.5k [36:17<09:14, 1s/it]\n",
      "Loss train: 1.19e-01 val: 1.12e-01 grad: 1.03e+01 lr: 2.5e-04 79.8%┣┫ 2.0k/2.5k [36:18<09:13, 1s/it]\n",
      "Loss train: 9.74e-02 val: 1.05e-01 grad: 1.14e+01 lr: 2.5e-04 79.8%┣┫ 2.0k/2.5k [36:19<09:12, 1s/it]\n",
      "Loss train: 8.69e-02 val: 1.12e-01 grad: 9.97e+00 lr: 2.5e-04 79.8%┣┫ 2.0k/2.5k [36:20<09:11, 1s/it]\n",
      "Loss train: 8.66e-02 val: 1.16e-01 grad: 1.01e+01 lr: 2.5e-04 79.9%┣┫ 2.0k/2.5k [36:21<09:10, 1s/it]\n",
      "Loss train: 7.71e-02 val: 1.13e-01 grad: 8.83e+00 lr: 2.5e-04 79.9%┣┫ 2.0k/2.5k [36:22<09:09, 1s/it]\n",
      "Loss train: 8.18e-02 val: 1.21e-01 grad: 8.35e+00 lr: 2.5e-04 80.0%┣┫ 2.0k/2.5k [36:23<09:07, 1s/it]\n",
      "Loss train: 7.70e-02 val: 1.07e-01 grad: 9.22e+00 lr: 1.3e-04 80.0%┣┫ 2.0k/2.5k [36:24<09:06, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.06e-01 grad: 9.74e+00 lr: 1.3e-04 80.0%┣┫ 2.0k/2.5k [36:25<09:05, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.05e-01 grad: 9.62e+00 lr: 1.3e-04 80.1%┣┫ 2.0k/2.5k [36:26<09:04, 1s/it]\n",
      "Loss train: 7.30e-02 val: 1.08e-01 grad: 8.59e+00 lr: 1.3e-04 80.1%┣┫ 2.0k/2.5k [36:28<09:03, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.10e-01 grad: 9.03e+00 lr: 1.3e-04 80.2%┣┫ 2.0k/2.5k [36:29<09:02, 1s/it]\n",
      "Loss train: 7.23e-02 val: 1.07e-01 grad: 8.93e+00 lr: 1.3e-04 80.2%┣┫ 2.0k/2.5k [36:30<09:01, 1s/it]\n",
      "Loss train: 7.21e-02 val: 1.05e-01 grad: 8.92e+00 lr: 1.3e-04 80.2%┣┫ 2.0k/2.5k [36:31<09:00, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.06e-01 grad: 8.79e+00 lr: 1.3e-04 80.3%┣┫ 2.0k/2.5k [36:32<08:59, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.03e-01 grad: 9.02e+00 lr: 1.3e-04 80.3%┣┫ 2.0k/2.5k [36:33<08:58, 1s/it]\n",
      "Loss train: 7.16e-02 val: 1.05e-01 grad: 9.21e+00 lr: 1.3e-04 80.4%┣┫ 2.0k/2.5k [36:35<08:57, 1s/it]\n",
      "Loss train: 7.13e-02 val: 1.05e-01 grad: 8.57e+00 lr: 1.3e-04 80.4%┣┫ 2.0k/2.5k [36:36<08:56, 1s/it]\n",
      "Loss train: 7.08e-02 val: 1.02e-01 grad: 8.58e+00 lr: 1.3e-04 80.4%┣┫ 2.0k/2.5k [36:38<08:55, 1s/it]\n",
      "Loss train: 7.09e-02 val: 9.70e-02 grad: 8.86e+00 lr: 1.3e-04 80.5%┣┫ 2.0k/2.5k [36:39<08:54, 1s/it]\n",
      "Loss train: 7.16e-02 val: 9.94e-02 grad: 9.63e+00 lr: 1.3e-04 80.5%┣┫ 2.0k/2.5k [36:41<08:53, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.86e-02 grad: 9.14e+00 lr: 1.3e-04 80.6%┣┫ 2.0k/2.5k [36:42<08:52, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.02e-01 grad: 8.23e+00 lr: 1.3e-04 80.6%┣┫ 2.0k/2.5k [36:43<08:51, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.03e-01 grad: 8.70e+00 lr: 1.3e-04 80.6%┣┫ 2.0k/2.5k [36:44<08:49, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.80e-02 grad: 9.48e+00 lr: 1.3e-04 80.7%┣┫ 2.0k/2.5k [36:45<08:48, 1s/it]\n",
      "Loss train: 7.11e-02 val: 1.01e-01 grad: 8.46e+00 lr: 1.3e-04 80.7%┣┫ 2.0k/2.5k [36:46<08:47, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.82e-02 grad: 8.06e+00 lr: 1.3e-04 80.8%┣┫ 2.0k/2.5k [36:47<08:46, 1s/it]\n",
      "Loss train: 7.22e-02 val: 8.79e-02 grad: 8.34e+00 lr: 1.3e-04 80.8%┣┫ 2.0k/2.5k [36:48<08:45, 1s/it]\n",
      "Loss train: 7.49e-02 val: 8.59e-02 grad: 1.82e+01 lr: 1.3e-04 80.8%┣┫ 2.0k/2.5k [36:50<08:44, 1s/it]\n",
      "Loss train: 7.78e-02 val: 9.84e-02 grad: 9.34e+00 lr: 1.3e-04 80.9%┣┫ 2.0k/2.5k [36:51<08:43, 1s/it]\n",
      "Loss train: 8.20e-02 val: 9.92e-02 grad: 7.28e+00 lr: 1.3e-04 80.9%┣┫ 2.0k/2.5k [36:52<08:42, 1s/it]\n",
      "Loss train: 7.91e-02 val: 9.70e-02 grad: 6.88e+00 lr: 1.3e-04 81.0%┣┫ 2.0k/2.5k [36:53<08:41, 1s/it]\n",
      "Loss train: 7.67e-02 val: 9.69e-02 grad: 9.01e+00 lr: 1.3e-04 81.0%┣┫ 2.0k/2.5k [36:54<08:40, 1s/it]\n",
      "Loss train: 7.52e-02 val: 1.01e-01 grad: 8.04e+00 lr: 1.3e-04 81.0%┣┫ 2.0k/2.5k [36:55<08:39, 1s/it]\n",
      "Loss train: 7.34e-02 val: 1.03e-01 grad: 6.71e+00 lr: 1.3e-04 81.1%┣┫ 2.0k/2.5k [36:56<08:37, 1s/it]\n",
      "Loss train: 7.34e-02 val: 9.99e-02 grad: 8.52e+00 lr: 1.3e-04 81.1%┣┫ 2.0k/2.5k [36:57<08:36, 1s/it]\n",
      "Loss train: 7.26e-02 val: 1.01e-01 grad: 8.62e+00 lr: 1.3e-04 81.2%┣┫ 2.0k/2.5k [36:58<08:35, 1s/it]\n",
      "Loss train: 7.21e-02 val: 1.01e-01 grad: 8.73e+00 lr: 1.3e-04 81.2%┣┫ 2.0k/2.5k [36:59<08:34, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.97e-02 grad: 8.48e+00 lr: 1.3e-04 81.2%┣┫ 2.0k/2.5k [37:01<08:33, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.01e-01 grad: 8.75e+00 lr: 1.3e-04 81.3%┣┫ 2.0k/2.5k [37:02<08:32, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.06e-01 grad: 9.14e+00 lr: 1.3e-04 81.3%┣┫ 2.0k/2.5k [37:03<08:31, 1s/it]\n",
      "Loss train: 7.12e-02 val: 1.02e-01 grad: 7.22e+00 lr: 1.3e-04 81.4%┣┫ 2.0k/2.5k [37:04<08:30, 1s/it]\n",
      "Loss train: 7.14e-02 val: 9.88e-02 grad: 8.35e+00 lr: 1.3e-04 81.4%┣┫ 2.0k/2.5k [37:05<08:29, 1s/it]\n",
      "Loss train: 7.28e-02 val: 9.72e-02 grad: 9.28e+00 lr: 1.3e-04 81.4%┣┫ 2.0k/2.5k [37:06<08:28, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.97e-02 grad: 9.29e+00 lr: 1.3e-04 81.5%┣┫ 2.0k/2.5k [37:07<08:26, 1s/it]\n",
      "Loss train: 7.21e-02 val: 1.04e-01 grad: 9.37e+00 lr: 1.3e-04 81.5%┣┫ 2.0k/2.5k [37:08<08:25, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.92e-02 grad: 8.45e+00 lr: 1.3e-04 81.6%┣┫ 2.0k/2.5k [37:09<08:24, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.99e-02 grad: 9.32e+00 lr: 1.3e-04 81.6%┣┫ 2.0k/2.5k [37:11<08:23, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.74e-02 grad: 8.80e+00 lr: 1.3e-04 81.6%┣┫ 2.0k/2.5k [37:12<08:22, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.66e-02 grad: 1.08e+01 lr: 1.3e-04 81.7%┣┫ 2.0k/2.5k [37:13<08:21, 1s/it]\n",
      "Loss train: 7.31e-02 val: 9.98e-02 grad: 8.52e+00 lr: 1.3e-04 81.7%┣┫ 2.0k/2.5k [37:14<08:20, 1s/it]\n",
      "Loss train: 7.23e-02 val: 9.97e-02 grad: 7.34e+00 lr: 1.3e-04 81.8%┣┫ 2.0k/2.5k [37:15<08:19, 1s/it]\n",
      "Loss train: 7.33e-02 val: 9.58e-02 grad: 9.05e+00 lr: 1.3e-04 81.8%┣┫ 2.0k/2.5k [37:16<08:18, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.73e-02 grad: 9.18e+00 lr: 1.3e-04 81.8%┣┫ 2.0k/2.5k [37:17<08:17, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.75e-02 grad: 8.97e+00 lr: 1.3e-04 81.9%┣┫ 2.0k/2.5k [37:18<08:16, 1s/it]\n",
      "Loss train: 7.10e-02 val: 1.00e-01 grad: 8.71e+00 lr: 1.3e-04 81.9%┣┫ 2.0k/2.5k [37:19<08:14, 1s/it]\n",
      "Loss train: 7.05e-02 val: 1.00e-01 grad: 8.01e+00 lr: 1.3e-04 82.0%┣┫ 2.0k/2.5k [37:20<08:13, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.99e-02 grad: 7.52e+00 lr: 1.3e-04 82.0%┣┫ 2.0k/2.5k [37:21<08:12, 1s/it]\n",
      "Loss train: 7.15e-02 val: 9.56e-02 grad: 1.13e+01 lr: 1.3e-04 82.0%┣┫ 2.1k/2.5k [37:23<08:11, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.98e-02 grad: 9.00e+00 lr: 1.3e-04 82.1%┣┫ 2.1k/2.5k [37:24<08:10, 1s/it]\n",
      "Loss train: 7.27e-02 val: 1.00e-01 grad: 7.46e+00 lr: 1.3e-04 82.1%┣┫ 2.1k/2.5k [37:25<08:09, 1s/it]\n",
      "Loss train: 7.18e-02 val: 9.89e-02 grad: 7.24e+00 lr: 1.3e-04 82.2%┣┫ 2.1k/2.5k [37:26<08:08, 1s/it]\n",
      "Loss train: 7.47e-02 val: 9.60e-02 grad: 8.95e+00 lr: 1.3e-04 82.2%┣┫ 2.1k/2.5k [37:27<08:07, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.77e-02 grad: 9.31e+00 lr: 1.3e-04 82.2%┣┫ 2.1k/2.5k [37:28<08:06, 1s/it]\n",
      "Loss train: 7.45e-02 val: 1.04e-01 grad: 8.07e+00 lr: 1.3e-04 82.3%┣┫ 2.1k/2.5k [37:29<08:05, 1s/it]\n",
      "Loss train: 7.05e-02 val: 1.01e-01 grad: 7.19e+00 lr: 1.3e-04 82.3%┣┫ 2.1k/2.5k [37:30<08:04, 1s/it]\n",
      "Loss train: 7.69e-02 val: 9.82e-02 grad: 9.89e+00 lr: 1.3e-04 82.4%┣┫ 2.1k/2.5k [37:31<08:02, 1s/it]\n",
      "Loss train: 7.25e-02 val: 9.55e-02 grad: 9.80e+00 lr: 1.3e-04 82.4%┣┫ 2.1k/2.5k [37:33<08:01, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.89e-02 grad: 9.22e+00 lr: 1.3e-04 82.4%┣┫ 2.1k/2.5k [37:34<08:00, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.02e-01 grad: 8.35e+00 lr: 1.3e-04 82.5%┣┫ 2.1k/2.5k [37:35<07:59, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.63e-02 grad: 6.84e+00 lr: 1.3e-04 82.5%┣┫ 2.1k/2.5k [37:36<07:58, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.91e-02 grad: 9.89e+00 lr: 1.3e-04 82.6%┣┫ 2.1k/2.5k [37:37<07:57, 1s/it]\n",
      "Loss train: 7.34e-02 val: 9.59e-02 grad: 8.62e+00 lr: 1.3e-04 82.6%┣┫ 2.1k/2.5k [37:38<07:56, 1s/it]\n",
      "Loss train: 7.26e-02 val: 9.63e-02 grad: 9.35e+00 lr: 1.3e-04 82.6%┣┫ 2.1k/2.5k [37:39<07:55, 1s/it]\n",
      "Loss train: 7.22e-02 val: 1.02e-01 grad: 8.48e+00 lr: 1.3e-04 82.7%┣┫ 2.1k/2.5k [37:40<07:54, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.83e-02 grad: 9.17e+00 lr: 1.3e-04 82.7%┣┫ 2.1k/2.5k [37:41<07:53, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.87e-02 grad: 9.15e+00 lr: 1.3e-04 82.8%┣┫ 2.1k/2.5k [37:43<07:52, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.89e-02 grad: 7.38e+00 lr: 1.3e-04 82.8%┣┫ 2.1k/2.5k [37:44<07:50, 1s/it]\n",
      "Loss train: 7.27e-02 val: 9.41e-02 grad: 1.04e+01 lr: 1.3e-04 82.8%┣┫ 2.1k/2.5k [37:45<07:49, 1s/it]\n",
      "Loss train: 7.09e-02 val: 9.74e-02 grad: 9.30e+00 lr: 1.3e-04 82.9%┣┫ 2.1k/2.5k [37:46<07:48, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.66e-02 grad: 8.47e+00 lr: 1.3e-04 82.9%┣┫ 2.1k/2.5k [37:47<07:47, 1s/it]\n",
      "Loss train: 7.06e-02 val: 9.63e-02 grad: 8.51e+00 lr: 1.3e-04 83.0%┣┫ 2.1k/2.5k [37:48<07:46, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.52e-02 grad: 8.74e+00 lr: 1.3e-04 83.0%┣┫ 2.1k/2.5k [37:49<07:45, 1s/it]\n",
      "Loss train: 7.21e-02 val: 9.83e-02 grad: 9.35e+00 lr: 1.3e-04 83.0%┣┫ 2.1k/2.5k [37:50<07:44, 1s/it]\n",
      "Loss train: 6.99e-02 val: 9.46e-02 grad: 7.47e+00 lr: 1.3e-04 83.1%┣┫ 2.1k/2.5k [37:51<07:43, 1s/it]\n",
      "Loss train: 7.12e-02 val: 9.35e-02 grad: 9.29e+00 lr: 1.3e-04 83.1%┣┫ 2.1k/2.5k [37:53<07:42, 1s/it]\n",
      "Loss train: 6.98e-02 val: 9.64e-02 grad: 9.27e+00 lr: 1.3e-04 83.2%┣┫ 2.1k/2.5k [37:54<07:41, 1s/it]\n",
      "Loss train: 7.06e-02 val: 1.01e-01 grad: 8.27e+00 lr: 1.3e-04 83.2%┣┫ 2.1k/2.5k [37:55<07:40, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.40e-02 grad: 7.21e+00 lr: 1.3e-04 83.2%┣┫ 2.1k/2.5k [37:56<07:38, 1s/it]\n",
      "Loss train: 7.15e-02 val: 9.72e-02 grad: 1.54e+01 lr: 1.3e-04 83.3%┣┫ 2.1k/2.5k [37:57<07:37, 1s/it]\n",
      "Loss train: 8.46e-02 val: 9.78e-02 grad: 8.10e+00 lr: 1.3e-04 83.3%┣┫ 2.1k/2.5k [37:58<07:36, 1s/it]\n",
      "Loss train: 8.47e-02 val: 9.51e-02 grad: 8.77e+00 lr: 1.3e-04 83.4%┣┫ 2.1k/2.5k [37:59<07:35, 1s/it]\n",
      "Loss train: 7.96e-02 val: 9.60e-02 grad: 9.04e+00 lr: 1.3e-04 83.4%┣┫ 2.1k/2.5k [38:00<07:34, 1s/it]\n",
      "Loss train: 7.49e-02 val: 9.92e-02 grad: 9.16e+00 lr: 1.3e-04 83.4%┣┫ 2.1k/2.5k [38:01<07:33, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.02e-01 grad: 9.12e+00 lr: 1.3e-04 83.5%┣┫ 2.1k/2.5k [38:02<07:32, 1s/it]\n",
      "Loss train: 7.66e-02 val: 1.09e-01 grad: 7.69e+00 lr: 1.3e-04 83.5%┣┫ 2.1k/2.5k [38:03<07:31, 1s/it]\n",
      "Loss train: 7.25e-02 val: 1.04e-01 grad: 8.37e+00 lr: 1.3e-04 83.6%┣┫ 2.1k/2.5k [38:04<07:30, 1s/it]\n",
      "Loss train: 7.20e-02 val: 1.02e-01 grad: 8.77e+00 lr: 1.3e-04 83.6%┣┫ 2.1k/2.5k [38:05<07:29, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.89e-02 grad: 8.82e+00 lr: 1.3e-04 83.6%┣┫ 2.1k/2.5k [38:07<07:27, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.76e-02 grad: 8.74e+00 lr: 1.3e-04 83.7%┣┫ 2.1k/2.5k [38:08<07:26, 1s/it]\n",
      "Loss train: 7.22e-02 val: 9.85e-02 grad: 8.93e+00 lr: 1.3e-04 83.7%┣┫ 2.1k/2.5k [38:09<07:25, 1s/it]\n",
      "Loss train: 7.11e-02 val: 9.38e-02 grad: 8.71e+00 lr: 1.3e-04 83.8%┣┫ 2.1k/2.5k [38:10<07:24, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.68e-02 grad: 8.92e+00 lr: 1.3e-04 83.8%┣┫ 2.1k/2.5k [38:11<07:23, 1s/it]\n",
      "Loss train: 7.05e-02 val: 9.38e-02 grad: 8.88e+00 lr: 1.3e-04 83.8%┣┫ 2.1k/2.5k [38:12<07:22, 1s/it]\n",
      "Loss train: 7.11e-02 val: 9.70e-02 grad: 8.71e+00 lr: 1.3e-04 83.9%┣┫ 2.1k/2.5k [38:13<07:21, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.61e-02 grad: 7.75e+00 lr: 1.3e-04 83.9%┣┫ 2.1k/2.5k [38:14<07:20, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.41e-02 grad: 8.91e+00 lr: 1.3e-04 84.0%┣┫ 2.1k/2.5k [38:15<07:19, 1s/it]\n",
      "Loss train: 6.96e-02 val: 9.60e-02 grad: 9.21e+00 lr: 1.3e-04 84.0%┣┫ 2.1k/2.5k [38:17<07:18, 1s/it]\n",
      "Loss train: 6.98e-02 val: 9.89e-02 grad: 8.52e+00 lr: 1.3e-04 84.0%┣┫ 2.1k/2.5k [38:18<07:17, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.01e-01 grad: 7.40e+00 lr: 1.3e-04 84.1%┣┫ 2.1k/2.5k [38:19<07:15, 1s/it]\n",
      "Loss train: 6.98e-02 val: 9.84e-02 grad: 9.03e+00 lr: 1.3e-04 84.1%┣┫ 2.1k/2.5k [38:20<07:14, 1s/it]\n",
      "Loss train: 7.25e-02 val: 9.57e-02 grad: 9.02e+00 lr: 1.3e-04 84.2%┣┫ 2.1k/2.5k [38:21<07:13, 1s/it]\n",
      "Loss train: 7.06e-02 val: 9.65e-02 grad: 9.23e+00 lr: 1.3e-04 84.2%┣┫ 2.1k/2.5k [38:22<07:12, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.93e-02 grad: 8.41e+00 lr: 1.3e-04 84.2%┣┫ 2.1k/2.5k [38:23<07:11, 1s/it]\n",
      "Loss train: 6.98e-02 val: 9.85e-02 grad: 8.02e+00 lr: 1.3e-04 84.3%┣┫ 2.1k/2.5k [38:24<07:10, 1s/it]\n",
      "Loss train: 7.11e-02 val: 9.39e-02 grad: 8.58e+00 lr: 1.3e-04 84.3%┣┫ 2.1k/2.5k [38:25<07:09, 1s/it]\n",
      "Loss train: 6.94e-02 val: 9.54e-02 grad: 8.89e+00 lr: 1.3e-04 84.4%┣┫ 2.1k/2.5k [38:26<07:08, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.73e-02 grad: 9.28e+00 lr: 1.3e-04 84.4%┣┫ 2.1k/2.5k [38:27<07:07, 1s/it]\n",
      "Loss train: 6.90e-02 val: 9.49e-02 grad: 9.21e+00 lr: 1.3e-04 84.4%┣┫ 2.1k/2.5k [38:29<07:06, 1s/it]\n",
      "Loss train: 6.89e-02 val: 9.48e-02 grad: 8.42e+00 lr: 1.3e-04 84.5%┣┫ 2.1k/2.5k [38:30<07:05, 1s/it]\n",
      "Loss train: 6.90e-02 val: 9.50e-02 grad: 8.70e+00 lr: 1.3e-04 84.5%┣┫ 2.1k/2.5k [38:31<07:03, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.71e-02 grad: 9.76e+00 lr: 1.3e-04 84.6%┣┫ 2.1k/2.5k [38:32<07:02, 1s/it]\n",
      "Loss train: 6.90e-02 val: 9.48e-02 grad: 7.32e+00 lr: 1.3e-04 84.6%┣┫ 2.1k/2.5k [38:33<07:01, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.22e-02 grad: 9.30e+00 lr: 1.3e-04 84.6%┣┫ 2.1k/2.5k [38:34<07:00, 1s/it]\n",
      "Loss train: 6.95e-02 val: 9.54e-02 grad: 8.44e+00 lr: 1.3e-04 84.7%┣┫ 2.1k/2.5k [38:35<06:59, 1s/it]\n",
      "Loss train: 6.88e-02 val: 9.52e-02 grad: 8.74e+00 lr: 1.3e-04 84.7%┣┫ 2.1k/2.5k [38:36<06:58, 1s/it]\n",
      "Loss train: 7.30e-02 val: 9.92e-02 grad: 8.27e+00 lr: 1.3e-04 84.8%┣┫ 2.1k/2.5k [38:37<06:57, 1s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      "  0.3   0.0   0.0   0.0   0.0  0.0   174.19  0.06  16.52  -0.3    0.02  -0.0    0.27  0.01  0.0\n",
      "  0.11  0.0   0.01  0.34  0.0  0.01  148.72  0.2   20.28  -0.11   0.29  -0.01  -0.34  0.18  0.0\n",
      "  0.76  0.21  0.01  0.0   0.0  0.16  153.22  0.43  26.68  -0.76  -0.21  -0.01   0.69  0.28  0.0\n",
      " -0.0   0.45  0.22  0.0   0.0  0.09  128.63  0.18  19.94   0.0   -0.45  -0.22   0.5   0.17  0.0\n",
      " -0.0   0.0   0.0   0.36  0.0  0.15  195.71  0.02  16.45   0.0   -0.0    0.28  -0.36  0.08  0.0\n",
      "  0.04  0.06  0.03  0.0   0.0  0.17  187.36  0.05  14.29  -0.04  -0.06  -0.03   0.14  0.0   0.0\n",
      " -0.0   0.0   0.0   0.85  0.0  0.35  104.75  0.29  22.06   0.0    0.21   0.25  -0.85  0.38  0.0\n",
      "  0.07  0.0   0.0   0.71  0.0  0.0   121.43  0.31  21.71  -0.07   0.26   0.07  -0.71  0.45  0.0\n",
      "\n",
      "Min Loss train: 6.88e-02 val: 6.36e-02\n",
      " update plot 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss train: 8.68e-02 val: 6.36e-02 grad: 9.08e+00 lr: 1.3e-04 84.8%┣┫ 2.1k/2.5k [38:39<06:56, 1s/it]\n",
      "Loss train: 7.72e-02 val: 9.65e-02 grad: 1.65e+01 lr: 1.3e-04 84.8%┣┫ 2.1k/2.5k [38:40<06:55, 1s/it]\n",
      "Loss train: 8.26e-02 val: 9.34e-02 grad: 7.12e+00 lr: 1.3e-04 84.9%┣┫ 2.1k/2.5k [38:41<06:54, 1s/it]\n",
      "Loss train: 7.90e-02 val: 9.38e-02 grad: 8.77e+00 lr: 1.3e-04 84.9%┣┫ 2.1k/2.5k [38:42<06:52, 1s/it]\n",
      "Loss train: 7.56e-02 val: 9.51e-02 grad: 8.23e+00 lr: 1.3e-04 85.0%┣┫ 2.1k/2.5k [38:43<06:51, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.01e-01 grad: 7.23e+00 lr: 1.3e-04 85.0%┣┫ 2.1k/2.5k [38:44<06:50, 1s/it]\n",
      "Loss train: 7.25e-02 val: 1.02e-01 grad: 7.60e+00 lr: 1.3e-04 85.0%┣┫ 2.1k/2.5k [38:45<06:49, 1s/it]\n",
      "Loss train: 7.33e-02 val: 9.83e-02 grad: 8.87e+00 lr: 1.3e-04 85.1%┣┫ 2.1k/2.5k [38:46<06:48, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.01e-01 grad: 8.88e+00 lr: 1.3e-04 85.1%┣┫ 2.1k/2.5k [38:47<06:47, 1s/it]\n",
      "Loss train: 7.16e-02 val: 9.82e-02 grad: 8.85e+00 lr: 1.3e-04 85.2%┣┫ 2.1k/2.5k [38:48<06:46, 1s/it]\n",
      "Loss train: 7.43e-02 val: 1.03e-01 grad: 8.62e+00 lr: 1.3e-04 85.2%┣┫ 2.1k/2.5k [38:49<06:45, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.78e-02 grad: 6.86e+00 lr: 1.3e-04 85.2%┣┫ 2.1k/2.5k [38:50<06:44, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.71e-02 grad: 8.66e+00 lr: 1.3e-04 85.3%┣┫ 2.1k/2.5k [38:51<06:43, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.70e-02 grad: 8.82e+00 lr: 1.3e-04 85.3%┣┫ 2.1k/2.5k [38:52<06:41, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.59e-02 grad: 8.66e+00 lr: 1.3e-04 85.4%┣┫ 2.1k/2.5k [38:53<06:40, 1s/it]\n",
      "Loss train: 7.06e-02 val: 9.93e-02 grad: 9.40e+00 lr: 1.3e-04 85.4%┣┫ 2.1k/2.5k [38:54<06:39, 1s/it]\n",
      "Loss train: 6.93e-02 val: 9.78e-02 grad: 7.52e+00 lr: 1.3e-04 85.4%┣┫ 2.1k/2.5k [38:55<06:38, 1s/it]\n",
      "Loss train: 6.95e-02 val: 9.63e-02 grad: 8.73e+00 lr: 1.3e-04 85.5%┣┫ 2.1k/2.5k [38:57<06:37, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.50e-02 grad: 8.67e+00 lr: 1.3e-04 85.5%┣┫ 2.1k/2.5k [38:58<06:36, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.92e-02 grad: 9.29e+00 lr: 1.3e-04 85.6%┣┫ 2.1k/2.5k [38:59<06:35, 1s/it]\n",
      "Loss train: 6.87e-02 val: 9.31e-02 grad: 9.00e+00 lr: 1.3e-04 85.6%┣┫ 2.1k/2.5k [39:00<06:34, 1s/it]\n",
      "Loss train: 6.93e-02 val: 9.52e-02 grad: 8.74e+00 lr: 1.3e-04 85.6%┣┫ 2.1k/2.5k [39:01<06:33, 1s/it]\n",
      "Loss train: 7.05e-02 val: 9.62e-02 grad: 8.08e+00 lr: 1.3e-04 85.7%┣┫ 2.1k/2.5k [39:02<06:32, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.21e-02 grad: 9.00e+00 lr: 1.3e-04 85.7%┣┫ 2.1k/2.5k [39:03<06:31, 1s/it]\n",
      "Loss train: 6.95e-02 val: 9.61e-02 grad: 8.34e+00 lr: 1.3e-04 85.8%┣┫ 2.1k/2.5k [39:04<06:29, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.30e-02 grad: 8.26e+00 lr: 1.3e-04 85.8%┣┫ 2.1k/2.5k [39:06<06:28, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.38e-02 grad: 8.82e+00 lr: 1.3e-04 85.8%┣┫ 2.1k/2.5k [39:07<06:27, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.35e-02 grad: 9.22e+00 lr: 1.3e-04 85.9%┣┫ 2.1k/2.5k [39:08<06:26, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.71e-02 grad: 7.50e+00 lr: 1.3e-04 85.9%┣┫ 2.1k/2.5k [39:09<06:25, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.92e-02 grad: 8.69e+00 lr: 1.3e-04 86.0%┣┫ 2.1k/2.5k [39:10<06:24, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.46e-02 grad: 8.83e+00 lr: 1.3e-04 86.0%┣┫ 2.1k/2.5k [39:11<06:23, 1s/it]\n",
      "Loss train: 6.90e-02 val: 9.61e-02 grad: 9.07e+00 lr: 1.3e-04 86.0%┣┫ 2.2k/2.5k [39:12<06:22, 1s/it]\n",
      "Loss train: 6.89e-02 val: 9.38e-02 grad: 8.29e+00 lr: 1.3e-04 86.1%┣┫ 2.2k/2.5k [39:13<06:21, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.73e-02 grad: 7.18e+00 lr: 1.3e-04 86.1%┣┫ 2.2k/2.5k [39:14<06:20, 1s/it]\n",
      "Loss train: 7.02e-02 val: 8.88e-02 grad: 8.31e+00 lr: 1.3e-04 86.2%┣┫ 2.2k/2.5k [39:15<06:19, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.34e-02 grad: 1.39e+01 lr: 1.3e-04 86.2%┣┫ 2.2k/2.5k [39:17<06:17, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.03e-01 grad: 8.23e+00 lr: 1.3e-04 86.2%┣┫ 2.2k/2.5k [39:18<06:16, 1s/it]\n",
      "Loss train: 7.56e-02 val: 9.53e-02 grad: 9.61e+00 lr: 1.3e-04 86.3%┣┫ 2.2k/2.5k [39:19<06:15, 1s/it]\n",
      "Loss train: 7.93e-02 val: 1.02e-01 grad: 9.85e+00 lr: 1.3e-04 86.3%┣┫ 2.2k/2.5k [39:20<06:14, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.01e-01 grad: 9.58e+00 lr: 1.3e-04 86.4%┣┫ 2.2k/2.5k [39:21<06:13, 1s/it]\n",
      "Loss train: 7.14e-02 val: 1.05e-01 grad: 8.06e+00 lr: 1.3e-04 86.4%┣┫ 2.2k/2.5k [39:22<06:12, 1s/it]\n",
      "Loss train: 7.07e-02 val: 1.03e-01 grad: 8.39e+00 lr: 1.3e-04 86.4%┣┫ 2.2k/2.5k [39:23<06:11, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.05e-01 grad: 8.99e+00 lr: 1.3e-04 86.5%┣┫ 2.2k/2.5k [39:24<06:10, 1s/it]\n",
      "Loss train: 7.04e-02 val: 1.00e-01 grad: 8.61e+00 lr: 1.3e-04 86.5%┣┫ 2.2k/2.5k [39:25<06:09, 1s/it]\n",
      "Loss train: 6.96e-02 val: 9.62e-02 grad: 8.65e+00 lr: 1.3e-04 86.6%┣┫ 2.2k/2.5k [39:26<06:08, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.86e-02 grad: 7.68e+00 lr: 1.3e-04 86.6%┣┫ 2.2k/2.5k [39:27<06:06, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.39e-02 grad: 9.27e+00 lr: 1.3e-04 86.6%┣┫ 2.2k/2.5k [39:28<06:05, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.77e-02 grad: 8.48e+00 lr: 1.3e-04 86.7%┣┫ 2.2k/2.5k [39:30<06:04, 1s/it]\n",
      "Loss train: 6.92e-02 val: 9.43e-02 grad: 6.88e+00 lr: 1.3e-04 86.7%┣┫ 2.2k/2.5k [39:31<06:03, 1s/it]\n",
      "Loss train: 7.09e-02 val: 9.40e-02 grad: 9.06e+00 lr: 1.3e-04 86.8%┣┫ 2.2k/2.5k [39:32<06:02, 1s/it]\n",
      "Loss train: 7.24e-02 val: 1.02e-01 grad: 9.77e+00 lr: 1.3e-04 86.8%┣┫ 2.2k/2.5k [39:33<06:01, 1s/it]\n",
      "Loss train: 7.02e-02 val: 8.95e-02 grad: 7.51e+00 lr: 1.3e-04 86.8%┣┫ 2.2k/2.5k [39:34<06:00, 1s/it]\n",
      "Loss train: 7.58e-02 val: 9.61e-02 grad: 1.35e+01 lr: 1.3e-04 86.9%┣┫ 2.2k/2.5k [39:35<05:59, 1s/it]\n",
      "Loss train: 8.10e-02 val: 1.00e-01 grad: 8.35e+00 lr: 1.3e-04 86.9%┣┫ 2.2k/2.5k [39:36<05:58, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.00e-01 grad: 7.32e+00 lr: 1.3e-04 87.0%┣┫ 2.2k/2.5k [39:37<05:57, 1s/it]\n",
      "Loss train: 7.37e-02 val: 1.00e-01 grad: 9.02e+00 lr: 1.3e-04 87.0%┣┫ 2.2k/2.5k [39:39<05:56, 1s/it]\n",
      "Loss train: 7.42e-02 val: 1.08e-01 grad: 9.49e+00 lr: 1.3e-04 87.0%┣┫ 2.2k/2.5k [39:41<05:55, 1s/it]\n",
      "Loss train: 7.11e-02 val: 1.03e-01 grad: 6.22e+00 lr: 1.3e-04 87.1%┣┫ 2.2k/2.5k [39:42<05:54, 1s/it]\n",
      "Loss train: 7.41e-02 val: 9.84e-02 grad: 8.81e+00 lr: 1.3e-04 87.1%┣┫ 2.2k/2.5k [39:43<05:52, 1s/it]\n",
      "Loss train: 7.34e-02 val: 9.72e-02 grad: 9.68e+00 lr: 1.3e-04 87.2%┣┫ 2.2k/2.5k [39:44<05:51, 1s/it]\n",
      "Loss train: 7.05e-02 val: 1.02e-01 grad: 8.91e+00 lr: 1.3e-04 87.2%┣┫ 2.2k/2.5k [39:45<05:50, 1s/it]\n",
      "Loss train: 7.04e-02 val: 1.01e-01 grad: 8.13e+00 lr: 1.3e-04 87.2%┣┫ 2.2k/2.5k [39:46<05:49, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.64e-02 grad: 8.03e+00 lr: 1.3e-04 87.3%┣┫ 2.2k/2.5k [39:47<05:48, 1s/it]\n",
      "Loss train: 6.93e-02 val: 9.65e-02 grad: 8.70e+00 lr: 1.3e-04 87.3%┣┫ 2.2k/2.5k [39:48<05:47, 1s/it]\n",
      "Loss train: 6.92e-02 val: 9.73e-02 grad: 8.79e+00 lr: 1.3e-04 87.4%┣┫ 2.2k/2.5k [39:49<05:46, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.03e-01 grad: 9.09e+00 lr: 1.3e-04 87.4%┣┫ 2.2k/2.5k [39:51<05:45, 1s/it]\n",
      "Loss train: 6.88e-02 val: 9.64e-02 grad: 8.26e+00 lr: 1.3e-04 87.4%┣┫ 2.2k/2.5k [39:52<05:44, 1s/it]\n",
      "Loss train: 6.84e-02 val: 9.72e-02 grad: 9.02e+00 lr: 1.3e-04 87.5%┣┫ 2.2k/2.5k [39:53<05:43, 1s/it]\n",
      "Loss train: 6.83e-02 val: 9.59e-02 grad: 8.55e+00 lr: 1.3e-04 87.5%┣┫ 2.2k/2.5k [39:54<05:42, 1s/it]\n",
      "Loss train: 6.89e-02 val: 9.65e-02 grad: 1.21e+01 lr: 1.3e-04 87.6%┣┫ 2.2k/2.5k [39:55<05:40, 1s/it]\n",
      "Loss train: 7.84e-02 val: 9.68e-02 grad: 7.19e+00 lr: 1.3e-04 87.6%┣┫ 2.2k/2.5k [39:56<05:39, 1s/it]\n",
      "Loss train: 7.70e-02 val: 9.20e-02 grad: 6.87e+00 lr: 1.3e-04 87.6%┣┫ 2.2k/2.5k [39:57<05:38, 1s/it]\n",
      "Loss train: 7.71e-02 val: 9.44e-02 grad: 9.32e+00 lr: 1.3e-04 87.7%┣┫ 2.2k/2.5k [39:58<05:37, 1s/it]\n",
      "Loss train: 7.27e-02 val: 9.65e-02 grad: 9.77e+00 lr: 1.3e-04 87.7%┣┫ 2.2k/2.5k [39:59<05:36, 1s/it]\n",
      "Loss train: 7.21e-02 val: 1.04e-01 grad: 9.14e+00 lr: 1.3e-04 87.8%┣┫ 2.2k/2.5k [40:00<05:35, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.97e-02 grad: 8.86e+00 lr: 1.3e-04 87.8%┣┫ 2.2k/2.5k [40:01<05:34, 1s/it]\n",
      "Loss train: 6.99e-02 val: 1.01e-01 grad: 8.90e+00 lr: 1.3e-04 87.8%┣┫ 2.2k/2.5k [40:02<05:33, 1s/it]\n",
      "Loss train: 6.93e-02 val: 9.77e-02 grad: 8.66e+00 lr: 1.3e-04 87.9%┣┫ 2.2k/2.5k [40:04<05:32, 1s/it]\n",
      "Loss train: 7.11e-02 val: 9.40e-02 grad: 8.73e+00 lr: 1.3e-04 87.9%┣┫ 2.2k/2.5k [40:05<05:31, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.32e-02 grad: 9.69e+00 lr: 1.3e-04 88.0%┣┫ 2.2k/2.5k [40:06<05:29, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.82e-02 grad: 8.63e+00 lr: 1.3e-04 88.0%┣┫ 2.2k/2.5k [40:07<05:28, 1s/it]\n",
      "Loss train: 6.96e-02 val: 9.97e-02 grad: 7.51e+00 lr: 1.3e-04 88.0%┣┫ 2.2k/2.5k [40:08<05:27, 1s/it]\n",
      "Loss train: 7.06e-02 val: 9.21e-02 grad: 9.20e+00 lr: 1.3e-04 88.1%┣┫ 2.2k/2.5k [40:09<05:26, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.42e-02 grad: 1.12e+01 lr: 1.3e-04 88.1%┣┫ 2.2k/2.5k [40:11<05:25, 1s/it]\n",
      "Loss train: 7.23e-02 val: 9.79e-02 grad: 8.55e+00 lr: 1.3e-04 88.2%┣┫ 2.2k/2.5k [40:12<05:24, 1s/it]\n",
      "Loss train: 7.96e-02 val: 1.01e-01 grad: 8.02e+00 lr: 1.3e-04 88.2%┣┫ 2.2k/2.5k [40:13<05:23, 1s/it]\n",
      "Loss train: 7.14e-02 val: 9.34e-02 grad: 7.19e+00 lr: 1.3e-04 88.2%┣┫ 2.2k/2.5k [40:14<05:22, 1s/it]\n",
      "Loss train: 7.29e-02 val: 9.55e-02 grad: 9.48e+00 lr: 1.3e-04 88.3%┣┫ 2.2k/2.5k [40:15<05:21, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.70e-02 grad: 9.54e+00 lr: 1.3e-04 88.3%┣┫ 2.2k/2.5k [40:16<05:20, 1s/it]\n",
      "Loss train: 7.35e-02 val: 1.07e-01 grad: 7.44e+00 lr: 1.3e-04 88.4%┣┫ 2.2k/2.5k [40:17<05:19, 1s/it]\n",
      "Loss train: 6.94e-02 val: 1.04e-01 grad: 7.35e+00 lr: 1.3e-04 88.4%┣┫ 2.2k/2.5k [40:18<05:17, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.85e-02 grad: 8.82e+00 lr: 1.3e-04 88.4%┣┫ 2.2k/2.5k [40:19<05:16, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.57e-02 grad: 1.09e+01 lr: 1.3e-04 88.5%┣┫ 2.2k/2.5k [40:20<05:15, 1s/it]\n",
      "Loss train: 7.48e-02 val: 9.96e-02 grad: 8.16e+00 lr: 1.3e-04 88.5%┣┫ 2.2k/2.5k [40:21<05:14, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.59e-02 grad: 6.73e+00 lr: 1.3e-04 88.6%┣┫ 2.2k/2.5k [40:23<05:13, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.62e-02 grad: 8.24e+00 lr: 1.3e-04 88.6%┣┫ 2.2k/2.5k [40:24<05:12, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.59e-02 grad: 9.16e+00 lr: 1.3e-04 88.6%┣┫ 2.2k/2.5k [40:25<05:11, 1s/it]\n",
      "Loss train: 7.05e-02 val: 1.01e-01 grad: 8.50e+00 lr: 1.3e-04 88.7%┣┫ 2.2k/2.5k [40:26<05:10, 1s/it]\n",
      "Loss train: 6.92e-02 val: 9.94e-02 grad: 6.74e+00 lr: 1.3e-04 88.7%┣┫ 2.2k/2.5k [40:27<05:09, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.49e-02 grad: 9.22e+00 lr: 1.3e-04 88.8%┣┫ 2.2k/2.5k [40:28<05:08, 1s/it]\n",
      "Loss train: 7.15e-02 val: 9.46e-02 grad: 9.57e+00 lr: 1.3e-04 88.8%┣┫ 2.2k/2.5k [40:29<05:07, 1s/it]\n",
      "Loss train: 6.86e-02 val: 9.58e-02 grad: 9.31e+00 lr: 1.3e-04 88.8%┣┫ 2.2k/2.5k [40:30<05:05, 1s/it]\n",
      "Loss train: 6.92e-02 val: 1.00e-01 grad: 7.85e+00 lr: 1.3e-04 88.9%┣┫ 2.2k/2.5k [40:31<05:04, 1s/it]\n",
      "Loss train: 6.91e-02 val: 9.92e-02 grad: 6.80e+00 lr: 1.3e-04 88.9%┣┫ 2.2k/2.5k [40:33<05:03, 1s/it]\n",
      "Loss train: 6.95e-02 val: 9.41e-02 grad: 1.01e+01 lr: 1.3e-04 89.0%┣┫ 2.2k/2.5k [40:34<05:02, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.43e-02 grad: 9.29e+00 lr: 1.3e-04 89.0%┣┫ 2.2k/2.5k [40:35<05:01, 1s/it]\n",
      "Loss train: 6.87e-02 val: 9.59e-02 grad: 9.21e+00 lr: 1.3e-04 89.0%┣┫ 2.2k/2.5k [40:36<05:00, 1s/it]\n",
      "Loss train: 8.34e-02 val: 1.10e-01 grad: 9.19e+00 lr: 1.3e-04 89.1%┣┫ 2.2k/2.5k [40:37<04:59, 1s/it]\n",
      "Loss train: 7.18e-02 val: 8.49e-02 grad: 8.24e+00 lr: 1.3e-04 89.1%┣┫ 2.2k/2.5k [40:38<04:58, 1s/it]\n",
      "Loss train: 8.75e-02 val: 9.56e-02 grad: 1.93e+01 lr: 1.3e-04 89.2%┣┫ 2.2k/2.5k [40:39<04:57, 1s/it]\n",
      "Loss train: 1.03e-01 val: 9.21e-02 grad: 1.17e+01 lr: 1.3e-04 89.2%┣┫ 2.2k/2.5k [40:40<04:56, 1s/it]\n",
      "Loss train: 9.54e-02 val: 1.24e-01 grad: 1.09e+01 lr: 1.3e-04 89.2%┣┫ 2.2k/2.5k [40:41<04:55, 1s/it]\n",
      "Loss train: 9.55e-02 val: 1.24e-01 grad: 1.00e+01 lr: 1.3e-04 89.3%┣┫ 2.2k/2.5k [40:42<04:53, 1s/it]\n",
      "Loss train: 7.83e-02 val: 1.17e-01 grad: 8.35e+00 lr: 1.3e-04 89.3%┣┫ 2.2k/2.5k [40:44<04:52, 1s/it]\n",
      "Loss train: 7.88e-02 val: 1.21e-01 grad: 8.76e+00 lr: 1.3e-04 89.4%┣┫ 2.2k/2.5k [40:45<04:51, 1s/it]\n",
      "Loss train: 7.60e-02 val: 1.16e-01 grad: 8.28e+00 lr: 1.3e-04 89.4%┣┫ 2.2k/2.5k [40:46<04:50, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.11e-01 grad: 8.23e+00 lr: 1.3e-04 89.4%┣┫ 2.2k/2.5k [40:47<04:49, 1s/it]\n",
      "Loss train: 7.16e-02 val: 1.05e-01 grad: 8.26e+00 lr: 1.3e-04 89.5%┣┫ 2.2k/2.5k [40:48<04:48, 1s/it]\n",
      "Loss train: 7.08e-02 val: 1.03e-01 grad: 8.71e+00 lr: 1.3e-04 89.5%┣┫ 2.2k/2.5k [40:49<04:47, 1s/it]\n",
      "Loss train: 7.14e-02 val: 1.06e-01 grad: 8.64e+00 lr: 1.3e-04 89.6%┣┫ 2.2k/2.5k [40:50<04:46, 1s/it]\n",
      "Loss train: 6.97e-02 val: 1.01e-01 grad: 8.25e+00 lr: 1.3e-04 89.6%┣┫ 2.2k/2.5k [40:52<04:45, 1s/it]\n",
      "Loss train: 6.94e-02 val: 9.93e-02 grad: 8.48e+00 lr: 1.3e-04 89.6%┣┫ 2.2k/2.5k [40:53<04:44, 1s/it]\n",
      "Loss train: 6.98e-02 val: 9.67e-02 grad: 8.94e+00 lr: 1.3e-04 89.7%┣┫ 2.2k/2.5k [40:54<04:42, 1s/it]\n",
      "Loss train: 6.94e-02 val: 1.00e-01 grad: 8.58e+00 lr: 1.3e-04 89.7%┣┫ 2.2k/2.5k [40:55<04:41, 1s/it]\n",
      "Loss train: 7.10e-02 val: 8.50e-02 grad: 7.54e+00 lr: 1.3e-04 89.8%┣┫ 2.2k/2.5k [40:56<04:40, 1s/it]\n",
      "Loss train: 7.28e-02 val: 9.56e-02 grad: 1.17e+01 lr: 1.3e-04 89.8%┣┫ 2.2k/2.5k [40:57<04:39, 1s/it]\n",
      "Loss train: 7.69e-02 val: 9.49e-02 grad: 8.54e+00 lr: 1.3e-04 89.8%┣┫ 2.2k/2.5k [40:58<04:38, 1s/it]\n",
      "Loss train: 7.49e-02 val: 9.59e-02 grad: 8.31e+00 lr: 1.3e-04 89.9%┣┫ 2.2k/2.5k [41:00<04:37, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.69e-02 grad: 7.27e+00 lr: 1.3e-04 89.9%┣┫ 2.2k/2.5k [41:01<04:36, 1s/it]\n",
      "Loss train: 7.16e-02 val: 9.73e-02 grad: 8.29e+00 lr: 1.3e-04 90.0%┣┫ 2.2k/2.5k [41:02<04:35, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.70e-02 grad: 8.38e+00 lr: 1.3e-04 90.0%┣┫ 2.2k/2.5k [41:03<04:34, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.63e-02 grad: 8.88e+00 lr: 1.3e-04 90.0%┣┫ 2.3k/2.5k [41:04<04:33, 1s/it]\n",
      "Loss train: 6.96e-02 val: 9.79e-02 grad: 8.35e+00 lr: 1.3e-04 90.1%┣┫ 2.3k/2.5k [41:05<04:32, 1s/it]\n",
      "Loss train: 6.92e-02 val: 9.91e-02 grad: 8.38e+00 lr: 1.3e-04 90.1%┣┫ 2.3k/2.5k [41:06<04:30, 1s/it]\n",
      "Loss train: 6.88e-02 val: 9.79e-02 grad: 8.93e+00 lr: 1.3e-04 90.2%┣┫ 2.3k/2.5k [41:07<04:29, 1s/it]\n",
      "Loss train: 7.03e-02 val: 1.02e-01 grad: 8.14e+00 lr: 1.3e-04 90.2%┣┫ 2.3k/2.5k [41:08<04:28, 1s/it]\n",
      "Loss train: 6.84e-02 val: 9.60e-02 grad: 8.10e+00 lr: 1.3e-04 90.2%┣┫ 2.3k/2.5k [41:09<04:27, 1s/it]\n",
      "Loss train: 6.81e-02 val: 9.69e-02 grad: 8.93e+00 lr: 1.3e-04 90.3%┣┫ 2.3k/2.5k [41:10<04:26, 1s/it]\n",
      "Loss train: 6.96e-02 val: 9.64e-02 grad: 8.39e+00 lr: 1.3e-04 90.3%┣┫ 2.3k/2.5k [41:11<04:25, 1s/it]\n",
      "Loss train: 6.88e-02 val: 9.61e-02 grad: 8.06e+00 lr: 1.3e-04 90.4%┣┫ 2.3k/2.5k [41:12<04:24, 1s/it]\n",
      "Loss train: 6.86e-02 val: 9.59e-02 grad: 8.20e+00 lr: 1.3e-04 90.4%┣┫ 2.3k/2.5k [41:13<04:23, 1s/it]\n",
      "Loss train: 6.83e-02 val: 9.51e-02 grad: 8.31e+00 lr: 1.3e-04 90.4%┣┫ 2.3k/2.5k [41:14<04:22, 1s/it]\n",
      "Loss train: 7.17e-02 val: 9.48e-02 grad: 1.20e+01 lr: 1.3e-04 90.5%┣┫ 2.3k/2.5k [41:16<04:21, 1s/it]\n",
      "Loss train: 7.54e-02 val: 9.28e-02 grad: 7.95e+00 lr: 1.3e-04 90.5%┣┫ 2.3k/2.5k [41:17<04:19, 1s/it]\n",
      "Loss train: 7.40e-02 val: 9.22e-02 grad: 8.29e+00 lr: 1.3e-04 90.6%┣┫ 2.3k/2.5k [41:18<04:18, 1s/it]\n",
      "Loss train: 7.40e-02 val: 9.69e-02 grad: 9.12e+00 lr: 1.3e-04 90.6%┣┫ 2.3k/2.5k [41:19<04:17, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.76e-02 grad: 9.18e+00 lr: 1.3e-04 90.6%┣┫ 2.3k/2.5k [41:20<04:16, 1s/it]\n",
      "Loss train: 7.24e-02 val: 1.03e-01 grad: 9.17e+00 lr: 1.3e-04 90.7%┣┫ 2.3k/2.5k [41:21<04:15, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.72e-02 grad: 7.90e+00 lr: 1.3e-04 90.7%┣┫ 2.3k/2.5k [41:22<04:14, 1s/it]\n",
      "Loss train: 7.29e-02 val: 9.77e-02 grad: 9.55e+00 lr: 1.3e-04 90.8%┣┫ 2.3k/2.5k [41:23<04:13, 1s/it]\n",
      "Loss train: 6.99e-02 val: 1.03e-01 grad: 9.00e+00 lr: 1.3e-04 90.8%┣┫ 2.3k/2.5k [41:24<04:12, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.07e-01 grad: 8.03e+00 lr: 1.3e-04 90.8%┣┫ 2.3k/2.5k [41:25<04:11, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.77e-02 grad: 7.26e+00 lr: 1.3e-04 90.9%┣┫ 2.3k/2.5k [41:26<04:10, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.02e-01 grad: 9.94e+00 lr: 1.3e-04 90.9%┣┫ 2.3k/2.5k [41:28<04:09, 1s/it]\n",
      "Loss train: 7.24e-02 val: 9.79e-02 grad: 9.81e+00 lr: 1.3e-04 91.0%┣┫ 2.3k/2.5k [41:29<04:07, 1s/it]\n",
      "Loss train: 7.76e-02 val: 1.10e-01 grad: 8.15e+00 lr: 1.3e-04 91.0%┣┫ 2.3k/2.5k [41:30<04:06, 1s/it]\n",
      "Loss train: 6.98e-02 val: 1.05e-01 grad: 6.56e+00 lr: 1.3e-04 91.0%┣┫ 2.3k/2.5k [41:31<04:05, 1s/it]\n",
      "Loss train: 7.35e-02 val: 1.01e-01 grad: 1.37e+01 lr: 1.3e-04 91.1%┣┫ 2.3k/2.5k [41:32<04:04, 1s/it]\n",
      "Loss train: 8.29e-02 val: 9.80e-02 grad: 8.95e+00 lr: 1.3e-04 91.1%┣┫ 2.3k/2.5k [41:33<04:03, 1s/it]\n",
      "Loss train: 8.58e-02 val: 1.02e-01 grad: 9.76e+00 lr: 1.3e-04 91.2%┣┫ 2.3k/2.5k [41:34<04:02, 1s/it]\n",
      "Loss train: 7.82e-02 val: 1.03e-01 grad: 9.69e+00 lr: 1.3e-04 91.2%┣┫ 2.3k/2.5k [41:35<04:01, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.08e-01 grad: 8.97e+00 lr: 1.3e-04 91.2%┣┫ 2.3k/2.5k [41:36<04:00, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.12e-01 grad: 7.38e+00 lr: 1.3e-04 91.3%┣┫ 2.3k/2.5k [41:37<03:59, 1s/it]\n",
      "Loss train: 7.27e-02 val: 1.09e-01 grad: 8.28e+00 lr: 1.3e-04 91.3%┣┫ 2.3k/2.5k [41:38<03:58, 1s/it]\n",
      "Loss train: 7.15e-02 val: 1.03e-01 grad: 9.06e+00 lr: 1.3e-04 91.4%┣┫ 2.3k/2.5k [41:39<03:56, 1s/it]\n",
      "Loss train: 7.05e-02 val: 1.03e-01 grad: 8.50e+00 lr: 1.3e-04 91.4%┣┫ 2.3k/2.5k [41:40<03:55, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.80e-02 grad: 9.03e+00 lr: 1.3e-04 91.4%┣┫ 2.3k/2.5k [41:41<03:54, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.72e-02 grad: 8.86e+00 lr: 1.3e-04 91.5%┣┫ 2.3k/2.5k [41:42<03:53, 1s/it]\n",
      "Loss train: 7.29e-02 val: 1.03e-01 grad: 7.37e+00 lr: 1.3e-04 91.5%┣┫ 2.3k/2.5k [41:43<03:52, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.56e-02 grad: 7.59e+00 lr: 1.3e-04 91.6%┣┫ 2.3k/2.5k [41:44<03:51, 1s/it]\n",
      "Loss train: 6.91e-02 val: 9.70e-02 grad: 9.33e+00 lr: 1.3e-04 91.6%┣┫ 2.3k/2.5k [41:46<03:50, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.08e-01 grad: 8.40e+00 lr: 1.3e-04 91.6%┣┫ 2.3k/2.5k [41:47<03:49, 1s/it]\n",
      "Loss train: 6.84e-02 val: 1.01e-01 grad: 8.45e+00 lr: 1.3e-04 91.7%┣┫ 2.3k/2.5k [41:48<03:48, 1s/it]\n",
      "Loss train: 6.82e-02 val: 9.94e-02 grad: 8.65e+00 lr: 1.3e-04 91.7%┣┫ 2.3k/2.5k [41:49<03:47, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.41e-02 grad: 8.53e+00 lr: 1.3e-04 91.8%┣┫ 2.3k/2.5k [41:50<03:46, 1s/it]\n",
      "Loss train: 7.20e-02 val: 9.66e-02 grad: 1.25e+01 lr: 1.3e-04 91.8%┣┫ 2.3k/2.5k [41:51<03:44, 1s/it]\n",
      "Loss train: 8.04e-02 val: 9.51e-02 grad: 8.00e+00 lr: 1.3e-04 91.8%┣┫ 2.3k/2.5k [41:52<03:43, 1s/it]\n",
      "Loss train: 7.58e-02 val: 9.47e-02 grad: 8.85e+00 lr: 1.3e-04 91.9%┣┫ 2.3k/2.5k [41:53<03:42, 1s/it]\n",
      "Loss train: 7.97e-02 val: 1.06e-01 grad: 9.99e+00 lr: 1.3e-04 91.9%┣┫ 2.3k/2.5k [41:55<03:41, 1s/it]\n",
      "Loss train: 7.08e-02 val: 1.04e-01 grad: 9.29e+00 lr: 1.3e-04 92.0%┣┫ 2.3k/2.5k [41:56<03:40, 1s/it]\n",
      "Loss train: 7.09e-02 val: 1.05e-01 grad: 8.28e+00 lr: 1.3e-04 92.0%┣┫ 2.3k/2.5k [41:57<03:39, 1s/it]\n",
      "Loss train: 7.26e-02 val: 1.07e-01 grad: 8.47e+00 lr: 1.3e-04 92.0%┣┫ 2.3k/2.5k [41:58<03:38, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.91e-02 grad: 8.32e+00 lr: 1.3e-04 92.1%┣┫ 2.3k/2.5k [41:59<03:37, 1s/it]\n",
      "Loss train: 7.30e-02 val: 9.86e-02 grad: 9.63e+00 lr: 1.3e-04 92.1%┣┫ 2.3k/2.5k [42:00<03:36, 1s/it]\n",
      "Loss train: 6.86e-02 val: 9.90e-02 grad: 8.62e+00 lr: 1.3e-04 92.2%┣┫ 2.3k/2.5k [42:01<03:35, 1s/it]\n",
      "Loss train: 7.02e-02 val: 1.01e-01 grad: 8.40e+00 lr: 1.3e-04 92.2%┣┫ 2.3k/2.5k [42:02<03:33, 1s/it]\n",
      "Loss train: 6.77e-02 val: 9.55e-02 grad: 8.80e+00 lr: 1.3e-04 92.2%┣┫ 2.3k/2.5k [42:03<03:32, 1s/it]\n",
      "Loss train: 6.87e-02 val: 9.22e-02 grad: 8.70e+00 lr: 1.3e-04 92.3%┣┫ 2.3k/2.5k [42:04<03:31, 1s/it]\n",
      "Loss train: 7.44e-02 val: 1.01e-01 grad: 9.36e+00 lr: 1.3e-04 92.3%┣┫ 2.3k/2.5k [42:05<03:30, 1s/it]\n",
      "Loss train: 7.09e-02 val: 7.79e-02 grad: 7.90e+00 lr: 1.3e-04 92.4%┣┫ 2.3k/2.5k [42:07<03:29, 1s/it]\n",
      "Loss train: 7.78e-02 val: 9.21e-02 grad: 1.38e+01 lr: 1.3e-04 92.4%┣┫ 2.3k/2.5k [42:08<03:28, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.00e-01 grad: 6.78e+00 lr: 1.3e-04 92.4%┣┫ 2.3k/2.5k [42:09<03:27, 1s/it]\n",
      "Loss train: 7.86e-02 val: 9.86e-02 grad: 8.78e+00 lr: 1.3e-04 92.5%┣┫ 2.3k/2.5k [42:10<03:26, 1s/it]\n",
      "Loss train: 8.57e-02 val: 1.13e-01 grad: 9.86e+00 lr: 1.3e-04 92.5%┣┫ 2.3k/2.5k [42:11<03:25, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.04e-01 grad: 9.84e+00 lr: 1.3e-04 92.6%┣┫ 2.3k/2.5k [42:12<03:24, 1s/it]\n",
      "Loss train: 7.34e-02 val: 1.09e-01 grad: 9.24e+00 lr: 1.3e-04 92.6%┣┫ 2.3k/2.5k [42:13<03:23, 1s/it]\n",
      "Loss train: 7.22e-02 val: 1.08e-01 grad: 7.79e+00 lr: 1.3e-04 92.6%┣┫ 2.3k/2.5k [42:14<03:21, 1s/it]\n",
      "Loss train: 6.95e-02 val: 1.03e-01 grad: 7.61e+00 lr: 1.3e-04 92.7%┣┫ 2.3k/2.5k [42:15<03:20, 1s/it]\n",
      "Loss train: 6.90e-02 val: 1.01e-01 grad: 8.54e+00 lr: 1.3e-04 92.7%┣┫ 2.3k/2.5k [42:16<03:19, 1s/it]\n",
      "Loss train: 6.90e-02 val: 1.01e-01 grad: 8.16e+00 lr: 1.3e-04 92.8%┣┫ 2.3k/2.5k [42:17<03:18, 1s/it]\n",
      "Loss train: 6.85e-02 val: 9.61e-02 grad: 8.52e+00 lr: 1.3e-04 92.8%┣┫ 2.3k/2.5k [42:19<03:17, 1s/it]\n",
      "Loss train: 6.77e-02 val: 9.52e-02 grad: 8.16e+00 lr: 1.3e-04 92.8%┣┫ 2.3k/2.5k [42:20<03:16, 1s/it]\n",
      "Loss train: 6.75e-02 val: 9.42e-02 grad: 8.61e+00 lr: 1.3e-04 92.9%┣┫ 2.3k/2.5k [42:21<03:15, 1s/it]\n",
      "Loss train: 6.72e-02 val: 9.47e-02 grad: 8.66e+00 lr: 1.3e-04 92.9%┣┫ 2.3k/2.5k [42:22<03:14, 1s/it]\n",
      "Loss train: 7.55e-02 val: 1.02e-01 grad: 7.59e+00 lr: 1.3e-04 93.0%┣┫ 2.3k/2.5k [42:23<03:13, 1s/it]\n",
      "Loss train: 6.81e-02 val: 9.28e-02 grad: 8.99e+00 lr: 1.3e-04 93.0%┣┫ 2.3k/2.5k [42:24<03:12, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.25e-02 grad: 9.52e+00 lr: 1.3e-04 93.0%┣┫ 2.3k/2.5k [42:25<03:10, 1s/it]\n",
      "Loss train: 6.98e-02 val: 9.79e-02 grad: 8.62e+00 lr: 1.3e-04 93.1%┣┫ 2.3k/2.5k [42:26<03:09, 1s/it]\n",
      "Loss train: 6.98e-02 val: 8.14e-02 grad: 6.88e+00 lr: 1.3e-04 93.1%┣┫ 2.3k/2.5k [42:27<03:08, 1s/it]\n",
      "Loss train: 7.92e-02 val: 9.39e-02 grad: 1.57e+01 lr: 1.3e-04 93.2%┣┫ 2.3k/2.5k [42:29<03:07, 1s/it]\n",
      "Loss train: 9.57e-02 val: 9.66e-02 grad: 9.30e+00 lr: 1.3e-04 93.2%┣┫ 2.3k/2.5k [42:30<03:06, 1s/it]\n",
      "Loss train: 8.45e-02 val: 1.13e-01 grad: 9.69e+00 lr: 1.3e-04 93.2%┣┫ 2.3k/2.5k [42:31<03:05, 1s/it]\n",
      "Loss train: 8.24e-02 val: 1.15e-01 grad: 9.03e+00 lr: 1.3e-04 93.3%┣┫ 2.3k/2.5k [42:32<03:04, 1s/it]\n",
      "Loss train: 7.48e-02 val: 1.15e-01 grad: 9.36e+00 lr: 1.3e-04 93.3%┣┫ 2.3k/2.5k [42:33<03:03, 1s/it]\n",
      "Loss train: 7.46e-02 val: 1.15e-01 grad: 7.67e+00 lr: 1.3e-04 93.4%┣┫ 2.3k/2.5k [42:34<03:02, 1s/it]\n",
      "Loss train: 7.22e-02 val: 1.05e-01 grad: 8.76e+00 lr: 1.3e-04 93.4%┣┫ 2.3k/2.5k [42:35<03:01, 1s/it]\n",
      "Loss train: 7.53e-02 val: 1.06e-01 grad: 9.61e+00 lr: 1.3e-04 93.4%┣┫ 2.3k/2.5k [42:36<03:00, 1s/it]\n",
      "Loss train: 7.41e-02 val: 1.11e-01 grad: 8.61e+00 lr: 1.3e-04 93.5%┣┫ 2.3k/2.5k [42:37<02:58, 1s/it]\n",
      "Loss train: 7.00e-02 val: 1.06e-01 grad: 6.35e+00 lr: 1.3e-04 93.5%┣┫ 2.3k/2.5k [42:38<02:57, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.90e-02 grad: 8.50e+00 lr: 1.3e-04 93.6%┣┫ 2.3k/2.5k [42:39<02:56, 1s/it]\n",
      "Loss train: 6.84e-02 val: 9.86e-02 grad: 8.92e+00 lr: 1.3e-04 93.6%┣┫ 2.3k/2.5k [42:41<02:55, 1s/it]\n",
      "Loss train: 7.03e-02 val: 1.05e-01 grad: 9.24e+00 lr: 1.3e-04 93.6%┣┫ 2.3k/2.5k [42:42<02:54, 1s/it]\n",
      "Loss train: 6.88e-02 val: 9.22e-02 grad: 8.29e+00 lr: 1.3e-04 93.7%┣┫ 2.3k/2.5k [42:43<02:53, 1s/it]\n",
      "Loss train: 8.59e-02 val: 1.01e-01 grad: 1.72e+01 lr: 1.3e-04 93.7%┣┫ 2.3k/2.5k [42:44<02:52, 1s/it]\n",
      "Loss train: 9.34e-02 val: 9.32e-02 grad: 8.51e+00 lr: 1.3e-04 93.8%┣┫ 2.3k/2.5k [42:45<02:51, 1s/it]\n",
      "Loss train: 8.80e-02 val: 1.01e-01 grad: 1.00e+01 lr: 1.3e-04 93.8%┣┫ 2.3k/2.5k [42:46<02:50, 1s/it]\n",
      "Loss train: 9.24e-02 val: 1.21e-01 grad: 8.76e+00 lr: 1.3e-04 93.8%┣┫ 2.3k/2.5k [42:47<02:49, 1s/it]\n",
      "Loss train: 7.85e-02 val: 1.12e-01 grad: 8.50e+00 lr: 1.3e-04 93.9%┣┫ 2.3k/2.5k [42:48<02:47, 1s/it]\n",
      "Loss train: 7.79e-02 val: 1.18e-01 grad: 8.08e+00 lr: 1.3e-04 93.9%┣┫ 2.3k/2.5k [42:49<02:46, 1s/it]\n",
      "Loss train: 7.63e-02 val: 1.17e-01 grad: 7.70e+00 lr: 1.3e-04 94.0%┣┫ 2.3k/2.5k [42:50<02:45, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.09e-01 grad: 7.64e+00 lr: 1.3e-04 94.0%┣┫ 2.4k/2.5k [42:51<02:44, 1s/it]\n",
      "Loss train: 7.11e-02 val: 1.03e-01 grad: 8.19e+00 lr: 1.3e-04 94.0%┣┫ 2.4k/2.5k [42:52<02:43, 1s/it]\n",
      "Loss train: 7.14e-02 val: 1.00e-01 grad: 8.71e+00 lr: 1.3e-04 94.1%┣┫ 2.4k/2.5k [42:53<02:42, 1s/it]\n",
      "Loss train: 7.04e-02 val: 1.04e-01 grad: 9.07e+00 lr: 1.3e-04 94.1%┣┫ 2.4k/2.5k [42:54<02:41, 1s/it]\n",
      "Loss train: 7.01e-02 val: 9.85e-02 grad: 8.76e+00 lr: 1.3e-04 94.2%┣┫ 2.4k/2.5k [42:55<02:40, 1s/it]\n",
      "Loss train: 7.21e-02 val: 9.87e-02 grad: 9.13e+00 lr: 1.3e-04 94.2%┣┫ 2.4k/2.5k [42:57<02:39, 1s/it]\n",
      "Loss train: 6.88e-02 val: 1.01e-01 grad: 9.11e+00 lr: 1.3e-04 94.2%┣┫ 2.4k/2.5k [42:58<02:38, 1s/it]\n",
      "Loss train: 6.84e-02 val: 9.66e-02 grad: 9.05e+00 lr: 1.3e-04 94.3%┣┫ 2.4k/2.5k [42:59<02:37, 1s/it]\n",
      "Loss train: 6.83e-02 val: 1.00e-01 grad: 9.22e+00 lr: 1.3e-04 94.3%┣┫ 2.4k/2.5k [43:00<02:35, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.85e-02 grad: 1.31e+01 lr: 1.3e-04 94.4%┣┫ 2.4k/2.5k [43:01<02:34, 1s/it]\n",
      "Loss train: 7.84e-02 val: 9.55e-02 grad: 5.96e+00 lr: 1.3e-04 94.4%┣┫ 2.4k/2.5k [43:02<02:33, 1s/it]\n",
      "Loss train: 8.11e-02 val: 9.30e-02 grad: 7.71e+00 lr: 1.3e-04 94.4%┣┫ 2.4k/2.5k [43:03<02:32, 1s/it]\n",
      "Loss train: 7.67e-02 val: 9.26e-02 grad: 8.79e+00 lr: 1.3e-04 94.5%┣┫ 2.4k/2.5k [43:04<02:31, 1s/it]\n",
      "Loss train: 7.43e-02 val: 9.67e-02 grad: 9.10e+00 lr: 1.3e-04 94.5%┣┫ 2.4k/2.5k [43:05<02:30, 1s/it]\n",
      "Loss train: 7.14e-02 val: 1.01e-01 grad: 8.14e+00 lr: 1.3e-04 94.6%┣┫ 2.4k/2.5k [43:06<02:29, 1s/it]\n",
      "Loss train: 7.29e-02 val: 1.04e-01 grad: 8.16e+00 lr: 1.3e-04 94.6%┣┫ 2.4k/2.5k [43:07<02:28, 1s/it]\n",
      "Loss train: 7.04e-02 val: 9.76e-02 grad: 7.84e+00 lr: 1.3e-04 94.6%┣┫ 2.4k/2.5k [43:08<02:27, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.77e-02 grad: 8.42e+00 lr: 1.3e-04 94.7%┣┫ 2.4k/2.5k [43:09<02:26, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.35e-02 grad: 8.89e+00 lr: 1.3e-04 94.7%┣┫ 2.4k/2.5k [43:10<02:24, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.61e-02 grad: 8.26e+00 lr: 1.3e-04 94.8%┣┫ 2.4k/2.5k [43:11<02:23, 1s/it]\n",
      "Loss train: 7.06e-02 val: 9.39e-02 grad: 7.76e+00 lr: 1.3e-04 94.8%┣┫ 2.4k/2.5k [43:12<02:22, 1s/it]\n",
      "Loss train: 6.78e-02 val: 9.51e-02 grad: 9.06e+00 lr: 1.3e-04 94.8%┣┫ 2.4k/2.5k [43:14<02:21, 1s/it]\n",
      "Loss train: 7.14e-02 val: 9.82e-02 grad: 8.39e+00 lr: 1.3e-04 94.9%┣┫ 2.4k/2.5k [43:15<02:20, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.40e-02 grad: 8.10e+00 lr: 1.3e-04 94.9%┣┫ 2.4k/2.5k [43:16<02:19, 1s/it]\n",
      "Loss train: 7.40e-02 val: 8.90e-02 grad: 9.30e+00 lr: 1.3e-04 95.0%┣┫ 2.4k/2.5k [43:17<02:18, 1s/it]\n",
      "Loss train: 7.15e-02 val: 9.72e-02 grad: 1.51e+01 lr: 1.3e-04 95.0%┣┫ 2.4k/2.5k [43:18<02:17, 1s/it]\n",
      "Loss train: 1.27e-01 val: 1.47e-01 grad: 7.07e+00 lr: 1.3e-04 95.0%┣┫ 2.4k/2.5k [43:19<02:16, 1s/it]\n",
      "Loss train: 7.76e-02 val: 9.89e-02 grad: 8.59e+00 lr: 1.3e-04 95.1%┣┫ 2.4k/2.5k [43:20<02:15, 1s/it]\n",
      "Loss train: 8.22e-02 val: 1.10e-01 grad: 8.80e+00 lr: 1.3e-04 95.1%┣┫ 2.4k/2.5k [43:21<02:14, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.06e-01 grad: 9.69e+00 lr: 1.3e-04 95.2%┣┫ 2.4k/2.5k [43:23<02:12, 1s/it]\n",
      "Loss train: 7.33e-02 val: 1.04e-01 grad: 8.67e+00 lr: 1.3e-04 95.2%┣┫ 2.4k/2.5k [43:24<02:11, 1s/it]\n",
      "Loss train: 7.24e-02 val: 1.06e-01 grad: 8.42e+00 lr: 1.3e-04 95.2%┣┫ 2.4k/2.5k [43:25<02:10, 1s/it]\n",
      "Loss train: 7.32e-02 val: 1.11e-01 grad: 7.40e+00 lr: 1.3e-04 95.3%┣┫ 2.4k/2.5k [43:26<02:09, 1s/it]\n",
      "Loss train: 7.06e-02 val: 1.05e-01 grad: 8.49e+00 lr: 1.3e-04 95.3%┣┫ 2.4k/2.5k [43:27<02:08, 1s/it]\n",
      "Loss train: 6.99e-02 val: 1.01e-01 grad: 7.83e+00 lr: 1.3e-04 95.4%┣┫ 2.4k/2.5k [43:28<02:07, 1s/it]\n",
      "Loss train: 6.94e-02 val: 9.84e-02 grad: 8.10e+00 lr: 1.3e-04 95.4%┣┫ 2.4k/2.5k [43:29<02:06, 1s/it]\n",
      "Loss train: 6.89e-02 val: 9.88e-02 grad: 8.31e+00 lr: 1.3e-04 95.4%┣┫ 2.4k/2.5k [43:30<02:05, 1s/it]\n",
      "Loss train: 6.83e-02 val: 9.69e-02 grad: 8.04e+00 lr: 1.3e-04 95.5%┣┫ 2.4k/2.5k [43:31<02:04, 1s/it]\n",
      "Loss train: 7.08e-02 val: 9.59e-02 grad: 9.32e+00 lr: 1.3e-04 95.5%┣┫ 2.4k/2.5k [43:32<02:03, 1s/it]\n",
      "Loss train: 7.04e-02 val: 1.00e-01 grad: 7.61e+00 lr: 1.3e-04 95.6%┣┫ 2.4k/2.5k [43:33<02:01, 1s/it]\n",
      "Loss train: 6.81e-02 val: 9.55e-02 grad: 7.07e+00 lr: 1.3e-04 95.6%┣┫ 2.4k/2.5k [43:34<02:00, 1s/it]\n",
      "Loss train: 7.30e-02 val: 9.50e-02 grad: 1.43e+01 lr: 1.3e-04 95.6%┣┫ 2.4k/2.5k [43:36<01:59, 1s/it]\n",
      "Loss train: 8.39e-02 val: 9.49e-02 grad: 7.44e+00 lr: 1.3e-04 95.7%┣┫ 2.4k/2.5k [43:37<01:58, 1s/it]\n",
      "Loss train: 8.18e-02 val: 9.50e-02 grad: 7.99e+00 lr: 1.3e-04 95.7%┣┫ 2.4k/2.5k [43:38<01:57, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.00e-01 grad: 9.00e+00 lr: 1.3e-04 95.8%┣┫ 2.4k/2.5k [43:39<01:56, 1s/it]\n",
      "Loss train: 7.32e-02 val: 1.09e-01 grad: 8.56e+00 lr: 1.3e-04 95.8%┣┫ 2.4k/2.5k [43:40<01:55, 1s/it]\n",
      "Loss train: 7.57e-02 val: 1.14e-01 grad: 7.27e+00 lr: 1.3e-04 95.8%┣┫ 2.4k/2.5k [43:41<01:54, 1s/it]\n",
      "Loss train: 7.24e-02 val: 1.05e-01 grad: 7.54e+00 lr: 1.3e-04 95.9%┣┫ 2.4k/2.5k [43:42<01:53, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.06e-01 grad: 8.17e+00 lr: 1.3e-04 95.9%┣┫ 2.4k/2.5k [43:43<01:52, 1s/it]\n",
      "Loss train: 7.04e-02 val: 1.02e-01 grad: 7.67e+00 lr: 1.3e-04 96.0%┣┫ 2.4k/2.5k [43:44<01:51, 1s/it]\n",
      "Loss train: 7.08e-02 val: 1.01e-01 grad: 8.80e+00 lr: 1.3e-04 96.0%┣┫ 2.4k/2.5k [43:45<01:49, 1s/it]\n",
      "Loss train: 7.23e-02 val: 1.05e-01 grad: 8.81e+00 lr: 1.3e-04 96.0%┣┫ 2.4k/2.5k [43:46<01:48, 1s/it]\n",
      "Loss train: 6.86e-02 val: 9.83e-02 grad: 8.32e+00 lr: 1.3e-04 96.1%┣┫ 2.4k/2.5k [43:47<01:47, 1s/it]\n",
      "Loss train: 6.82e-02 val: 9.76e-02 grad: 7.84e+00 lr: 1.3e-04 96.1%┣┫ 2.4k/2.5k [43:48<01:46, 1s/it]\n",
      "Loss train: 7.06e-02 val: 9.49e-02 grad: 8.17e+00 lr: 1.3e-04 96.2%┣┫ 2.4k/2.5k [43:49<01:45, 1s/it]\n",
      "Loss train: 7.32e-02 val: 1.02e-01 grad: 8.54e+00 lr: 1.3e-04 96.2%┣┫ 2.4k/2.5k [43:51<01:44, 1s/it]\n",
      "Loss train: 6.78e-02 val: 9.63e-02 grad: 6.89e+00 lr: 1.3e-04 96.2%┣┫ 2.4k/2.5k [43:52<01:43, 1s/it]\n",
      "Loss train: 6.88e-02 val: 9.78e-02 grad: 9.20e+00 lr: 1.3e-04 96.3%┣┫ 2.4k/2.5k [43:53<01:42, 1s/it]\n",
      "Loss train: 6.96e-02 val: 1.06e-01 grad: 8.31e+00 lr: 1.3e-04 96.3%┣┫ 2.4k/2.5k [43:54<01:41, 1s/it]\n",
      "Loss train: 7.12e-02 val: 1.06e-01 grad: 1.04e+01 lr: 1.3e-04 96.4%┣┫ 2.4k/2.5k [43:55<01:40, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.67e-02 grad: 8.03e+00 lr: 1.3e-04 96.4%┣┫ 2.4k/2.5k [43:56<01:38, 1s/it]\n",
      "Loss train: 7.26e-02 val: 9.57e-02 grad: 9.08e+00 lr: 1.3e-04 96.4%┣┫ 2.4k/2.5k [43:57<01:37, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.02e-01 grad: 8.42e+00 lr: 1.3e-04 96.5%┣┫ 2.4k/2.5k [43:58<01:36, 1s/it]\n",
      "Loss train: 6.80e-02 val: 9.51e-02 grad: 7.64e+00 lr: 1.3e-04 96.5%┣┫ 2.4k/2.5k [43:59<01:35, 1s/it]\n",
      "Loss train: 6.76e-02 val: 9.42e-02 grad: 8.67e+00 lr: 1.3e-04 96.6%┣┫ 2.4k/2.5k [44:00<01:34, 1s/it]\n",
      "Loss train: 6.97e-02 val: 9.11e-02 grad: 8.79e+00 lr: 1.3e-04 96.6%┣┫ 2.4k/2.5k [44:01<01:33, 1s/it]\n",
      "Loss train: 6.93e-02 val: 9.14e-02 grad: 9.40e+00 lr: 1.3e-04 96.6%┣┫ 2.4k/2.5k [44:02<01:32, 1s/it]\n",
      "Loss train: 7.40e-02 val: 1.00e-01 grad: 8.74e+00 lr: 1.3e-04 96.7%┣┫ 2.4k/2.5k [44:03<01:31, 1s/it]\n",
      "Loss train: 6.95e-02 val: 9.55e-02 grad: 8.50e+00 lr: 1.3e-04 96.7%┣┫ 2.4k/2.5k [44:04<01:30, 1s/it]\n",
      "Loss train: 7.39e-02 val: 1.01e-01 grad: 1.03e+01 lr: 1.3e-04 96.8%┣┫ 2.4k/2.5k [44:06<01:29, 1s/it]\n",
      "Loss train: 6.81e-02 val: 9.69e-02 grad: 9.37e+00 lr: 1.3e-04 96.8%┣┫ 2.4k/2.5k [44:07<01:28, 1s/it]\n",
      "Loss train: 6.93e-02 val: 1.01e-01 grad: 7.67e+00 lr: 1.3e-04 96.8%┣┫ 2.4k/2.5k [44:08<01:26, 1s/it]\n",
      "Loss train: 6.76e-02 val: 9.74e-02 grad: 7.84e+00 lr: 1.3e-04 96.9%┣┫ 2.4k/2.5k [44:09<01:25, 1s/it]\n",
      "Loss train: 6.86e-02 val: 9.35e-02 grad: 8.02e+00 lr: 1.3e-04 96.9%┣┫ 2.4k/2.5k [44:10<01:24, 1s/it]\n",
      "Loss train: 7.02e-02 val: 9.70e-02 grad: 9.38e+00 lr: 1.3e-04 97.0%┣┫ 2.4k/2.5k [44:11<01:23, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.37e-02 grad: 1.48e+01 lr: 1.3e-04 97.0%┣┫ 2.4k/2.5k [44:12<01:22, 1s/it]\n",
      "Loss train: 1.09e-01 val: 1.05e-01 grad: 7.98e+00 lr: 1.3e-04 97.0%┣┫ 2.4k/2.5k [44:13<01:21, 1s/it]\n",
      "Loss train: 8.91e-02 val: 1.02e-01 grad: 1.19e+01 lr: 1.3e-04 97.1%┣┫ 2.4k/2.5k [44:14<01:20, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.05e-01 grad: 9.39e+00 lr: 1.3e-04 97.1%┣┫ 2.4k/2.5k [44:15<01:19, 1s/it]\n",
      "Loss train: 7.37e-02 val: 1.11e-01 grad: 8.81e+00 lr: 1.3e-04 97.2%┣┫ 2.4k/2.5k [44:16<01:18, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.13e-01 grad: 7.47e+00 lr: 1.3e-04 97.2%┣┫ 2.4k/2.5k [44:17<01:17, 1s/it]\n",
      "Loss train: 7.19e-02 val: 1.06e-01 grad: 7.13e+00 lr: 1.3e-04 97.2%┣┫ 2.4k/2.5k [44:18<01:15, 1s/it]\n",
      "Loss train: 7.26e-02 val: 1.04e-01 grad: 8.23e+00 lr: 1.3e-04 97.3%┣┫ 2.4k/2.5k [44:20<01:14, 1s/it]\n",
      "Loss train: 7.16e-02 val: 1.02e-01 grad: 8.94e+00 lr: 1.3e-04 97.3%┣┫ 2.4k/2.5k [44:21<01:13, 1s/it]\n",
      "Loss train: 7.07e-02 val: 1.06e-01 grad: 7.83e+00 lr: 1.3e-04 97.4%┣┫ 2.4k/2.5k [44:22<01:12, 1s/it]\n",
      "Loss train: 6.89e-02 val: 1.02e-01 grad: 7.64e+00 lr: 1.3e-04 97.4%┣┫ 2.4k/2.5k [44:23<01:11, 1s/it]\n",
      "Loss train: 6.83e-02 val: 9.98e-02 grad: 7.98e+00 lr: 1.3e-04 97.4%┣┫ 2.4k/2.5k [44:24<01:10, 1s/it]\n",
      "Loss train: 6.76e-02 val: 9.92e-02 grad: 8.03e+00 lr: 1.3e-04 97.5%┣┫ 2.4k/2.5k [44:25<01:09, 1s/it]\n",
      "Loss train: 6.76e-02 val: 9.94e-02 grad: 8.06e+00 lr: 1.3e-04 97.5%┣┫ 2.4k/2.5k [44:26<01:08, 1s/it]\n",
      "Loss train: 6.81e-02 val: 9.72e-02 grad: 7.93e+00 lr: 1.3e-04 97.6%┣┫ 2.4k/2.5k [44:27<01:07, 1s/it]\n",
      "Loss train: 7.10e-02 val: 8.09e-02 grad: 8.20e+00 lr: 1.3e-04 97.6%┣┫ 2.4k/2.5k [44:28<01:06, 1s/it]\n",
      "Loss train: 7.53e-02 val: 9.48e-02 grad: 1.45e+01 lr: 1.3e-04 97.6%┣┫ 2.4k/2.5k [44:29<01:05, 1s/it]\n",
      "Loss train: 8.84e-02 val: 9.10e-02 grad: 8.73e+00 lr: 1.3e-04 97.7%┣┫ 2.4k/2.5k [44:30<01:03, 1s/it]\n",
      "Loss train: 8.66e-02 val: 9.59e-02 grad: 9.30e+00 lr: 1.3e-04 97.7%┣┫ 2.4k/2.5k [44:31<01:02, 1s/it]\n",
      "Loss train: 7.97e-02 val: 1.03e-01 grad: 9.84e+00 lr: 1.3e-04 97.8%┣┫ 2.4k/2.5k [44:33<01:01, 1s/it]\n",
      "Loss train: 7.36e-02 val: 1.04e-01 grad: 9.33e+00 lr: 1.3e-04 97.8%┣┫ 2.4k/2.5k [44:34<01:00, 1s/it]\n",
      "Loss train: 7.38e-02 val: 1.09e-01 grad: 7.45e+00 lr: 1.3e-04 97.8%┣┫ 2.4k/2.5k [44:35<00:59, 1s/it]\n",
      "Loss train: 7.88e-02 val: 1.11e-01 grad: 7.20e+00 lr: 1.3e-04 97.9%┣┫ 2.4k/2.5k [44:36<00:58, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.99e-02 grad: 6.26e+00 lr: 1.3e-04 97.9%┣┫ 2.4k/2.5k [44:37<00:57, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.75e-02 grad: 8.48e+00 lr: 1.3e-04 98.0%┣┫ 2.4k/2.5k [44:38<00:56, 1s/it]\n",
      "Loss train: 7.07e-02 val: 9.66e-02 grad: 8.66e+00 lr: 1.3e-04 98.0%┣┫ 2.5k/2.5k [44:39<00:55, 1s/it]\n",
      "Loss train: 6.91e-02 val: 9.85e-02 grad: 8.20e+00 lr: 1.3e-04 98.0%┣┫ 2.5k/2.5k [44:40<00:54, 1s/it]\n",
      "Loss train: 6.93e-02 val: 9.85e-02 grad: 7.63e+00 lr: 1.3e-04 98.1%┣┫ 2.5k/2.5k [44:41<00:53, 1s/it]\n",
      "Loss train: 6.96e-02 val: 9.39e-02 grad: 7.99e+00 lr: 1.3e-04 98.1%┣┫ 2.5k/2.5k [44:42<00:51, 1s/it]\n",
      "Loss train: 6.99e-02 val: 9.35e-02 grad: 9.11e+00 lr: 1.3e-04 98.2%┣┫ 2.5k/2.5k [44:43<00:50, 1s/it]\n",
      "Loss train: 6.74e-02 val: 9.49e-02 grad: 8.61e+00 lr: 1.3e-04 98.2%┣┫ 2.5k/2.5k [44:44<00:49, 1s/it]\n",
      "Loss train: 6.66e-02 val: 9.42e-02 grad: 8.07e+00 lr: 1.3e-04 98.2%┣┫ 2.5k/2.5k [44:46<00:48, 1s/it]\n",
      "Loss train: 6.67e-02 val: 9.35e-02 grad: 8.17e+00 lr: 1.3e-04 98.3%┣┫ 2.5k/2.5k [44:47<00:47, 1s/it]\n",
      "Loss train: 6.66e-02 val: 8.63e-02 grad: 8.13e+00 lr: 1.3e-04 98.3%┣┫ 2.5k/2.5k [44:48<00:46, 1s/it]\n",
      "Loss train: 8.44e-02 val: 1.03e-01 grad: 1.46e+01 lr: 1.3e-04 98.4%┣┫ 2.5k/2.5k [44:49<00:45, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.05e-01 grad: 8.47e+00 lr: 1.3e-04 98.4%┣┫ 2.5k/2.5k [44:50<00:44, 1s/it]\n",
      "Loss train: 9.51e-02 val: 1.12e-01 grad: 9.94e+00 lr: 1.3e-04 98.4%┣┫ 2.5k/2.5k [44:51<00:43, 1s/it]\n",
      "Loss train: 1.01e-01 val: 1.22e-01 grad: 9.76e+00 lr: 1.3e-04 98.5%┣┫ 2.5k/2.5k [44:52<00:42, 1s/it]\n",
      "Loss train: 7.67e-02 val: 1.01e-01 grad: 9.47e+00 lr: 1.3e-04 98.5%┣┫ 2.5k/2.5k [44:53<00:40, 1s/it]\n",
      "Loss train: 7.32e-02 val: 1.02e-01 grad: 8.11e+00 lr: 1.3e-04 98.6%┣┫ 2.5k/2.5k [44:54<00:39, 1s/it]\n",
      "Loss train: 7.28e-02 val: 1.06e-01 grad: 8.09e+00 lr: 1.3e-04 98.6%┣┫ 2.5k/2.5k [44:55<00:38, 1s/it]\n",
      "Loss train: 7.81e-02 val: 1.09e-01 grad: 9.29e+00 lr: 1.3e-04 98.6%┣┫ 2.5k/2.5k [44:56<00:37, 1s/it]\n",
      "Loss train: 7.32e-02 val: 1.09e-01 grad: 8.90e+00 lr: 1.3e-04 98.7%┣┫ 2.5k/2.5k [44:58<00:36, 1s/it]\n",
      "Loss train: 7.06e-02 val: 1.03e-01 grad: 8.35e+00 lr: 1.3e-04 98.7%┣┫ 2.5k/2.5k [44:59<00:35, 1s/it]\n",
      "Loss train: 6.96e-02 val: 1.03e-01 grad: 7.61e+00 lr: 1.3e-04 98.8%┣┫ 2.5k/2.5k [45:00<00:34, 1s/it]\n",
      "Loss train: 6.94e-02 val: 1.03e-01 grad: 8.28e+00 lr: 1.3e-04 98.8%┣┫ 2.5k/2.5k [45:01<00:33, 1s/it]\n",
      "Loss train: 6.91e-02 val: 9.79e-02 grad: 7.38e+00 lr: 1.3e-04 98.8%┣┫ 2.5k/2.5k [45:02<00:32, 1s/it]\n",
      "Loss train: 6.80e-02 val: 9.61e-02 grad: 8.57e+00 lr: 1.3e-04 98.9%┣┫ 2.5k/2.5k [45:03<00:31, 1s/it]\n",
      "Loss train: 6.73e-02 val: 9.46e-02 grad: 8.12e+00 lr: 1.3e-04 98.9%┣┫ 2.5k/2.5k [45:04<00:30, 1s/it]\n",
      "Loss train: 7.39e-02 val: 1.01e-01 grad: 7.02e+00 lr: 1.3e-04 99.0%┣┫ 2.5k/2.5k [45:05<00:28, 1s/it]\n",
      "Loss train: 6.67e-02 val: 9.49e-02 grad: 6.44e+00 lr: 1.3e-04 99.0%┣┫ 2.5k/2.5k [45:06<00:27, 1s/it]\n",
      "Loss train: 6.86e-02 val: 9.45e-02 grad: 1.02e+01 lr: 1.3e-04 99.0%┣┫ 2.5k/2.5k [45:07<00:26, 1s/it]\n",
      "Loss train: 7.13e-02 val: 9.53e-02 grad: 7.87e+00 lr: 1.3e-04 99.1%┣┫ 2.5k/2.5k [45:08<00:25, 1s/it]\n",
      "Loss train: 7.00e-02 val: 9.49e-02 grad: 7.74e+00 lr: 1.3e-04 99.1%┣┫ 2.5k/2.5k [45:09<00:24, 1s/it]\n",
      "Loss train: 7.58e-02 val: 9.99e-02 grad: 8.11e+00 lr: 1.3e-04 99.2%┣┫ 2.5k/2.5k [45:11<00:23, 1s/it]\n",
      "Loss train: 7.03e-02 val: 9.57e-02 grad: 7.16e+00 lr: 1.3e-04 99.2%┣┫ 2.5k/2.5k [45:12<00:22, 1s/it]\n",
      "Loss train: 6.84e-02 val: 9.42e-02 grad: 8.80e+00 lr: 1.3e-04 99.2%┣┫ 2.5k/2.5k [45:13<00:21, 1s/it]\n",
      "Loss train: 6.77e-02 val: 9.36e-02 grad: 8.67e+00 lr: 1.3e-04 99.3%┣┫ 2.5k/2.5k [45:14<00:20, 1s/it]\n",
      "Loss train: 7.49e-02 val: 1.02e-01 grad: 6.12e+00 lr: 1.3e-04 99.3%┣┫ 2.5k/2.5k [45:15<00:19, 1s/it]\n",
      "Loss train: 7.10e-02 val: 9.91e-02 grad: 6.60e+00 lr: 1.3e-04 99.4%┣┫ 2.5k/2.5k [45:16<00:18, 1s/it]\n",
      "Loss train: 7.71e-02 val: 1.04e-01 grad: 1.59e+01 lr: 1.3e-04 99.4%┣┫ 2.5k/2.5k [45:17<00:16, 1s/it]\n",
      "Loss train: 1.08e-01 val: 1.10e-01 grad: 9.47e+00 lr: 1.3e-04 99.4%┣┫ 2.5k/2.5k [45:18<00:15, 1s/it]\n",
      "Loss train: 8.72e-02 val: 9.56e-02 grad: 1.05e+01 lr: 1.3e-04 99.5%┣┫ 2.5k/2.5k [45:19<00:14, 1s/it]\n",
      "Loss train: 8.82e-02 val: 1.21e-01 grad: 8.16e+00 lr: 1.3e-04 99.5%┣┫ 2.5k/2.5k [45:20<00:13, 1s/it]\n",
      "Loss train: 7.68e-02 val: 1.14e-01 grad: 9.24e+00 lr: 1.3e-04 99.6%┣┫ 2.5k/2.5k [45:21<00:12, 1s/it]\n",
      "Loss train: 7.61e-02 val: 1.16e-01 grad: 8.52e+00 lr: 1.3e-04 99.6%┣┫ 2.5k/2.5k [45:22<00:11, 1s/it]\n",
      "Loss train: 7.45e-02 val: 1.09e-01 grad: 7.96e+00 lr: 1.3e-04 99.6%┣┫ 2.5k/2.5k [45:23<00:10, 1s/it]\n",
      "Loss train: 7.18e-02 val: 1.07e-01 grad: 8.73e+00 lr: 1.3e-04 99.7%┣┫ 2.5k/2.5k [45:25<00:09, 1s/it]\n",
      "Loss train: 6.96e-02 val: 1.08e-01 grad: 8.43e+00 lr: 1.3e-04 99.7%┣┫ 2.5k/2.5k [45:26<00:08, 1s/it]\n",
      "Loss train: 6.85e-02 val: 1.01e-01 grad: 8.00e+00 lr: 1.3e-04 99.8%┣┫ 2.5k/2.5k [45:27<00:07, 1s/it]\n",
      "Loss train: 6.75e-02 val: 1.01e-01 grad: 8.58e+00 lr: 1.3e-04 99.8%┣┫ 2.5k/2.5k [45:28<00:05, 1s/it]\n",
      "Loss train: 6.71e-02 val: 9.96e-02 grad: 8.12e+00 lr: 1.3e-04 99.8%┣┫ 2.5k/2.5k [45:29<00:04, 1s/it]\n",
      "Loss train: 7.29e-02 val: 9.80e-02 grad: 1.12e+01 lr: 1.3e-04 99.9%┣┫ 2.5k/2.5k [45:30<00:03, 1s/it]\n",
      "Loss train: 7.74e-02 val: 1.02e-01 grad: 7.58e+00 lr: 1.3e-04 99.9%┣┫ 2.5k/2.5k [45:31<00:02, 1s/it]\n",
      "Loss train: 7.24e-02 val: 9.70e-02 grad: 7.33e+00 lr: 1.3e-04 100.0%┣┫ 2.5k/2.5k [45:32<00:01, 1s/it]\n",
      "Loss train: 7.16e-02 val: 9.74e-02 grad: 7.80e+00 lr: 6.3e-05 100.0%┣┫ 2.5k/2.5k [45:33<00:00, 1s/it]\n",
      "Loss train: 7.16e-02 val: 9.74e-02 grad: 7.80e+00 lr: 6.3e-05 100.0%┣┫ 2.5k/2.5k [45:33<00:00, 1s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = ProgressBar(iter:n_epoch);\n",
    "loss_epoch = zeros(Float64, n_exp);\n",
    "grad_norm = zeros(Float64, n_exp);\n",
    "for epoch in epochs\n",
    "    global p\n",
    "    for i_exp in randperm(n_exp)\n",
    "        if i_exp in l_val\n",
    "            continue\n",
    "        end\n",
    "        grad = ForwardDiff.gradient(x -> loss_neuralode(x, i_exp), p)\n",
    "        grad_norm[i_exp] = norm(grad, 2)\n",
    "        if grad_norm[i_exp] > grad_max\n",
    "            grad = grad ./ grad_norm[i_exp] .* grad_max\n",
    "        end\n",
    "        update!(opt, p, grad)\n",
    "    end\n",
    "    for i_exp = 1:n_exp\n",
    "        loss_epoch[i_exp] = loss_neuralode(p, i_exp)\n",
    "    end\n",
    "    loss_train = mean(loss_epoch[l_train])\n",
    "    loss_val = mean(loss_epoch[l_val])\n",
    "    grad_mean = mean(grad_norm[l_train])\n",
    "    set_description(\n",
    "        epochs,\n",
    "        string(\n",
    "            @sprintf(\n",
    "                \"Loss train: %.2e val: %.2e grad: %.2e lr: %.1e\",\n",
    "                loss_train,\n",
    "                loss_val,\n",
    "                grad_mean,\n",
    "                opt[1].eta\n",
    "                #opt.eta\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    cb(p, loss_train, loss_val, grad_mean)\n",
    "end\n",
    "\n",
    "conf[\"loss_train\"] = minimum(l_loss_train)\n",
    "conf[\"loss_val\"] = minimum(l_loss_val)\n",
    "YAML.write_file(config_path, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2ea0be79-930f-45b0-9b7f-0db824f24fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×15 Matrix{Float64}:\n",
      " -0.0   0.54  0.01  0.0   0.0  0.0   80.24  0.14   9.29   0.0   -0.54  -0.01   0.5   0.05  0.0\n",
      "  0.33  0.19  0.05  0.0   0.0  0.01  72.31  0.25   9.66  -0.33  -0.19  -0.05   0.48  0.09  0.0\n",
      "  0.39  0.0   0.0   0.0   0.0  0.25  95.21  0.24  11.09  -0.39   0.27   0.01   0.1   0.0   0.0\n",
      " -0.0   0.0   0.0   0.52  0.0  0.15  89.59  0.14   9.54   0.0    0.13   0.32  -0.52  0.08  0.0\n",
      " -0.0   0.0   0.0   0.65  0.0  0.0   77.4   0.27  11.19   0.0    0.41  -0.0   -0.65  0.24  0.0\n",
      "  0.12  0.0   0.0   0.36  0.0  0.01  91.15  0.28  11.15  -0.12   0.35  -0.0   -0.36  0.12  0.0\n",
      " -0.0   0.0   0.0   0.47  0.0  0.07  63.42  0.13   9.59   0.0    0.07   0.17  -0.47  0.23  0.0\n",
      "  0.29  0.0   0.01  0.0   0.0  0.08  98.2   0.13   9.39  -0.29   0.07  -0.01   0.23  0.01  0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08336832189868705, 0.0822633956374589)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr_name = \"5s8r1c-01\"\n",
    "fig_path = string(\"./results_catalyst/\", expr_name, \"/figs\")\n",
    "ckpt_path = string(\"./results_catalyst/\", expr_name, \"/checkpoint\")\n",
    "@load string(ckpt_path, \"/mymodel.bson\") p #opt l_loss_train l_loss_val list_grad iter\n",
    "\n",
    "#p[9] = -2\n",
    "p_cutoff = 0.0\n",
    "\n",
    "display_p(p)\n",
    "\n",
    "for i_exp = 1:n_exp\n",
    "    cbi(p, i_exp)\n",
    "end\n",
    "\n",
    "loss_epoch = zeros(Float64, n_exp);\n",
    "for i_exp = 1:n_exp\n",
    "    loss_epoch[i_exp] = loss_neuralode(p, i_exp)\n",
    "end\n",
    "loss_train = mean(loss_epoch[l_train])\n",
    "loss_val = mean(loss_epoch[l_val])\n",
    "loss_train, loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "180f0087-7d23-4caf-b963-59a640c7f79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"./results_cat3/7s8r1c-08/figs\\\\loss_grad.png\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyplot()\n",
    "plot_loss(l_loss_train, l_loss_val; yscale = :log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "32b7324b-1f3c-49ce-82d5-bdea38ee0760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_neuralode_res (generic function with 1 method)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_neuralode_res(p)\n",
    "    l_loss_exp = zeros(n_exp)\n",
    "    for i_exp in 1:n_exp\n",
    "        exp_data = l_exp_data[i_exp]\n",
    "        pred = Array(pred_n_ode(p, i_exp, exp_data))\n",
    "        masslist = sum(clamp.(@view(pred[1:end-1, :]), 0, Inf), dims = 1)'\n",
    "        l_loss_exp[i_exp] = mae(masslist, @view(exp_data[1:length(masslist), 3]))\n",
    "    end\n",
    "    return l_loss_exp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "694abb24-251e-48ea-b8c3-0508068d5e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results after pruning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"results after pruning\")\n",
    "maxiters = 1e5\n",
    "p_cutoff = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8838fa78-1bd6-4852-941d-8cd57e1e9cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: -0.59 val: -0.58 p_cutoff: 1.85e-01\n",
      " species (column) reaction (row)\n",
      "w_in | w_cat_in | Ea | b | lnA | w_out | w_cat_out\n",
      "8×19 Matrix{Float64}:\n",
      " -0.0   -0.0  -0.0   -0.0   -0.0  0.0   -0.0  0.0   289.36  0.12  26.97   0.0   0.0   0.0    0.0    0.0   -0.0   0.0   0.0\n",
      "  1.75  -0.0  -0.0   -0.0    0.0  0.0   -0.0  0.19  238.25  0.56  44.6   -1.75  0.0   0.0    0.0    0.52   1.23  0.0   0.0\n",
      " -0.0   -0.0   0.68  -0.0   -0.0  0.0    0.0  0.35  251.77  0.45  43.44   0.0   0.0  -0.68   0.0    0.0    0.1   0.59  0.0\n",
      " -0.0   -0.0  -0.0   -0.0   -0.0  0.0   -0.0  0.19  263.4   0.08  28.33   0.0   0.0   0.0    0.0    0.0   -0.0   0.0   0.0\n",
      " -0.0   -0.0  -0.0    0.21   0.0  1.21   0.0  0.01  261.82  0.71  49.65   0.0   0.0   0.0   -0.21   0.28  -1.21  1.13  0.0\n",
      " -0.0   -0.0   0.21  -0.0    0.6  0.0   -0.0  0.07  279.12  0.67  49.71   0.0   0.0  -0.21   0.0   -0.6    0.81  0.0   0.0\n",
      " -0.0   -0.0   0.0   -0.0    0.0  0.53  -0.0  0.16  127.97  0.0   25.21   0.0   0.0   0.27   0.0    0.26  -0.53  0.0   0.0\n",
      " -0.0   -0.0  -0.0   -0.0   -0.0  0.0   -0.0  0.01  300.0   0.15  25.33   0.0   0.0   0.0    0.0    0.0   -0.0   0.0   0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_epoch = zeros(Float64, n_exp);\n",
    "for i_exp = 1:n_exp\n",
    "    loss_epoch[i_exp] = loss_neuralode(p, i_exp)\n",
    "end\n",
    "loss_train = mean(loss_epoch[l_train])\n",
    "loss_val = mean(loss_epoch[l_val])\n",
    "@printf(\n",
    "    \"Loss train: %.2f val: %.2f p_cutoff: %.2e\",\n",
    "    log10(loss_train),\n",
    "    log10(loss_val),\n",
    "    p_cutoff\n",
    ")\n",
    "\n",
    "display_p(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bf45d227-0338-417c-9d00-f7e335945bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_exp in randperm(n_exp)\n",
    "    cbi(p, i_exp)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b36d80ae-cc4d-4586-9036-52cd9f48cd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18-element Vector{Float64}:\n",
       " 0.5731249928232994\n",
       " 0.573818698550572\n",
       " 0.5599319238596631\n",
       " 0.6038493091687965\n",
       " 0.6018047217616849\n",
       " 0.6049788272576078\n",
       " 0.5073004092208907\n",
       " 0.42357193899865203\n",
       " 0.4033434698914744\n",
       " 0.4233822601722467\n",
       " 0.5204082517584994\n",
       " 0.4756764430372351\n",
       " 0.5732156460521934\n",
       " 0.5729433777049404\n",
       " 0.5785797501762748\n",
       " 0.5167738478487672\n",
       " 0.5376574507332098\n",
       " 0.5233036501654168"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_loss_exp = loss_neuralode_res(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a930109e-6e17-4306-b3ef-33029d8e7c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18×8 Matrix{Float64}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_exp_data = []\n",
    "l_exp_info = zeros(Float64, length(l_exp), 3)\n",
    "for (i_exp, value) in enumerate(l_exp)\n",
    "    filename = string(\"exp_data_cat3/expdata_no\", string(value), \".txt\")\n",
    "    exp_data = Float64.(load_exp(filename))\n",
    "    push!(l_exp_data, exp_data)\n",
    "    l_exp_info[i_exp, 1] = exp_data[1, 2] # initial temperature, K\n",
    "end\n",
    "l_exp_info[:, 2] = readdlm(\"exp_data_cat3/beta.txt\")[l_exp]\n",
    "#l_exp_info[:, 3] = log.(readdlm(\"exp_data_cat3/ocen.txt\")[l_exp] .+ llb)\n",
    "l_exp_info[:, 3] = (readdlm(\"exp_data_cat3/cata_conc.txt\")[l_exp]);\n",
    "\n",
    "l_loss_exp = loss_neuralode_res(p)\n",
    "\n",
    "conc_end = zeros(n_exp, ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba4c10-f9a4-4ced-a9fe-ed1147baf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot TGA data\n",
    "list_plt = []\n",
    "for i_exp = 1:18\n",
    "    T0, beta, cata_conc = l_exp_info[i_exp, :]\n",
    "    exp_data = l_exp_data[i_exp]\n",
    "    sol = pred_n_ode(p, i_exp, exp_data)\n",
    "    Tlist = similar(sol.t)\n",
    "    T0, beta, cata_conc = l_exp_info[i_exp, :]\n",
    "    for (i, t) in enumerate(sol.t)\n",
    "        Tlist[i] = getsampletemp(t, T0, beta)\n",
    "    end\n",
    "\n",
    "    conc_end[i_exp, :] .= sol[:, end]\n",
    "\n",
    "    plt = plot(\n",
    "        Tlist,\n",
    "        exp_data[:, 3],\n",
    "        seriestype = :scatter,\n",
    "        label = \"Exp-$i_exp\",\n",
    "        framestyle = :box,\n",
    "    )\n",
    "    plot!(\n",
    "        plt,\n",
    "        Tlist,\n",
    "        sum(sol[1:end-1, :], dims = 1)',\n",
    "        lw = 2,\n",
    "        legend = false,\n",
    "    )\n",
    "    xlabel!(plt, \"Temperature [K]\")\n",
    "\n",
    "    exp_cond = @sprintf(\"T0:%.0f K\", T0)\n",
    "    exp_cond *= @sprintf(\"\\nbeta:%.0f K/min\", beta)\n",
    "\n",
    "    ann_loc = [0.2, 0.3]\n",
    "    if i_exp in [6, 7, 14]\n",
    "        ann_loc = [0.7, 0.4]\n",
    "    end\n",
    "    annotate!(plt, xlims(plt)[1] + (xlims(plt)[end] - xlims(plt)[1]) * ann_loc[1],\n",
    "                   ylims(plt)[1] + (ylims(plt)[end] - ylims(plt)[1]) * ann_loc[2],\n",
    "                   text(exp_cond, 11))\n",
    "\n",
    "    ylabel!(plt, \"Mass [-] (No. $i_exp)\")\n",
    "    ylims!(plt, (0.0, 1.0))\n",
    "    plot!(\n",
    "        plt,\n",
    "        xtickfontsize = 11,\n",
    "        ytickfontsize = 11,\n",
    "        xguidefontsize = 12,\n",
    "        yguidefontsize = 12,\n",
    "    )\n",
    "\n",
    "    push!(list_plt, plt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9c7e8-e64e-4af2-a060-a0f8db1c88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_all = plot(list_plt..., layout = (7, 2))\n",
    "plot!(plt_all, size = (800, 1200))\n",
    "png(plt_all, string(fig_path, \"/TGA_mass_summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cb7eb-b65e-465f-b547-08daa8f66a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DENG-MIT/Biomass.jl/blob/main/backup/crnn_cellulose_ocen_test.jl\n",
    "varnames = [\"Cellu\", \"S2\", \"S3\", \"Vola\"]\n",
    "for i_exp in 1:n_exp\n",
    "    T0, beta, ocen = l_exp_info[i_exp, :]\n",
    "    exp_data = l_exp_data[i_exp]\n",
    "    sol = pred_n_ode(p, i_exp, exp_data)\n",
    "    Tlist = similar(sol.t)\n",
    "    T0, beta, ocen = l_exp_info[i_exp, :]\n",
    "    for (i, t) in enumerate(sol.t)\n",
    "        Tlist[i] = getsampletemp(t, T0, beta)\n",
    "    end\n",
    "    value = l_exp[i_exp]\n",
    "    list_plt = []\n",
    "    scale_factor = 1 ./ maximum(sol[:, :], dims=2)\n",
    "    scale_factor .= 1.0\n",
    "    plt = plot(Tlist, clamp.(sol[1, :], 0, Inf), label = varnames[1])\n",
    "    for i in 2:ns\n",
    "        if scale_factor[i] > 1.1\n",
    "            _label =  @sprintf(\"%s x %.2e\", varnames[i], scale_factor[i])\n",
    "        else\n",
    "            _label = varnames[i]\n",
    "        end\n",
    "        plot!(plt, Tlist, clamp.(sol[i, :], 0, Inf) * scale_factor[i], label = _label)\n",
    "    end\n",
    "    xlabel!(plt, \"Temperature [K]\");\n",
    "    ylabel!(plt, \"Mass (-)\");\n",
    "    plot!(plt, size=(350, 350), legend=:topleft, framestyle=:box)\n",
    "    plot!(\n",
    "        plt,\n",
    "        xtickfontsize = 11,\n",
    "        ytickfontsize = 11,\n",
    "        xguidefontsize = 12,\n",
    "        yguidefontsize = 12,\n",
    "    )\n",
    "    png(plt, string(fig_path, \"/pred_S_exp_$value\"))\n",
    "end\n",
    "\n",
    "include(\"dataset.jl\")\n",
    "gr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
